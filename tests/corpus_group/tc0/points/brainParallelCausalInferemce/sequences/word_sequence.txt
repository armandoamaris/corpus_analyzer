2020
ieee
26th
international
conference
on
parallel
and
distributed
systems
icpads
978-1-7281-9074-7/20/$31.00
©2020
ieee
doi
10.1109/icpads51040.2020.00035
2020
ieee
26th
international
conference
on
parallel
and
distributed
systems
icpads
massively
parallel
causal
inference
of
whole
brain
dynamics
at
single
neuron
resolution
wassapon
watanakeesuntorn®
keichi
takahashi
kohei
ichikawa
joseph
park
george
sugihara
ryousei
takano$
jason
haga®
gerald
m
pao¥
nara
institute
of
science
and
technology
nara
japan
{wassapon.watanakeesuntorn.wq0
keichi
ichikawa}
@is.naist.jp
tus
department
of
the
interior
florida
usa
josephpark
@ieee.org
university
of
california
san
diego
california
usa
gsugihara@ucsd.edu
national
institute
of
advanced
industrial
science
and
technology
tsukuba
japan
{takano-ryousei
jh.haga}
@aist.go.jp
salk
institute
for
biological
studies
california
usa
pao@salk.edu
abstract—empirical
dynamic
modeling
edm
is
nonlinear
time
series
causal
inference
framework
the
latest
implementa
tion
of
edm
cppedm
has
only
been
used
for
small
datasets
due
to
computational
cost
with
the
growth
of
data
collection
capabilities
there
is
great
need
to
identify
causal
relationships
in
large
datasets
we
present
mpedm
parallel
distributed
implementation
of
edm
optimized
for
modern
gpu-centric
supercomputers
we
improve
the
original
algorithm
to
reduce
redundant
computation
and
optimize
the
implementation
to
fully
utilize
hardware
resources
such
as
gpus
and
simd
units
as
use
case
we
run
mpedm
on
ai
bridging
cloud
infrastructure
abci
using
datasets
of
an
entire
animal
brain
sampled
at
single
neuron
resolution
to
identify
dynamical
causation
patterns
across
the
brain
mpedm
is
1,530
faster
than
cppedm
and
dataset
containing
101,729
neuron
was
analyzed
in
199
seconds
on
512
nodes
this
is
the
largest
edm
causal
inference
achieved
to
date
keywords-empirical
dynamic
modeling
causal
inference
parallel
distributed
computing
gpu
high-performance
com
puting
neuroscience
1
introduction
reverse-engineering
and
building
digital
reconstruction
of
the
brain
is
one
of
the
greatest
scientific
challenges
of
today
recent
study
on
the
mouse
cortex
1
showed
that
97%
of
the
possible
connections
between
neurons
exist
this
result
suggests
that
it
is
likely
more
informative
to
investi
gate
the
dynamic
interactions
between
neurons
rather
than
the
static
connectivity
between
them
to
fully
understand
the
function
of
the
brain
based
on
this
insight
we
are
building
mathematical
and
computational
tools
to
analyze
the
dynamic
interactions
between
neurons
based
on
empirical
dynamic
modeling
edm
edm
is
nonlinear
time
series
causal
inference
framework
based
on
the
generalized
takens
embedding
theorem
on
state
2690-5965/20/$31.00
©2020
ieee
doi
10.1109/icpads51040.2020.00035
196
space
reconstruction
2
edm
is
used
to
study
and
predict
the
behavior
of
nonlinear
dynamical
systems
convergent
cross
mapping
ccm
is
one
of
the
edm
algorithms
that
allows
to
estimate
the
existence
and
strength
of
the
causal
strength
between
two
time
series
in
dynamical
system
3
in
this
study
we
utilize
ccm
to
infer
the
causal
rela
tionships
between
every
neuron
in
an
entire
brain
and
con
struct
causal
map
that
describes
the
dynamic
interactions
among
neurons
for
this
purpose
we
have
recorded
the
neural
activity
i.e
firing
rate
of
an
entire
larval
zebrafish
brain
at
singe-neuron
resolution
by
using
light
sheet
fluorescence
microscopy
the
original
implementation
of
edm
cppedm
has
mostly
been
used
for
individual
time
series
of
relatively
short
length
and
and
mostly
small
numbers
of
variables
for
its
computational
cost
since
larval
zebrafish
brain
contains
approximately
10°
neurons
staggering
number
of
10
cross
mappings
need
to
be
performed
in
total
ccm
of
this
enormous
scale
has
never
been
achieved
so
far
because
of
the
sheer
amount
of
computation
required
the
goal
of
this
paper
is
to
develop
highly
scalable
and
optimized
implementation
of
edm
that
is
able
to
analyze
the
whole
zebrafish
brain
dataset
within
reasonable
time
we
present
mpedm
parallel
distributed
implementation
of
edm
optimized
for
execution
on
modern
gpu-centric
su
percomputers
we
improve
the
original
algorithm
in
cppedm
to
reduce
redundant
computation
and
optimize
the
implemen
tation
to
fully
utilize
hardware
resources
such
as
gpus
and
simd
units
our
evaluation
on
al
bridging
cloud
infrastructure
abci
japan’s
most
high
performance
supercomputer
as
of
today
demonstrated
the
unprecedented
performance
of
mpedm
mpedm
was
used
to
analyze
dataset
containing
the
activity
https://github.com/keichi/mpedm
authorized
licensed
use
limited
to
boston
university
downloaded
on
may
19,2021
at
00:11:38
utc
from
ieee
xplore
restrictions
apply
of
53,053
neurons
in
only
20
seconds
using
512
abci
nodes
in
contrast
cppedm
took
8.5
hours
to
analyze
the
same
dataset
using
the
same
number
of
nodes
4
furthermore
mpedm
analyzed
larger
dataset
containing
101,729
neurons
in
199
seconds
on
512
nodes
to
our
knowledge
this
is
the
largest
ccm
calculation
achieved
to
date
this
result
shows
the
potential
for
mpedm
and
abci
to
analyze
even
larger
datasets
in
the
future
the
rest
of
this
paper
is
structured
as
follows
section
ii
describes
the
background
of
this
research
and
edm
algorithm
section
iii
explains
our
proposal
to
improve
the
algorithm
of
the
edm
for
parallelization
and
to
support
gpu
architecture
section
1v
evaluates
the
performance
of
mpedm
and
presents
the
scientific
outcomes
obtained
with
mpedm
finally
sec
tion
concludes
this
paper
and
discusses
future
work
ii
background
a
causal
map
of
the
zebrafish
brain
at
single
neuron
resolution
to
understand
the
human
brain
activity
dynamics
with
complexity
of
10
neurons
and
10'°
synapses
at
single
neuron
resolution
is
currently
technically
impossible
task
similarly
mouse
brain
with
7.6
107
neurons
is
not
tractable
because
mammalian
brains
are
opaque
and
it
is
impossible
to
image
complete
mouse
brain
with
this
in
mind
the
zebrafish
embryo
is
an
attractive
model
system
with
120,000
neurons
and
transgenic
technology
as
well
as
natural
brain
transparency
the
zebrafish
embryo
is
sufficiently
complex
to
exhibit
interesting
behaviors
and
is
technologically
feasible
to
study
to
infer
basic
principles
of
systems
neuroscience
even
in
the
case
of
the
larval
zebrafish
with
about
120,000
neurons
we
do
not
have
the
physical
connectivity
map
that
is
the
connectome
of
the
larval
zebrafish
nor
do
we
have
the
synaptic
strengths
which
are
pieces
of
information
required
to
understand
the
brain
starting
from
the
physical
connectivity
complicating
this
notion
recent
work
from
the
mouse
brain
shows
that
97%
of
possible
physical
connections
exist
within
the
mouse
cortex
thus
making
it
difficult
to
analyze
given
this
difficulty
using
an
analogy
of
city
to
understand
how
city
works
it
will
be
easier
to
understand
the
city
from
the
traffic
patterns
than
from
the
street
map
thus
we
wished
to
analyze
the
fish
brain
at
single
neuron
resolution
from
network
activity
dynamics
perspective
although
imperfect
we
used
neural
activity
imaging
data
of
an
entire
brain
at
single
cell
resolution
in
behaving
larval
zebrafish
a
transparent
vertebrate
to
extract
all
relationships
in
an
intact
vertebrate
brain
to
achieve
this
we
recorded
whole
brain
neural
activity
patterns
in
multiple
animals
experiencing
hypoxia
using
selective
plane
illumination
microscope
spim
5
we
ob
tained
data
from
the
entire
5-day-old
larval
brain
120,000
neurons
at
hz
in
response
to
hypoxia
for
varying
amounts
of
time
typically
ranging
from
1,500
time
steps
to
up
to
8,000+
6
ccm
allows
the
inference
of
causation
from
nonlinear
time
series
even
with
substantial
noise
and
complete
absence
of
time
series
manifold
s4l
i~
j«
ni
1ﬂ1\«‘\uwww‘m‘w
ra
\(\444\4
i~
%jm
ﬂ\u‘\n‘nﬂ
yu-«\t‘d
jﬂﬂﬂv\ﬂgqu\
a‘ul
1—441‘1_
<a
i\
smooth
mapping
time
delayed
embedding
reconstructed
manifold
mx
reconstructed
manifold
my
fig
1
basic
idea
behind
empirical
dynamic
modeling
correlation
7
8
we
used
ccm
and
other
tools
from
the
edm
framework
for
the
inference
of
existence
strength
and
sign
of
causal
relationships
within
the
neural
activity
network
of
the
transparent
larval
fish
brain
5
ccm
determines
whether
and
how
much
causality
exists
between
individual
neurons
the
adjacency
in
the
network
is
determined
by
time
delay
cross
mapping
8
predictive
accuracy
values
give
the
interaction
strength
allow
us
to
infer
relationships
within
the
neural
network
without
observing
the
physical
connectivity
as
test
case
we
have
collected
multiple
data
sets
of
lengths
around
1600
time
steps
at
hz
which
contain
50,000-80,000
active
neurons
in
most
cases
we
have
analyzed
this
data
and
show
that
the
generated
time
series
are
suitable
for
causal
network
inference
using
the
edm
framework
and
thus
demonstrated
proof
of
principle
of
computational
tractability
b
empirical
dynamic
modeling
edm
is
mathematical
framework
designed
for
studying
nonlinear
dynamical
systems
edm
is
based
upon
the
concept
of
state
space
reconstruction
ssr
9
takens
theorem
states
that
the
attractor
manifold
of
multivariate
dynamical
system
can
be
reconstructed
from
time
lagged
coordinates
of
single
time
series
variable
10
figure
illustrates
the
concept
of
state
spaces
reconstruction
in
this
example
three
causally
related
time
series
variables
z(t
y
and
z
that
constitute
dynamical
system
form
an
attractor
manifold
in
the
state
space
shadow
manifold
mm
can
be
reconstructed
using
the
time
delayed
embeddings
of
z(t),z(t
7),z(t
27
where
denotes
the
time
lag
in
the
same
manner
lags
of
form
shadow
manifold
m
takens
theorem
states
that
the
reconstructed
manifolds
m
and
m
preserve
essential
mathe
matical
properties
such
as
the
topology
of
the
true
manifold
m
in
particular
there
exist
smooth
mappings
between
m
m
and
m
suggesting
that
neighbors
in
m
are
neighbors
in
j[y
as
well
simplex
projection
is
nonlinear
forecasting
algorithm
often
used
for
estimating
the
dimensionality
of
dynamical
authorized
licensed
use
limited
to
boston
university
downloaded
on
may
19,2021
at
00:11:38
utc
from
ieee
xplore
restrictions
apply
system
in
simplex
projection
the
input
time
series
is
split
into
two
halves
library
and
target
y
both
halves
are
embedded
into
e-dimensional
state
space
by
using
delayed
embeddings
given
point
y(t
y(tp),y(ty
1),...,y(t
1
in
the
target
state
space
its
nearest
neighbors
i.e
vertices
of
the
simplex
enclosing
y(t
are
searched
from
the
embedded
library
suppose
those
neighbors
are
x1(t1
22(t2
.
®p41(tps1
forecast
y(t
+1
can
be
made
by
averaging
the
future
of
the
neighbors
in
the
library
z1(t1
+1),z2(ta+1),...,2p41(tpr1
1
this
prediction
is
performed
for
every
point
in
and
the
results
are
compared
with
the
true
to
evaluate
the
prediction
accuracy
this
entire
procedure
is
repeated
for
different
values
and
the
that
achieves
the
highest
prediction
accuracy
is
determined
as
the
optimal
embedding
dimension
of
the
dynamical
system
ccm
determines
the
existence
and
strength
of
causality
between
two
time
series
variables
11
it
works
similar
to
simplex
projection
but
instead
of
predicting
within
single
time
series
ccm
predicts
one
time
series
from
another
if
can
be
predicted
from
with
significant
accuracy
we
conclude
that
ccm
causes
x
there
have
been
extensive
studies
on
causal
inference
structural
causal
model
scm
is
one
of
the
most
popular
causal
models
12
based
on
statistical
modeling
of
equilib
rium
systems
in
contrast
to
scm
edm
is
based
on
the
principle
of
state-space
reconstruction
shown
in
takens
theo
rem
of
non-equilibrium
systems
granger
causality
is
another
causal
inference
technique
based
on
statistical
modeling
13
granger
causality
however
as
stated
by
granger
himself
only
works
with
linear
and
stochastic
systems
and
cannot
be
applied
to
nonlinear
dynamical
system
compared
to
these
alternatives
edm
is
better
suited
to
find
the
causal
relationships
in
nonlinear
dynamical
system
such
as
the
brain
tajima
er
al
14
also
applied
embedding
theorems
in
nonlinear
state-space
reconstruction
to
analyze
dynamic
system
they
also
built
on
the
causality
inference
method
from
sugihara
et
al
3
in
their
work
edm
has
been
successfully
applied
to
diverse
research
fields
15
in
neuroscience
ccm
was
applied
to
identify
the
effective
connectivity
between
brain
areas
from
magnetoen
cephalography
meg
data
16
in
ecology
grziwotz
et
al
found
the
causal
relationships
between
the
environment
and
mosquito
abundance
by
using
ccm
17
environmental
fac
tors
such
as
temperature
precipitation
dew
point
air
pressure
and
mean
tide
level
were
identified
to
causally
affect
mosquito
abundance
ma
et
al
applied
simplex
projection
to
forecast
wind
generation
18
in
19
an
edm
algorithm
called
s
map
20
was
used
to
find
the
relationship
between
harvested
and
unharvested
fish
in
terms
of
size
age
and
others
luo
et
al
applied
ccm
to
estimate
the
causal
relationships
of
user
behavior
in
an
online
social
network
21
these
use
cases
demonstrate
the
wide
applicability
of
edm
to
analyze
nonlinear
dynamical
systems
198
c
cppedm
cppedm
22
is
the
latest
implementation
of
the
edm
framework
cppedm
is
general
purpose
c++
library
used
as
backend
by
redm
23
and
pyedm
24
which
are
edm
implementations
for
the
and
python
language
respectively
we
have
identified
two
major
issues
in
cppedm
that
hinder
large-scale
analysis
on
hpc
systems
redundant
computation
and
lack
of
gpu
support
since
cppedm
is
general
purpose
library
it
provides
one-to-one
cross
mapping
function
to
identify
the
causality
between
selected
combinations
of
time
series
variables
the
all-to-all
cross
mapping
function
is
im
plemented
by
reusing
the
one-to-one
cross
mapping
function
this
results
in
redundant
computation
additionally
cppedm
is
reference
implementation
of
edm
therefore
it
is
not
optimized
for
specific
hardware
architecture
such
as
gpus
furthermore
cppedm
suffers
from
significant
load
imbalance
among
workers
because
it
performs
static
decomposition
of
the
problem
in
fact
performance
evaluation
in
previous
work
showed
that
the
runtime
of
workers
varied
greatly
from
hours
to
8.5
hours
4
iii
mpedm
in
this
section
we
first
outline
the
original
causal
inference
algorithm
in
cppedm
then
we
describe
the
algorithmic
improvement
and
the
design
of
the
inter-node
and
intra-node
parallelization
in
mpedm
a
original
algorithm
algorithm
outlines
the
causal
inference
algorithm
in
cppedm
the
input
to
the
algorithm
is
an
array
ts
where
is
the
number
of
time
steps
within
time
series
and
is
the
number
of
time
series
in
addition
to
the
input
dataset
maximum
embedding
dimension
e
and
time
lag
need
to
be
supplied
the
output
is
an
casual
map
p
the
algorithm
consists
of
two
phases
1
simplex
projection
and
2
ccm
simplex
projection
finds
the
optimal
embedding
dimension
for
each
time
series
ccm
estimates
the
causal
relationship
between
two
time
series
using
the
optimal
embedded
dimension
obtained
in
the
first
phase
note
that
in
the
original
definition
of
ccm
predictions
are
made
multiple
times
using
randomly
subsampled
library
sets
of
different
sizes
and
it
is
tested
whether
increasing
the
library
set
size
improves
the
prediction
accuracy
in
this
research
we
excluded
this
step
since
the
convergence
test
passes
in
most
cases
if
the
prediction
using
the
full
library
set
achieves
high
accuracy
in
the
first
phase
simplex
projection
line
1-11
takes
time
series
in
the
dataset
and
splits
into
into
library
the
first
half
and
target
the
second
half
line
3—4
next
both
library
and
target
are
embedded
into
e-dimensional
space
using
time
delayed
embeddings
k-nearest
neighbors
knn
search
is
performed
in
the
state
space
to
find
the
nearest
target
points
from
each
library
point
line
5
the
search
results
are
stored
in
two
lookup
tables
indices
and
distances
both
of
which
are
two-dimensional
arrays
of
shape
e
1
element
7,j
in
the
indices
array
is
the
index
of
the
j-th
nearest
target
point
from
library
point
i
whereas
element
(
j
authorized
licensed
use
limited
to
boston
university
downloaded
on
may
19,2021
at
00:11:38
utc
from
ieee
xplore
restrictions
apply
algorithm
1
causal
inference
in
cppedm
input
dataset
s
n
time
series
of
length
l
maximum
embedding
dimension
e,,,q
output
causal
map
//
phase
1
simplex
projection
for
to
do
for
e«+
1to
e
do
library
first
half
of
s[i
target
second
half
of
s[i
indices
distances
kknn(library
target
e
distances
<—normalize(distances
prediction
lookup(indices
distances
library
e
|e
corrcoef(target
prediction
end
10
optel[i
argmax
p[e
end
//
phase
2
ccm
for
to
do
for
to
do
12
13
14
indices
distances
opte[j
15
<—normalize(distances
16
prediction
lookup(ind
distances
ts[j
opt
e[j
17
eli
4
corrcoef(ts[j
prediction
18
end
19
end
in
the
distances
array
is
the
euclidean
distance
between
the
library
point
and
its
j-th
nearest
target
point
the
distances
array
is
then
converted
to
exponential
scale
and
each
row
is
normalized
line
6
one
step
ahead
prediction
of
target
point
is
made
by
1
obtaining
the
indices
of
its
£'+
library
neighbors
from
indices
2
obtaining
the
one
step
ahead
values
of
those
library
points
from
library
and
3
computing
weighted
average
of
the
future
library
points
using
distances
line
7
finally
pearson’s
correlation
coefficient
is
computed
to
evaluate
the
predictive
skill
of
the
simplex
projection
using
the
prediction
results
and
real
observed
withheld
values
line
8
this
is
repeated
for
every
ranging
from
to
e
4
<20
in
practice
the
value
that
achieves
the
highest
accuracy
is
determined
to
be
the
optimal
embedding
dimension
for
the
time
series
and
stored
in
opte
line
10
in
the
second
phase
ccm
line
12-19
works
similar
to
simplex
projection
but
predicts
between
two
different
time
series
given
library
time
series
is
used
to
cross
predict
another
target
time
series
in
the
dataset
to
evaluate
whether
the
latter
is
the
cause
of
the
former
it
computes
and
normalizes
the
knn
tables
from
the
library
time
series
line
14—15
and
uses
the
tables
to
predict
the
target
time
series
line
16
note
that
simplex
projection
predicts
within
the
same
time
series
while
ccm
predicts
across
two
different
time
series
199
therefore
the
knn
tables
computed
in
the
simplex
projection
phase
cannot
be
reused
in
the
ccm
phase
the
correlation
between
the
predicted
values
and
the
actual
values
represents
strength
of
causality
line
17
in
this
manner
causal
inference
is
performed
for
all
combinations
of
time
series
in
the
dataset
we
have
profiled
cppedm
and
found
out
that
over
97%
of
the
total
runtime
is
spent
in
the
knn
search
in
addition
we
have
discovered
that
the
time
delayed
embedding
in
cppedm
replicates
the
time
series
times
and
causes
significant
memory
overhead
b
improved
algorithm
the
key
observation
behind
our
algorithmic
improvement
is
that
the
knn
lookup
table
for
ccm
is
constructed
from
the
library
time
series
only
and
the
target
time
series
is
not
used
this
suggests
that
once
the
knn
lookup
table
is
computed
for
particular
library
time
series
we
can
reuse
the
precomputed
table
to
make
predictions
for
every
target
time
series
this
improvement
is
trivial
if
nv
is
in
the
same
order
as
e,,q
which
was
the
case
in
previous
use
cases
of
edm
however
in
our
use
case
nv
is
equal
to
the
number
of
active
neurons
in
zebrafish
brain
which
is
roughly
10°
therefore
the
potential
speedup
becomes
significantly
large
algorithm
shows
the
pseudocode
of
the
improved
causal
inference
algorithm
in
mpedm
the
simplex
projection
algo
rithm
is
unchanged
from
cppedm
but
its
knn
and
lookup
functions
are
parallelized
and
optimized
the
ccm
algorithm
in
mpedm
is
improved
in
the
following
manner
for
each
library
time
series
we
first
compute
the
knn
lookup
tables
for
every
embedding
dimension
ranging
from
to
e,;,4
line
4-7
then
we
iterate
through
all
target
time
series
and
use
the
precomputed
lookup
table
for
the
optimal
embedding
dimension
of
the
target
time
series
to
predict
the
target
time
series
line
9-10
finally
we
compute
the
correlation
between
the
prediction
and
the
actual
target
to
estimate
the
causality
line
11
algorithms
outlines
the
knn
function
for
cpu
we
first
calculate
the
all-to-all
distances
between
every
library
and
target
point
in
the
state
space
note
that
we
do
not
explicitly
create
the
time
series
embeddings
on
memory
but
we
compute
them
on-the-fly
to
reduce
memory
footprint
and
increase
cache
hit
in
addition
both
indices
and
distances
are
stored
in
row-major
format
to
match
the
access
pattern
then
each
row
in
the
distances
and
indices
arrays
is
partially
sorted
in
descending
order
using
the
distances
as
sort
keys
we
use
heap
sort
to
implement
partial
sort
after
the
sorting
both
arrays
are
trimmed
from
to
e
1
and
returned
algorithm
shows
the
knn
function
for
gpu
in
the
gpu
version
we
create
time
series
embeddings
on
the
host
and
transfer
them
to
the
device
the
knn
search
is
executed
on
the
gpu
and
the
resulting
knn
tables
are
returned
to
the
host
algorithm
outlines
the
lookup
function
it
uses
the
knn
lookup
tables
indices
and
distances
of
the
library
time
series
for
each
target
point
the
indices
of
its
£'+
neighbors
are
retrieved
from
the
indices
table
then
those
neighbors
are
authorized
licensed
use
limited
to
boston
university
downloaded
on
may
19,2021
at
00:11:38
utc
from
ieee
xplore
restrictions
apply
algorithm
2
causal
inference
in
mpedm
algorithm
3
knn
for
cpu
input
dataset
s
n
time
series
of
length
l
maximum
embedding
dimension
e,,q
output
causal
map
//
phase
1
simplex
projection
for
to
do
//
same
as
cppedm
algorithm
1
end
//
phase
2
ccm
3fori«
1to
do
for
<+
1to
e
do
indices[e
distances[e
knn(¢s[i
ts]i
e
distances
<—normalize(distances
end
for
to
do
e
optel[j
10
prediction
lookup(indices[e
distances|ej|
ts[j
e
eli
4
corrcoef(ts[j
prediction
12
end
13
end
accumulated
using
the
weights
stored
in
the
distances
table
finally
the
function
returns
the
predicted
target
time
series
the
average
time
complexity
of
each
algorithm
is
analyzed
as
follows
the
time
complexity
of
the
knn
function
in
algorithm
and
is
o(l?e
because
the
all-to-all
dis
tance
calculation
is
o(l?e
and
the
sorting
is
approximately
o(l?log
e
the
time
complexity
of
the
lookup
function
in
algorithm
is
o(le
by
combining
these
results
the
time
complexity
of
simplex
projection
in
mpedm
is
o(nl?e
which
is
the
same
as
cppedm
the
time
complexity
of
ccm
in
mpedm
on
the
other
hand
is
o(nl*e
n2le
in
cppedm
the
time
complexity
of
ccm
is
o(n2l2e
as
result
the
time
complexity
of
the
whole
causal
inference
algorithm
in
mpedm
is
o(nl?e
n2le
c
inter-node
parallelism
to
distribute
the
work
across
multiple
compute
nodes
we
naturally
choose
the
loops
with
the
highest
granularity
that
is
the
two
outermost
loops
that
iterate
over
the
time
series
line
1-2
and
3-13
in
algorithm
2
we
implement
simple
master-worker
framework
based
on
mpi
to
distribute
these
loops
to
dynamically
distribute
work
and
mitigate
load
imbal
ance
among
workers
we
adopt
self-scheduling
in
our
master
worker
framework
in
self-scheduling
the
master
accounts
and
dispatches
tasks
to
workers
each
worker
performs
assigned
tasks
and
once
it
completes
the
worker
asks
the
master
for
new
task
the
high-level
organization
of
the
inter-node
parallelism
is
as
follows
first
the
workers
execute
the
embedding
di
mension
phase
the
optimal
embedding
dimension
for
each
time
series
is
reported
back
to
the
master
once
the
first
200
input
library
and
target
time
series
embedding
dimension
e
time
lag
output
arrays
diatances
and
indices
for
lookup
//
all-to-all
distance
calculation
for
to
do
for
to
do
distancesli
for
1to
do
indic
ir
distancesli
j
distances|i
j
target[kt+i
library[kt+j
end
end
end
//
sorting
10
top_k
e+1
for
i«
1to
do
12
indices[i
partialsort(indices
distances
top_k
13
end
algorithm
4
knn
for
gpu
input
library
and
target
time
series
embedding
dimension
e
time
lag
output
arrays
diatances
and
indices
for
lookup
//
embedding
for
to
do
for
1to
do
libraryblockli
j
librarylit+j
targetblockli
j|
target[it+j
end
end
//
all-to-all
distance
calculation
and
sorting
top_k
e+1
copy
libraryblock
and
targetblock
to
device
indices
distances
<«
nearestneighbour(libraryblock
tar
get
block
top_k
copy
indices
and
distances
to
host
psr
10
phase
is
complete
the
master
broadcasts
opte
to
all
workers
subsequently
the
workers
execute
the
all-to-all
ccm
phase
the
final
results
are
written
to
the
file
system
by
each
worker
to
alleviate
the
load
on
the
master
both
the
input
dataset
and
the
inferred
causal
map
are
stored
as
hdf5
25
files
for
easy
integration
with
the
pre/post
processing
workflow
the
workers
read
the
input
hdfs5
file
in
parallel
and
keep
the
entire
dataset
on
memory
during
the
execution
every
time
worker
completes
cross
map
the
worker
writes
an
element
of
the
causal
map
asynchronously
to
the
output
hdf5
file
this
small
random
write
pattern
however
is
known
to
be
slow
on
parallel
file
systems
in
fact
authorized
licensed
use
limited
to
boston
university
downloaded
on
may
19,2021
at
00:11:38
utc
from
ieee
xplore
restrictions
apply
algorithm
5
lookup
input
array
of
indices
and
distances
target
time
series
embedding
dimension
of
target
output
prediction
of
the
time
series
prediction
for
to
do
prediction[i
for
1to
e+1
do
idx
indicesli
j
ances|i
j
prediction[i
predictionli
target[idx
dist
end
end
we
observed
that
write
i/o
becomes
significant
bottleneck
of
the
application
on
gpfs
we
therefore
take
advantage
of
beeond
beegfs
on
demand
26
the
burst
buffer
deployed
on
abcil
beeond
combines
local
ssds
installed
on
the
compute
nodes
and
provides
an
on-demand
parallel
file
system
to
job
the
workers
write
the
results
to
beeond
to
minimize
i/o
overhead
d
intra-node
parallelism
we
focus
our
efforts
to
parallelize
and
optimize
the
knn
kernel
since
it
is
the
primary
bottleneck
in
cppedm
as
dis
cussed
in
section
i1i-a
we
design
and
implement
knn
kernels
for
both
cpu
and
gpu
architecture
to
ensure
that
mpedm
can
efficiently
run
on
wide
variety
of
computing
platforms
in
the
knn
kernel
for
cpu
shown
in
algorithm
3
the
two
loops
that
iterate
over
the
time
steps
within
time
series
are
parallelized
using
openmp
line
1-9
and
10-13
in
algorithm
3
we
also
utilize
openmp
4.0
simd
directives
to
vectorize
the
innermost
loop
explicitly
note
that
the
nested
loops
are
ordered
such
that
the
memory
accesses
in
the
innermost
loop
are
contiguous
in
the
knn
kernel
for
gpu
shown
in
algorithm
4
we
take
advantage
of
arrayfire
27
highly
optimized
library
for
gpu-accelerated
computing
arrayfire
provides
backends
for
cuda
opencl
and
cpu
but
in
this
paper
we
only
use
the
cuda
backend
since
abci
is
installed
with
tesla
v100
gpus
the
knn
algorithm
implemented
in
arrayfire
is
essentially
the
same
as
our
cpu
implementation
arrayfire
uses
block-wide
parallel
radix
sort
implementation
in
the
cuda
unbound
cub
template
library
since
each
abci
compute
node
is
equipped
with
four
gpus
we
also
distribute
the
work
across
multiple
gpus
to
achieve
this
the
loop
that
iterates
over
line
4-7
in
algorithm
2
is
parallelized
such
that
each
gpu
computes
lookup
tables
for
one
or
more
e
we
dynamically
schedule
this
loop
to
ensure
load
balancing
across
gpus
because
the
runtime
of
the
knn
kernel
depends
on
as
discussed
in
section
iii-b
for
the
lookup
kernel
shown
in
algorithm
5
we
currently
only
have
cpu
version
of
this
kernel
the
time
step
loop
is
parallelized
using
openmp
line
1-8
in
algorithm
5
this
kernel
is
heavily
memory
bandwidth
bound
since
it
requires
random
memory
access
iv
evaluation
the
computational
performance
of
mpedm
was
evaluated
on
abci
furthermore
we
present
the
scientific
outcomes
obtained
using
mpedm
a
evaluation
environment
abci
28
is
the
world’s
first
large-scale
open
al
com
puting
infrastructure
which
is
constructed
and
operated
by
the
national
institute
of
advanced
industrial
science
and
technology
aist
according
to
the
latest
tops00
list
published
in
november
2019
29
abcl
is
the
most
powerful
supercomputer
in
japan
and
the
8th
in
the
world
abci
has
1,088
compute
nodes
each
equipped
with
two
20-core
intel
xeon
gold
6148
cpus
four
nvidia
tesla
v100
sxm2
16gb
gpus
384gb
of
ram
and
1.6tb
of
local
nvme
ssd
the
parallel
file
system
is
based
on
gpfs
with
total
capacity
of
22pb
b
performance
evaluation
we
compared
mpedm
with
cppedm
from
the
following
three
aspects
total
runtime
parallel
scalability
and
impact
of
dataset
size
on
the
runtime
we
used
three
real-world
datasets
recorded
from
larval
zebrafish
under
different
conditions
table
shows
the
list
of
datasets
used
in
the
evaluation
table
i
datasets
used
in
the
evaluation
dataset
of
time
steps
of
time
series
size
fish1_normo
1,450
53.053
0.7gb
subject6
3,780
92,538
3.0gb
subject11
8,528
101,729
9.5
gb
1
total
runtime
mpedm
shows
significantly
higher
per
formance
compared
to
cppedm
table
ii
shows
the
perfor
mance
comparison
between
cppedm
and
mpedm
cppedm
took
8.5
hours
to
analyze
the
fishl
normo
dataset
using
512
abci
nodes
4
whereas
mpedm
took
only
20
seconds
to
analyze
the
same
dataset
using
512
abci
nodes
with
gpu
architecture
the
result
shows
that
mpedm
is
1,530x
faster
than
cppedm
moreover
mpedm
finished
the
causal
inference
of
two
larger
datasets
subject6
in
101
seconds
and
subjectl1
6
in
199
seconds
table
1i
performance
comparison
between
cppedm
and
mpedm
cppedm
mpedm
dataset
512
nodes
node
512
nodes
fish1_normo
8.5h
1,973s
20s
subject6
n/a
13953s
101s
subject11
n/a
39,572s
199s
authorized
licensed
use
limited
to
boston
university
downloaded
on
may
19,2021
at
00:11:38
utc
from
ieee
xplore
restrictions
apply
1,400
t
cpu
1200
gru||
1,000
|
800
|
600
|
400
200
%_
71,201
%1,81,76
92,64
720
<54
5
fig
2
strong
scaling
performance
absolute
runtime
2
parallel
scalability
we
measured
the
parallel
scalability
of
mpedm
by
varying
the
number
of
workers
and
measuring
the
runtime
of
mpedm
with
and
without
gpu
we
used
the
largest
subjectl
dataset
in
this
evaluation
figure
shows
the
strong
scaling
performance
of
mpedm
in
the
single
node
setup
mpedm
is
executed
on
single
node
without
mpl
in
the
workers
setup
mpedm
is
executed
with
mpi
using
the
specified
number
of
workers
we
measured
up
to
511
workers
since
abci
allows
maximum
of
512
nodes
per
job
except
for
jobs
running
under
the
abci
grand
challenge
program
which
can
use
the
full
1,088
nodes
the
result
shows
that
the
gpu
version
runs
as
twice
as
fast
as
the
cpu
version
in
every
case
we
noticed
that
the
cpu
version
ran
in
the
single
worker
setup
10%
slower
than
the
single
node
setup
we
believe
this
slowdown
is
caused
from
the
interference
between
the
background
tasks
performed
by
the
beeond
daemon
and
the
computation
in
mpedm
this
does
not
happen
with
the
gpu
version
because
the
average
cpu
utilization
is
lower
than
the
cpu
version
figure
shows
the
relative
speedup
of
the
multi-node
setup
in
relation
to
the
single
node
setup
it
reveals
that
the
speedup
is
nearly
linear
with
both
gpu
and
cpu
however
the
speedup
of
the
gpu
version
drops
when
the
number
of
nodes
is
64
or
more
we
measured
the
breakdown
of
each
phase
to
investigate
the
cause
behind
the
scalability
decline
we
compared
32
workers
and
128
workers
since
the
gpu
version
declines
beyond
64
nodes
figures
and
show
the
breakdown
of
average
runtime
for
processing
single
time
series
in
simplex
projection
and
ccm
the
two
figures
clearly
indicate
that
memory
copy
mpi
communication
and
i/o
are
not
bottlenecks
and
do
not
significantly
increase
with
the
number
of
workers
however
the
knn
function
becomes
slower
when
the
number
of
workers
increases
we
found
out
that
the
knn
search
for
the
first
time
series
processed
on
worker
is
significantly
slower
ranging
from
3.3
seconds
to
16.4
seconds
than
the
subsequent
ones
we
believe
this
is
caused
by
the
initialization
process
of
the
gpus
202
512
256
128
64
32
16
1%
12
16
32
64
128
256
512
number
of
workers
fig
3
strong
scaling
performance
relative
speedup
40
1.80
35
160
0k
1.40
poo
120
25
1%
1.00
k5
20
0.80
feef
45
fef
0.60
k
040
fof
lol
0.20
0.00
knn
lookup
memory
mpi
copy
==z
32
nodes
=
128
nodes
fig
4
breakdown
of
simplex
projection
average
runtime
per
time
series
to
verify
this
we
created
simple
program
that
initializes
the
gpus
and
allocates
some
gpu
memory
on
single
node
we
submitted
job
that
run
this
program
100
times
and
measured
the
initialization
time
the
result
revealed
that
the
initialization
time
follows
long-tailed
distribution
the
median
was
4.6
seconds
while
the
maximum
was
22.9
seconds
this
suggests
that
few
stragglers
impact
the
total
runtime
and
degrade
the
scalability
as
the
number
of
workers
increases
3
impact
of
dataset
size
we
evaluated
how
the
size
of
the
dataset
impacts
the
runtime
of
mpedm
using
dummy
datasets
with
different
sizes
furthermore
we
measured
the
time
spent
in
each
function
we
also
measured
the
speedup
of
the
gpu
version
over
the
cpu
version
with
varying
number
of
time
steps
figures
and
show
the
runtime
of
mpedm
when
increas
ing
the
number
of
time
series
and
time
steps
respectively
we
confirmed
that
the
increase
of
runtime
is
not
bigger
than
the
increase
predicted
from
the
time
complexity
we
also
confirmed
that
ccm
consumed
the
majority
of
the
total
runtime
and
other
tasks
including
i/0
and
mpi
communication
authorized
licensed
use
limited
to
boston
university
downloaded
on
may
19,2021
at
00:11:38
utc
from
ieee
xplore
restrictions
apply
600
3.00
—
500
|
250
<
<
<
400
|
200
o
300
|
150
il
<
<
200
1.00
<
<
100
050
<
<
<
0.00
bl
knn
lookup
memorympi
10
copy
oy
32
nodes
=1
128
nodes
fig
5
breakdown
of
cross
mapping
average
runtime
per
time
series
1,000
===x1
simplex
projection
900
c
ccm
others
800
700
600
500
400
300
200
100
1,000
5000
10,000
50,000
100,000
number
of
time
series
fig
6
runtime
with
varying
number
of
time
series
10,000
time
steps
are
ignorable
figures
8a
and
8b
show
the
runtime
breakdown
of
each
function
in
ccm
when
increasing
the
number
of
time
series
and
time
steps
respectively
figure
8a
shows
that
the
runtime
of
the
lookup
function
becomes
dominant
when
increasing
the
number
of
time
series
on
the
other
hand
fig
8b
shows
that
the
runtime
of
the
knn
function
becomes
dominant
when
increasing
the
number
of
time
steps
these
trends
can
be
explained
from
the
time
complexity
analysis
of
each
algorithm
described
in
section
iii-b
figure
shows
the
speedup
of
the
gpu
version
over
the
cpu
version
when
varying
number
of
time
steps
we
compared
the
performance
between
single
cpu
socket
and
one
or
more
gpus
to
evaluate
the
gpu
speedup
evidently
the
gpu
speedup
increases
with
the
number
of
time
steps
single
gpu
is
slower
than
the
cpu
if
the
number
of
time
steps
is
2,000
or
less
this
is
because
of
the
overhead
inherent
to
offloading
computation
to
the
gpu
however
single
gpu
consistently
surpasses
the
cpu
if
the
number
of
time
steps
203
900
===3
pimslex
projection
800
=——=
ccm
c
others
700
600
~
500
|
400
300
200
20k
88
100
5k
288k
§x§§
1,000
2,000
5000
10,000
20,000
40,000
number
of
time
stess
fig
7
runtime
with
varying
number
of
time
steps
1,000
time
series
100
100
80
(
80
155
sr
162
6o
fee
rt
ol
sr
sr
sr
20
pt
20
sr
sr
162
lb
ledes
&
7s
709,80
70
9,°
%
0%
%%
%,%,%
number
of
time
series
number
of
ima
steps
==za
knn
c—1
lookup
a
varying
number
of
time
series
b
varying
number
of
time
steps
10,000
time
steps
1,000
time
series
fig
8
breakdown
of
ccm
10°
t==za
single
cpu
single
gpu
c
2gpus
==
gpus
104
||
=
gpus
10
102
10
number
of
time
steps
fig
9
gpu
speedup
with
varying
number
of
time
steps
1,000
time
series
authorized
licensed
use
limited
to
boston
university
downloaded
on
may
19,2021
at
00:11:38
utc
from
ieee
xplore
restrictions
apply
10
normoxia
hypoxia
ors
l
15
embedding
dimension
hypoxia
fig
10
scientific
results
a
zebrafish
larvae
were
imaged
to
study
their
response
to
low
oxygen
b
the
day
larvae
were
imaged
using
spim
lightsheet
microscope
and
whole
brain
calcium
activity
was
recorded
at
single
cell
resolution
c
our
calculation
of
the
dimensionality
of
the
neuronal
populations
show
decrease
under
low
oxygen
hypoxia
as
seen
in
the
distribution
d
measured
transitions
between
normal
oxygen
concentrations
normoxia
to
hypoxia
show
bias
below
to
the
right
of
the
diagonal
line
showing
that
dimensionality
decreases
as
oxygen
decreases
e
f
whole
brain
ccm
all
vs
all
causal
inference
matrix
of
an
all
vs
all
neurons
results
show
more
homogeneous
map
in
hypoxia
f
than
normoxia
e
indicating
simplification
of
behavior
consistent
with
the
above
dimensionality
drop
g
an
identified
signal
integration
manifold
capable
of
predicting
turns
of
the
fish
at
least
0.5
seconds
a
single
time
step
ahead
of
time
whenever
the
neural
activity
trajectory
enters
one
of
the
loops
of
the
manifold
the
fish
will
turn
if
5,000
or
more
if
the
number
of
time
steps
is
40,000
the
speedup
of
single
gpu
is
3.5
times
compared
to
cpu
when
four
gpus
are
used
the
speed
up
is
13.4
times
c
scientific
outcomes
figure
10
shows
the
scientific
outcomes
obtained
using
mpedm
our
results
showed
that
we
could
determine
the
causal
connectivity
across
the
entire
brain
across
two
be
haviors
this
shows
that
depending
on
task
the
network
of
relationships
between
individual
neurons
change
and
become
more
connected
homogeneous
and
simplified
with
goal
di
rected
task
in
the
resulting
network
connectivity
increased
and
became
simpler
furthermore
we
were
able
identify
individual
neurons
that
integrate
signals
from
multiple
other
neurons
that
contain
decision
making
information
these
neurons
allow
the
prediction
of
fish
turn
behaviors
while
swimming
and
generate
low
dimensional
manifold
models
based
on
data
geometry
that
are
able
to
predict
the
fish’s
behavior
at
least
0.5
seconds
a
204
single
time
step
ahead
three
dimensional
projection
of
one
of
these
manifolds
is
shown
in
fig
10
g
where
entering
the
loop
predicts
turn
behavior
based
on
the
combined
activity
of
two
neurons
and
information
on
prior
states
we
are
able
to
predict
when
the
fish
will
turn
beyond
this
this
is
the
first
map
of
causal
connectivity
of
any
vertebrate
animal
at
single
neuron
resolution
v
conclusion
future
work
edm
is
nonlinear
time
series
analysis
framework
proven
its
applicability
in
various
fields
however
edm
has
only
been
applied
to
small
datasets
due
to
its
computational
cost
in
this
paper
we
designed
and
implemented
mpedm
parallel
distributed
implementation
of
edm
optimized
for
execution
on
modern
gpu-centric
supercomputers
mpedm
improves
the
edm
algorithm
to
reduce
redundant
computation
and
optimizes
the
implementation
to
fully
utilize
hardware
resources
such
as
gpus
and
simd
units
mpedm
took
only
authorized
licensed
use
limited
to
boston
university
downloaded
on
may
19,2021
at
00:11:38
utc
from
ieee
xplore
restrictions
apply
20
seconds
to
finish
the
causal
inference
of
dataset
containing
the
activity
of
53,053
zebrafish
neurons
on
512
abci
nodes
this
is
1,530x
faster
than
cppedm
the
current
standard
implementation
of
edm
moreover
mpedm
could
analyze
13x
larger
dataset
in
199
seconds
this
is
the
largest
edm
causal
inference
achieved
to
date
we
will
continue
to
optimize
the
performance
of
mpedm
as
discussed
in
section
iv-b
we
need
to
improve
the
perfor
mance
of
the
lookup
as
it
becomes
the
primary
bottleneck
when
we
scale
up
the
number
of
time
series
further
we
will
also
explore
other
efficient
implementations
of
nearest
neighbor
search
on
gpus
currently
mpedm
uses
the
exact
knn
search
implementation
provided
by
arrayfire
there
exist
many
studies
on
efficient
approximate
nearest
neighbor
ann
search
30
31
however
it
is
unclear
how
ann
affects
the
accuracy
of
edm
predictions
another
well-known
approach
is
to
use
spatial
indices
such
as
kd-trees
and
ball
trees
to
accelerate
knn
search
32
33
additionally
edm
algorithms
other
than
simplex
projection
and
ccm
will
be
implemented
in
mpedm
to
expand
mpedm
to
standard
implementation
of
edm
on
hpc
systems
we
will
make
this
edm
library
widely
available
to
the
community
with
hope
to
assist
scientists
in
need
to
analyze
large-scale
time
series
datasets
of
nonlinear
dynamical
systems
acknowledgment
this
work
was
supported
by
jsps
kakenhi
grant
num
ber
jp20k19808
kt
and
an
innovation
grant
by
the
kavli
institute
for
brain
and
mind
gmp
computational
resources
of
the
al
bridging
cloud
infrastructure
abci
were
provided
by
the
national
institute
of
advanced
industrial
science
and
technology
aist
references
1
r
gamanuf
h
kennedy
z
toroczkai
m
ercsey-ravasz
d
c
van
essen
k
knoblauch
and
a
burkhalter
the
mouse
cortical
connectome
characterized
by
an
ultra-dense
cortical
graph
maintains
specificity
by
distinct
connectivity
profiles
neuron
vol
97
no
3
pp
698-715
2018
2
e.r
deyle
and
g
sugihara
generalized
theorems
for
nonlinear
state
space
reconstruction
plos
one
vol
6
no
3
2011
3
g
sugihara
r
may
h
ye
c.-h
hsieh
e
deyle
m
fogarty
and
s
munch
detecting
causality
in
complex
ecosystems
science
vol
338
no
6106
pp
496-500
2012
4
j
park
g
m
pao
e
saberski
c
smith
j
haga
r
takano
c
m
yeh
s
chalasani
and
g
sugihara
massively
parallel
empirical
dynamic
cross
mapping
in
37th
meeting
of
the
pacific
rim
applications
and
grid
middleware
assembly
pragma37
2019
5
m
b
ahrens
m
b
orger
d
n
robson
j
m
li
and
p
j
keller
whole-brain
functional
imaging
at
cellular
resolution
using
light-sheet
microscopy
nature
methods
vol
10
no
5
p
413
2013
6
x
chen
y
mu
y
hu
a
t
kuan
m
nikitchenko
o
randlett
a
b
chen
j
p
gavornik
h
sompolinsky
f
engert
et
al
brain
wide
organization
of
neuronal
activity
and
convergent
sensorimotor
transformations
in
larval
zebrafish
neuron
vol
100
no
4
pp
876
890
2018
7
a
t
clark
h
ye
f
isbell
e
r
deyle
j
cowles
g
d
tilman
and
g
sugihara
spatial
convergent
cross
mapping
to
detect
causal
relationships
from
short
time
series
ecology
vol
96
no
5
pp
1174
1181
2015
8
h
ye
e
r
deyle
l
j
gilarranz
and
g
sugihara
distinguishing
time-delayed
causal
interactions
using
convergent
cross
mapping
sci
entific
reports
vol
5
no
14750
pp
1-9
2015
205
20
21
22
24
25
26
27
28
30
31
32
33
g
sugihara
and
r
m
may
nonlinear
forecasting
as
way
of
distinguishing
chaos
from
measurement
error
in
time
series
nature
vol
344
no
6268
pp
734-741
1990
e
takens
detecting
strange
attractors
in
turbulence
in
dynamical
systems
and
turbulence
warwick
1980
pp
366-381
springer
1981
k
schiecke
b
pester
m
feucht
l
leistritz
and
h
witte
conver
gent
cross
mapping
basic
concept
influence
of
estimation
parameters
and
practical
application
in
37th
annual
international
conference
of
the
engineering
in
medicine
and
biology
society
embc
pp
7418
7421
2015
j
pearl
causal
inference
in
statistics
an
overview
statistics
surveys
vol
3
pp
96-146
2009
c
w
granger
investigating
causal
relations
by
econometric
models
and
cross-spectral
methods
econometrica
journal
of
the
econometric
society
pp
424-438
1969
s
tajima
t
yanagawa
n
fujii
and
t
toyoizumi
untangling
brain
wide
dynamics
in
consciousness
by
cross-embedding
plos
computa
tional
biology
vol
11
no
11
2015
c.-w
chang
m
ushio
and
c.-h
hsieh
empirical
dynamic
modeling
for
beginners
ecological
research
vol
32
no
6
pp
785-796
2017
h
natsukawa
and
k
koyamada
visual
analytics
of
brain
effective
connectivity
using
convergent
cross
mapping
in
siggraph
asia
2017
symposium
on
visualization
pp
1-9
2017
e
grziwotz
j
f
straub
c.-h
hsich
and
a
telschow
empirical
dynamic
modelling
identifies
different
responses
of
aedes
polynesiensis
subpopulations
to
natural
environmental
variables
scientific
reports
vol
8
no
1
pp
1-10
2018
j
ma
m
yang
x
han
and
z
li
ultra-short-term
wind
generation
forecast
based
on
multivariate
empirical
dynamic
modeling
ieee
transactions
on
industry
applications
vol
54
no
2
pp
1029-1038
2017
c
n
anderson
c.-h
hsieh
s
a
sandin
r
hewitt
a
hollowed
j
beddington
r
m
may
and
g
sugihara
why
fishing
magnifies
fluctuations
in
fish
abundance
nature
vol
452
no
7189
pp
835
839
2008
g
sugihara
nonlinear
forecasting
for
the
classification
of
natural
time
ilosophical
transactions
of
the
royal
society
of
london
series
a
physical
and
engineering
sciences
vol
348
no
1688
pp
477-495
1994
c
luo
x
zheng
and
d
zeng
causal
inference
in
social
media
using
convergent
cross
mapping
in
2014
joint
intelligence
and
security
informatics
conference
pp
260-263
2014
cppedm
library
https://github.com/sugiharalab/cppedm
h
ye
a
clark
e
deyle
and
g
sugihara
redm
an
package
for
empirical
dynamic
modeling
and
convergent
cross-mapping
https
/lgithub.com/sugiharalab/fedm
2016
pyedm
library
https://github.com/sugiharalab/pyedm
m
folk
a
cheng
and
k
yates
hdf5
file
format
and
1/o
library
for
high
performance
computing
applications
in
international
conference
for
high
performance
computing
networking
storage
and
analysis
vol
99
pp
5-33
1999
beeond
beegfs
on
demand
https://www.beegfs.io/wiki/beeond
j
malcolm
p
yalamanchili
c
mcclanahan
v
venugopalakrishnan
k
patel
and
j
melonakos
arrayfire
gpu
acceleration
platform
in
modeling
and
simulation
for
defense
systems
and
applications
vii
vol
8403
pp
49-56
2012
abci
official
website
https://abci.ai/
top500
list
november
2019
https://top500.org/lists/2019/11/
w
chen
j
chen
f
zou
y.-f
li
p
lu
and
w
zhao
robustiq
robust
ann
search
method
for
billion-scale
similarity
search
on
gpus
in
proceedings
of
the
2019
on
international
conference
on
multimedia
retrieval
pp
132140
2019
j
pan
and
d
manocha
fast
gpu-based
locality
sensitive
hashing
for
k-nearest
neighbor
computation
in
proceedings
of
the
19th
acm
sigspatial
international
conference
on
advances
in
geographic
infor
mation
systems
pp
211-220
2011
v
garcia
e
debreuve
and
m
barlaud
fast
nearest
neighbor
search
using
gpu
in
2008
ieee
computer
society
conference
on
computer
vision
and
pattern
recognition
workshops
pp
1-6
2008
m
r
abbasifard
b
ghahremani
and
h
naderi
a
survey
on
nearest
neighbor
search
methods
international
journal
of
computer
applications
vol
95
no
25
pp
39-52
2014
authorized
licensed
use
limited
to
boston
university
downloaded
on
may
19,2021
at
00:11:38
utc
from
ieee
xplore
restrictions
apply
