arxiv:2011.01797v1
cs.lg
nov
2020
doubly
robust
off-policy
learning
on
low-dimensional
manifolds
by
deep
neural
networks
minshuo
chen
haoliu
wenjing
liao
tuo
zhao
abstract
causal
inference
explores
the
causation
between
actions
and
the
consequent
rewards
on
covariate
set
recently
deep
learning
has
achieved
remarkable
performance
in
causal
inference
but
existing
statistical
theories
cannot
well
explain
such
an
empirical
success
especially
when
the
covariates
are
high-dimensional
most
theoretical
results
in
causal
inference
are
asymptotic
suffer
from
the
curse
of
dimensionality
and
only
work
for
the
finite-action
scenario
to
bridge
such
gap
between
theory
and
practice
this
paper
studies
doubly
robust
off-policy
learning
by
deep
neural
networks
when
the
covariates
lie
on
low-dimensional
manifold
we
prove
nonasymptotic
regret
bounds
which
converge
at
fast
rate
depending
on
the
intrinsic
dimension
of
the
manifold
our
results
cover
both
the
finite
and
continuous-action
scenarios
our
theory
shows
that
deep
neural
networks
are
adaptive
to
the
low-dimensional
geometric
structures
of
the
covariates
and
partially
explains
the
success
of
deep
learning
for
causal
inference
introduction
causal
inference
studies
the
causal
connection
between
actions
and
rewards
which
has
wide
applications
in
healthcare
kim
et
al
2011
lunceford
and
davidian
2004
digital
advertising
farias
and
li
2019
product
recommendation
sharma
et
al
2015
and
policy
formulation
heckman
and
vytlacil
2007
for
example
in
healthcare
each
patient
can
be
characterized
by
set
of
covariates
also
called
features
and
the
actions
are
set
of
treatments
each
patient
has
the
corresponding
reactions
or
rewards
to
different
treatments
causal
inference
enables
one
to
personalize
the
treatment
to
each
patient
to
maximize
the
total
rewards
such
personalized
decision-making
rule
is
referred
to
as
policy
which
is
map
from
the
covariate
set
to
the
action
set
in
off-policy
learning
batch
of
observational
data
is
given
which
typically
consists
of
covariate
also
called
feature
the
action
taken
e.g
medical
treatments
and
recommendations
and
the
observed
reward
in
this
paper
we
are
interested
in
learning
an
optimal
policy
that
targets
personalized
treatments
or
services
to
different
individuals
based
on
the
logged
data
this
is
also
known
as
the
optimal
treatment
assignment
in
literature
rubin
1974
heckman
1977
conventional
causal
inference
methods
often
rely
on
parametric
models
lunceford
and
david
ian
2004
cao
et
al
2009
robins
et
al
1994
kitagawa
and
tetenov
2018
which
can
introduce
large
bias
when
the
real
model
is
not
in
the
assumed
parametric
form
many
nonparametric
methods
are
proposed
hill
2011
kitagawa
and
tetenov
2018
zhao
et
al
2015
kennedy
et
al
2017
richardson
et
al
2014
chan
et
al
2016
frolich
et
al
2017
benkeser
et
al
2020
kennedy
work
in
progress
fminshuo
chen
and
hao
liu
contribute
equally
minshuo
chen
and
tuo
zhao
are
affiliated
with
the
isye
depart
ment
at
georgia
tech
hao
liu
and
wenjing
liao
are
affiliated
with
the
math
department
at
georgia
tech
email
{mchen393
wliao60
tzhao80}@gatech.edu
hao.liu@math.gatech.edu
2020
lee
et
al
2020
crump
et
al
2008
benkeser
et
al
2017
while
the
statistical
theories
often
suffer
from
the
curse
of
dimensionality
recently
neural
networks
became
popular
modeling
tool
for
causal
inference
many
results
have
shown
that
neural
networks
outperform
conventional
nonparametric
approaches
especially
when
the
learning
task
involves
high-dimensional
complex
data
for
example
lopez-paz
et
al
2017
proposed
to
discover
causal
and
anticausal
features
in
images
from
imagenet
using
20-layer
residual
network
pham
and
shen
2017
used
recurrent
neural
networks
to
study
the
causality
between
group
forming
loans
and
the
funding
time
on
an
online
non-profit
financial
platform
other
examples
can
be
found
in
diverse
areas
including
climate
analysis
medical
diagnosis
cognitive
science
and
online
recommendations
chalupka
et
al
2014
van
amsterdam
et
al
2019
johansson
et
al
2016
hartford
et
al
2017
zhang
et
al
2019
lim
2018
despite
the
great
progress
of
causal
inference
there
is
still
huge
gap
between
theory
and
prac
tice
in
casual
inference
many
existing
theories
on
nonparametric
or
neural
networks
approaches
are
asymptotic
and
suffer
from
the
curse
of
dimensionality
specifically
to
achieve
an
accuracy
the
sample
complexity
needs
to
grow
in
the
order
of
€
where
is
the
covariate
dimension
such
theories
can
not
explain
the
empirical
success
when
is
large
for
example
in
lopez-paz
et
al
2017
the
rgb
images
in
imagenet
are
of
resolution
x224x
224
to
obtain
0.1
error
the
sample
complexity
needs
to
scale
like
1073*?24x224
which
well
exceeds
the
training
size
of
99,309
besides
the
curse
of
dimensionality
is
inevitable
unless
additional
data
structures
are
considered
gao
and
han
2020
proved
that
for
binary
policy
learning
problems
the
sample
complexity
obtained
by
the
optimal
algorithm
still
grows
exponentially
in
the
covariate
dimension
in
the
order
of
2
to
bridge
this
gap
we
take
the
low-dimensional
geometric
structures
of
the
covariates
into
consideration
this
is
motivated
by
the
fact
that
real-world
data
often
exhibit
low-dimensional
structures
due
to
rich
local
regularities
global
symmetries
or
repetitive
patterns
tenenbaum
et
al
2000
roweis
and
saul
2000
peyré
2009
for
example
many
images
describe
the
same
object
with
different
transformations
like
translation
rotation
projection
and
skeletonization
these
transformations
are
often
represented
by
small
number
of
parameters
including
the
translation
position
the
rotation
and
the
projection
angle
etc
similar
low-dimensional
structures
exist
in
medical
data
choi
et
al
2016
mahoney
and
drineas
2009
and
financial
data
baptista
etal
2000
to
incorporate
such
low-dimensional
structures
of
data
we
assume
that
the
input
covariates
are
concentrated
on
d-dimensional
riemannian
manifold
embedded
in
rp
with
d
in
many
off-policy
learning
methods
policy
evaluation
plays
an
important
role
by
evaluating
the
expected
reward
of
given
policy
most
of
the
existing
works
dedicate
to
policy
evaluation
with
finite
actions
only
few
works
addressed
the
continuous
action
scenario
kallus
and
zhou
2018b
demirer
et
al
2019
kallus
and
santacatterina
2019
among
the
policy
evaluation
methods
with
finite
actions
the
doubly
robust
method
cassel
et
al
1976
robins
et
al
1994
dudik
et
al
2011
has
the
advantage
of
being
consistent
if
either
the
reward
function
or
the
propensity
score
the
probability
of
choosing
certain
action
given
the
covariate
is
correctly
specified
the
statistical
theory
for
policy
evaluation
has
been
intensively
studied
in
the
past
with
many
asymptotic
results
swaminathan
and
joachims
2015
zhou
et
al
2018
zhao
et
al
2012
kallus
and
zhou
2018a
kitagawa
and
tetenov
2018
this
paper
establishes
statistical
guarantees
of
policy
learning
in
causal
inference
using
neural
networks
we
consider
the
doubly
robust
method
see
section
for
details
and
use
deep
relu
neural
networks
to
parameterize
the
policy
class
the
propensity
score
and
the
conditional
expected
reward
we
prove
nonasymptotic
regret
bounds
which
converge
at
fast
rate
depending
on
the
intrinsic
dimension
d
instead
of
the
covariate
dimension
d
furthermore
our
theory
applies
to
both
the
finite-action
and
continuous-action
scenarios
this
paper
has
three
main
contributions
1
by
taking
the
low-dimensional
geometric
struc
tures
of
the
covariates
into
consideration
we
prove
fast
convergence
rate
of
the
learned
policy
depending
on
the
intrinsic
dimension
of
the
covariates
2
our
statistical
theory
is
nonasymptotic
for
policy
learning
while
most
existing
works
established
asymptotic
theories
for
policy
evaluation
using
the
doubly
robust
method
while
policy
evaluation
gives
rise
to
the
performance
of
any
specific
policy
policy
learning
furhter
returns
an
optimal
policy
3
to
our
best
knowledge
we
prove
the
first
regret
bound
of
policy
learning
in
continuous
action
space
related
work
in
off-policy
learning
one
line
of
research
learns
the
optimal
policy
by
evaluating
the
expected
reward
of
candidate
policies
and
then
finding
the
policy
with
the
largest
expected
reward
the
procedure
of
evaluating
target
policy
from
the
given
data
is
called
off-policy
evaluation
which
has
been
intensively
studied
in
literature
the
simplest
way
to
evaluate
policy
is
the
direct
method
which
estimates
the
empirical
reward
of
the
target
policy
from
collected
data
beygelzimer
and
langford
2009
the
direct
method
is
unbiased
if
one
specifies
the
reward
model
correctly
however
model
specification
is
difficult
task
in
practice
another
method
is
the
inverse
propensity
weighting
horvitz
and
thompson
1952
robins
et
al
1994
which
uses
the
importance
weighting
to
correct
the
mismatch
between
the
propensity
scores
of
the
target
policy
and
the
data
collection
policy
this
method
is
unbiased
if
the
data
collection
policy
can
be
exactly
estimated
yet
it
has
large
variance
especially
when
some
actions
are
rarely
observed
more
robust
method
is
the
doubly
robust
method
cassel
et
al
1976
cao
et
al
2009
dudik
et
al
2011
which
integrates
the
direct
method
and
the
inverse
propensity
weighting
this
method
is
unbiased
if
the
reward
model
is
correctly
specified
or
the
data
collection
policy
is
known
the
aforementioned
methods
have
been
used
in
kitagawa
and
tetenov
2018
zhao
et
al
2015
athey
and
wager
2017
zhou
et
al
2018
for
off-policy
learning
kitagawa
and
tetenov
2018
used
the
inverse
propensity
weighting
and
athey
and
wager
2017
and
zhao
et
al
2015
used
the
doubly
robust
method
to
learn
the
optimal
policy
with
binary
actions
in
zhou
et
al
2018
an
algorithm
based
on
decision
trees
was
proposed
to
learn
the
optimal
policy
with
multiple
actions
using
the
doubly
robust
method
kallus
2018
proposed
balanced
method
which
minimizes
the
worst-case
conditional
mean
squared
error
to
evaluate
and
learn
the
optimal
policy
with
multiple
actions
another
line
of
research
learns
the
optimal
policy
without
evaluating
policies
in
zhang
et
al
2012
zhao
et
al
2012
the
authors
transformed
the
policy
learning
task
with
binary
actions
into
classification
problem
other
works
on
off-policy
learning
include
kallus
2020
ward
et
al
2019
and
bennett
and
kallus
2020
most
of
the
aforementioned
works
provide
asymptotic
regret
bounds
with
finite
actions
which
are
valid
when
the
number
of
samples
goes
to
infinity
nonasymptotic
bound
was
derived
in
kitagawa
and
tetenov
2018
but
this
work
requires
that
the
propensity
score
is
known
and
the
algorithm
only
works
for
policy
learning
with
binary
actions
meanwhile
off-policy
learning
with
continuous
actions
has
not
been
addressed
until
recently
kallus
and
zhou
2018b
demirer
et
al
2019
kallus
and
santacatterina
2019
demirer
et
al
2019
developed
semi-parametric
off-policy
learning
algorithm
which
requires
the
reward
function
in
specific
class
kallus
and
zhou
2018b
applied
kernel
method
to
extend
the
inverse
propensity
weighting
and
the
doubly
robust
method
to
the
continuous-action
setting
these
works
on
continuous
actions
did
not
provide
nonasymptotic
regret
bound
with
an
explicit
dependency
on
the
number
of
samples
the
rest
of
the
paper
is
organized
as
follows
section
introduces
manifold
and
neural
networks
section
presents
the
doubly
robust
estimation
framework
section
states
our
regret
bounds
of
the
learned
policy
section
gives
proof
sketch
of
our
theory
section
discusses
several
related
topics
notations
we
use
bold
lowercase
letters
to
denote
vectors
i.e
rp
we
use
x
to
denote
the
i-th
entry
of
&
and
define
|z|
2?:1
|x;|
and
||m||§
id:l
xiz
for
function
r®
and
multi-index
sy,...,5p]t
9°f
denotes
di*!f/dx}'---9x}°
let
q1
be
the
support
of
probability
distribution
ip
the
l
norm
of
with
respect
to
ip
is
denoted
as
||f||22
o
fz(:n)dll’(m
we
use
to
denote
function
composition
for
set
a
a|
denotes
its
cardinality
for
scalar
0
|a
denotes
the
largest
integer
which
is
no
larger
than
g4
a
denotes
the
smallest
integer
which
is
no
smaller
than
a
for
4,b
r
we
denote
vv
max{a
b}
and
min{a
b}
we
refer
to
one-hot
vector
as
canonical
basis
i.e
v
0,...,0,1,0,...,0
with
j-th
element
being
1
we
use
=
to
define
important
quantities
preliminaries
on
manifold
and
neural
networks
we
briefly
review
smooth
manifolds
see
lee
2003
and
tu
2010
for
more
details
holder
space
on
smooth
manifold
and
define
the
neural
network
class
considered
throughout
this
paper
2.1
low-dimensional
manifold
let
be
d-dimensional
riemannian
manifold
isometrically
embedded
in
irp
chart
for
is
pair
u
¢
such
that
is
open
and
is
homeomorphism
i.e
is
bijection
its
inverse
and
itself
are
continuous
two
charts
u
¢
and
v
are
called
c
compatible
if
and
only
if
the
transition
functions
gotb
p(unv
p(unv
and
odpl:p(unv)—=p(unv
are
both
c
functions
c
atlas
of
is
collection
of
c¥
compatible
charts
{(uj
}
such
that
u
ui
m
an
atlas
of
contains
an
open
cover
of
and
the
mappings
from
each
open
cover
to
r%
definition
smooth
manifold
manifold
is
smooth
if
it
has
c
atlas
through
the
concept
of
atlas
we
are
able
to
define
c
functions
and
holder
space
on
smooth
manifold
definition
c°
functions
on
m
let
be
smooth
manifold
and
fix
c
atlas
of
it
for
function
r
we
say
is
c
function
on
if
for
any
u
¢
in
the
atlas
fo¢p~lisa
c
function
in
r¥
definition
holder
space
on
m
let
be
compact
manifold
function
ir
belongs
to
the
holder
space
h%(m
with
holder
index
0
if
for
any
chart
u
¢
we
have
|9°f
p~
@)=
f
w)l
lf
llpe(uy
max
sup
|9°fo
@)+
max
sup
f
ai[ilj
lsl<fa-ngep(u
lsl=la11gayeq(u
llz
yll
q
for
fixed
atlas
{(u
}
the
holder
norm
of
is
defined
as
f”ha(m
sup
|fllxe
we
occasionally
omit
in
the
holder
norm
when
it
is
clear
from
the
context
we
introduce
the
reach
federer
1959
niyogi
et
al
2008
of
manifold
to
characterize
the
local
curvature
of
m
definition
reach
the
medial
axis
of
is
defined
as
t(m)={zerp
|3z
2
m
such
that
||x
z,||
|l
x|
inj\f/l||mfy||2}
ye
the
reach
of
is
the
minimum
distance
between
and
m
i.e
inf
yll
met(_i/\i/ll),ye,m”m
yll2
roughly
speaking
reach
measures
how
fast
manifold
bends
manifold
with
large
reach
bends
relatively
slowly
2.2
neural
network
we
focus
on
feedforward
neural
networks
with
the
relu
activation
function
relu(x
max{0
x}
when
the
argument
is
vector
or
matrix
relu
is
applied
entrywise
given
an
input
«
an
l-layer
network
computes
an
output
as
f(x)=w-relu
w
_1---reluwjx
+by
+---+
by
_1)+
by
2.1
where
the
w;’s
are
weight
matrices
and
b;’s
are
intercepts
we
define
class
of
neural
networks
as
f(l,p,k,x,r)={f
has
the
form
of
2.1
with
layers
and
width
bounded
by
p
||f|lcc
<r
iwillo
+11billo
k
1willooco
5
libylloo
for
1,...,l}
i=1
where
||h||
o
max
j|h;;|
for
matrix
and
||
||
denotes
the
number
of
non-zero
elements
of
its
argument
off-policy
learning
with
low-dimensional
covariates
we
introduce
two-stage
policy
learning
scheme
using
neural
networks
suppose
we
receive
iid
triples
{(z
a;,y;)}!_
where
x
denotes
covariate
independently
sampled
from
an
unknown
distribution
on
m
a
denotes
the
action
taken
and
y
is
the
observed
reward
to
incorporate
the
low-dimensional
geometric
structures
of
the
covariates
we
assume
is
d-dimensional
riemannian
manifold
isometrically
embedded
in
rp
the
action
space
can
be
either
finite
or
continuous
for
each
covariate
and
action
pair
x,a
there
is
an
associated
random
reward
we
adopt
the
unconfoundedness
assumption
to
simplify
the
model
which
is
commonly
used
in
existing
literature
on
causal
inference
wasserman
2013
zhou
et
al
2018
assumption
unconfoundedness
the
reward
is
independent
of
conditioned
on
x
to
better
interpret
assumption
1
we
first
consider
finite
action
space
={a
a\ai}
where
aj
is
one-hot
vector
i.e
a
=[0,...,0,1,0,...,0
with
appearing
at
the
j-th
position
given
the
covariate
@
there
is
reward
{y1(z
y|4/(z)}
for
each
action
where
the
randomness
of
y;(x
only
depends
on
the
observed
reward
y
is
realization
of
yj(z
with
a
a
3.1
policy
learning
with
finite
actions
when
the
action
space
is
finite
policy
maps
covariate
on
to
vector
on
the
|a|-dimensional
simplex
aw
{z
er:z
>0and
zz
1}
the
j-th
entry
of
7t(z
denotes
the
probability
of
choosing
the
action
a
given
z
policy
in
the
interior
of
the
simplex
is
called
randomized
policy
if
7(x
is
one-hot
vector
it
is
called
deterministic
policy
the
expected
reward
of
deploying
policy
is
q(r
e[y
n(@
e[([y
@
@
x
3.1
we
investigate
the
doubly
robust
approach
cassel
et
al
1976
robins
et
al
1994
dudik
et
al
2011
for
policy
learning
which
consists
of
two
stages
after
receiving
the
training
data
we
split
them
into
two
groups
sy
={(@jappll
and
={(xia;yi)ls
3.2
we
denote
n
n—mn
and
choose
17,1
to
be
proportional
to
such
that
n;/n
is
constant
in
the
first
stage
we
solve
nonparametric
regression
problems
using
s
to
estimate
two
important
functions
the
propensity
score
and
the
conditional
expected
reward
for
any
action
a}
the
propensity
score
ea](m
=pla=4;|z
quantifies
the
probability
of
choosing
a
given
the
covariate
x
and
the
expected
reward
of
choosing
a
is
ja
@
e[yj(z
substituting
the
definition
above
into
3.1
we
can
write
q
e([pua
@
pa
(
(
jm([m
@
pia
@
@)}
3.3
in
the
second
stage
we
learn
policy
using
s
based
on
our
estimated
s
and
ha
s
which
only
requires
that
either
the
e
s
or
the
ha
s
are
accurately
estimated
stage
1
estimating
ha
and
e
for
each
action
a
we
use
neural
network
to
estimate
the
reward
function
yi4
by
minimizing
the
following
empirical
quadratic
loss
fia
x
=argmin
3~
f(@
ula
=4
with
ny
=
ta;=aj
3.4
ferm
m4
where
fyn
ris
properly
chosen
network
class
defined
in
lemma
1
an
estimator
of
the
propensity
score
ey
is
obtained
by
minimizing
the
multinomial
logistic
loss
let
gan
rhi
be
properly
chosen
network
class
defined
in
lemma
1
we
obtain
a](m
via
a1
argmin
=
g(e
ey
+log(1+
explls(@l
5
8eonn
i=1
j=1
exp((g(2
ey
(
for
<|a|-1
and
ey
(
3.6
exp(e@
s+
exp((e@
here
g
denotes
the
j-th
entry
and
t,1
rmi
is
obtained
by
augmenting
by
1
stage
2
policy
learning
given
ia
and
a
we
learn
an
optimal
policy
by
maximizing
doubly
robust
empirical
reward
~
ha
t
qmi=
with
r,-:%ﬂf_)’”-a,-+[ﬂal<mi>,...,,4alal(m,-)]tewl
3.7
i=ny+1
doubly
robust
optimal
policy
is
learned
by
tipk
argmax
q(r
3.8
nellyn
where
iy
is
properly
chosen
network
class
see
section
for
the
configurations
of
i\
e.g
4.8
and
4.9
the
doubly
robust
reward
can
tolerate
relatively
large
estimation
error
in
either
faj
or
e}
see
the
discussion
after
theorem
1
3.2
policy
learning
with
continuous
actions
continuous
actions
e.g
doses
of
drugs
often
arise
in
applications
but
there
are
limited
studies
on
policy
learning
with
continuous
actions
in
this
paper
we
consider
the
continuous
action
space
a=[0,1
and
use
to
denote
an
action
when
the
random
action
takes
the
value
0,1
we
denote
y(x
a
as
its
random
reward
the
propensity
score
and
conditional
expected
reward
are
defined
analogously
to
the
finite
action
case
e(x,a
%p(ﬂ
<aaealxz
and
p(x,a)=e[y(x,a)|x
note
that
e(z
a
is
probability
density
function
in
this
scenario
we
can
learn
an
optimal
policy
by
replicating
the
two-stage
scheme
with
discretization
technique
on
the
continuous
action
space
specifically
we
uniformly
partition
the
action
space
into
sub-intervals
and
denote
i
j
1)/v,j/v]for
1
v
accordingly
we
define
the
discretized
propensity
score
and
conditional
expected
reward
for
the
sub-interval
i
as
e,](:n
=p(ael;j|z
and
y,](m
=e[y(z,a)l{ac
l}
m]/e,j
3.9
after
the
discretization
on
the
action
space
we
identify
all
the
actions
belonging
to
single
sub-interval
i
as
the
midpoint
a
2j
1)/2v
of
i
and
equips
a
with
the
average
expected
reward
pj
after
discretization
we
resemble
the
setup
in
the
finite-action
scenario
and
then
apply
the
aforementioned
two-stage
doubly
robust
approach
to
learn
discretized
policy
concentrated
on
the
aj’s
in
the
first
stage
we
obtain
71}
and
}
as
estimators
of
p
and
ey
respectively
in
the
second
stage
we
use
neural
networks
for
policy
learning
by
maximizing
the
discretized
doubly
robust
empirical
reward
specifically
we
define
i(a
i
for
g
i
which
maps
the
continuous
action
to
the
corresponding
discretized
sub-interval
for
a
i
we
denote
a
{0,1}v
as
the
one-hot
vector
with
the
j-th
element
being
1
which
encodes
the
action
a
the
discretized
doubly
robust
empirical
reward
is
defined
as
@
qpm
tp
mta
with
2b
a4
(
)
3.10
i=n;+1
el(“’)(mi
where
the
superscript
d
denotes
the
discretized
quantities
we
learn
an
optimal
policy
by
solving
the
following
maximization
problem
cc.dr
argmax
qp(x
3.11
tellnn
where
ilyy
is
properly
chosen
neural
network
see
section
4.2
for
more
details
of
the
learning
procedure
proper
choice
of
v
and
the
statistical
guarantees
of
the
learned
policy
main
results
our
main
results
are
nonasymptotic
regret
bounds
see
definition
5
on
the
policy
learned
by
the
two-stage
scheme
in
section
3
when
the
covariates
are
concentrated
on
low-dimensional
manifold
the
regret
of
policy
against
reference
policy
7t
is
defined
as
the
difference
between
their
respective
expected
rewards
the
formal
definition
is
given
as
follows
definition
5
let
77
be
fixed
reference
policy
for
any
policy
7
the
regret
of
against
is
r(7t
1
q(7
q(m
here
q(m
is
the
expected
reward
either
in
the
finite-action
scenario
defined
in
3.1
or
the
continuous-action
scenario
which
is
defined
later
in
4.15
we
consider
two
reference
policies
1
the
optimal
holder
policy
that
maximizes
the
expected
reward
2
the
unconstrained
optimal
policy
that
maximizes
the
expected
reward
we
establish
high
probability
bounds
on
the
regret
of
the
learned
policy
for
both
discrete
actions
section
4.1
and
continuous
actions
section
4.2
4.1
policy
learning
with
finite
actions
our
theory
is
based
on
the
following
assumptions
including
manifold
model
for
covariates
some
standard
assumptions
on
the
smoothness
of
the
propensity
score
and
the
reward
assumption
2
is
d-dimensional
compact
smooth
manifold
isometrically
embedded
in
rp
there
exists
such
that
||z||
whenever
m
the
reach
of
satisfies
0
assumption
a.3
the
propensity
score
and
random
reward
satisfy
i
owverlap
ea](m
for
j=1,...,|a|
where
11
is
constant
ii
bounded
reward
yj(z)is
bounded
and
has
bounded
variance
i.e
sup
|yj(z)|
m
and
var[y
o
for
any
j=1,...,|a|
where
m
>0
and
are
constants
assumption
a.3
is
standard
assumption
for
statistical
guarantees
of
all
learning
approaches
using
the
inverse
propensity
score
wasserman
2013
farrell
et
al
2018
zhou
et
al
2018
assumption
a.3
implies
that
expected
reward
ha
is
bounded
since
|}4ai(€e)|
<e[[yj(z)||
x
m
for
every
m
assumption
a.4
given
holder
index
1
we
assume
pa,(x
h*(m
and
x
hy(m
for
j=1,...,]a|
moreover
for
fixed
c
atlas
of
m
there
exists
m
such
that
max”ya]”h
<m
and
max||10geaj||ha
<m
thanks
to
assumption
a.3
i
e4
h®
implies
loges
h
see
lemma
in
appendix
g
now
we
are
ready
to
derive
the
following
estimation
bounds
for
14
and
e
using
nonparametric
regression
techniques
tsybakov
2008
to
simplify
the
notation
we
denote
=max{1,m;,2m,,—logn}
4.1
4.1.1
estimation
bounds
of
uaj(m
and
ea](:n
by
choosing
networks
an
=f
l
p1,ky,x1,ry
and
gyn
f(la
p2
ko
k2
r3
4.2
to
estimate
ha
and
e
in
3.4
and
3.6
respectively
we
prove
the
following
estimation
error
bounds
for
the
estimators
ﬁai
and
y
lemma
is
proved
in
appendix
a
we
use
o
to
hide
absolute
constants
and
polynomial
factors
of
&
holder
norm
log
d
d
7
a
and
the
surface
area
of
m
lemma
1
suppose
assumptions
and
a.3
a.4
hold
we
choose
ly
=o(lognny
py=
o((ﬂnl)tdﬂ
k=
o((’]"l)td“’log’?”l)r
x1
max{b,m
\/e,tz}
ri=m
for
fyn
and
l
=o(logny
pp
o(nlz“i
k
o(nlzﬁ
log
nl
ky
max{b,m
vd
%
r
=m
4.4
for
gy
in
4.2
then
for
any
=1,...,]a|
we
have
cy(m
+02)(nny
74
lo
3('7
4.5
es
|[fa
14y
ci(m
nny
1
4.5
||e
e
||2
<c
m2|a|2;l+'d
log®n
4.6
sy
|84
al
m1
where
c;,c
depend
on
log
d
b
and
the
surface
area
of
m
in
4.5
and
4.6
the
expectation
is
taken
with
respect
to
s
defined
in
3.2
lemma
provides
performance
guarantees
of
neural
networks
to
solve
regression
problems
3.4
and
3.6
in
order
to
estimate
yi4
and
e4
when
the
covariates
are
on
manifold
we
prove
that
the
estimation
errors
converge
at
fast
rate
in
which
the
exponent
only
depends
on
the
intrinsic
dimension
instead
of
the
ambient
dimension
d
4.1.2
regret
bound
of
learned
policy
versus
constrained
oracle
policy
our
first
main
result
is
regret
bound
of
ipy
obtained
in
3.8
against
the
oracle
policy
in
holder
policy
class
71
=argmaxe[q(n(x
nell,p
where
the
hélder
policy
class
iy
is
defined
as
ilyp
{softmax[yl(a:),...,v|a|(93)]t
1v
hp(m
and
anw
<mforj=
l,...,|a|}
4.7
accordingly
we
pick
the
neural
network
policy
class
as
t
softmax(f
with
rl
f(lyy
pry
kip
k71
rep
4.8
our
first
theorem
shows
that
tipr
is
consistent
estimator
of
the
oracle
holder
policy
71;3
as
long
as
the
network
parameters
ly
pry
k1
k11
rp7
are
properly
chosen
theorem
1
suppose
assumptions
and
a.3
a.4
hold
under
the
setup
in
lemma
1
if
the
network
parameters
of
ij\ll\
are
chosen
with
=0o(logn
pm=
o(|a|nzl‘%
o(|a|nzﬂdﬁ
log
n
4.9
k11
max{b
m
\/e
2}
rp=m
then
with
probability
no
less
than
cy|a|n
%7
over
the
randomness
of
data
s
and
s
the
following
bound
holds
2~
1002
r(my
tipr
cla|"n
log™n
v«
2|1
al
i
pa
(
@
@)-eaf@
410
i=nj+1
i=n;+1
where
c
is
an
absolute
constant
and
depends
on
logd
d
b
m
7
1
b
and
the
surface
area
of
m
theorem
is
proved
in
section
5.1
theorem
corroborates
the
doubly
robust
property
of
tpr
the
regret
of
tpy
is
not
sensitive
to
the
individual
estimation
error
of
either
ﬁai
or
a
since
the
bound
depends
on
the
product
of
the
estimation
errors
combining
theorem
and
lemma
yields
the
following
corollary
see
proof
in
section
5.2
corollary
1
suppose
assumptions
and
a.3
a.4
hold
if
the
network
structures
are
chosen
as
in
lemma
and
theorem
1
the
following
regret
bound
holds
with
probability
no
less
than
__ang
1-cyn
2arpd
log3
sa+2d
__onp
r(n;,ndr
c|
al
2a+d
2@npied
log2
4.11
where
c
is
an
absolute
constant
and
depends
on
logd
d
b
m
o
7
17
@
b
and
the
surface
area
of
m
in
comparison
with
existing
works
our
theory
has
several
advantages
by
considering
the
low-dimensional
geometric
structures
of
the
covariates
we
obtain
fast
rate
depending
on
the
intrinsic
dimension
d
our
theory
partially
justifies
the
success
of
off-policy
learning
by
neural
networks
for
high-dimensional
data
with
low-dimensional
structures
our
assumptions
on
the
propensity
score
and
expected
reward
are
weak
in
the
sense
that
the
holder
index
can
be
arbitrary
in
farrell
et
al
2018
and
zhou
et
al
2018
the
holder
index
of
the
propensity
score
and
expected
reward
needs
to
satisfy
2a
d
this
condition
is
hard
to
satisfy
when
the
covariates
are
high-dimensional
unless
the
p4,’s
and
ea;’s
are
super
smooth
with
bounded
high-order
derivatives
our
theory
is
nonasymptotic
while
most
existing
works
focus
on
an
asymptotic
analysis
zhou
et
al
2018
athey
and
wager
2017
farrell
et
al
2018
4.1.3
regret
bound
of
learned
policy
versus
unconstrained
optimal
policy
we
have
shown
that
neural
networks
can
accurately
learn
an
oracle
holder
policy
in
corollary
1
10
in
this
section
we
enlarge
the
oracle
policy
class
to
capture
all
possible
policies
including
highly
nonsmooth
polices
e.g
deterministic
policies
we
show
that
neural
networks
can
still
achieve
small
regret
due
to
their
strong
expressive
power
the
relationship
between
the
holder
policy
class
neural
net
work
policy
class
and
unconstrained
policy
class
is
depicted
in
figure
1
the
unconstrained
optimal
policy
is
defined
as
n
argmax
q(m
us
to
establish
the
regret
bound
of
tpg
in
3.8
against
7
we
need
the
following
assumption
on
the
ji4
s
assumption
a.5
noise
condition
let
and
denote
(
argmax
pia
x
there
exists
0
such
that
ip[lma
x
max
py
m)|
mt
<ct
foranyte(0,1
@
j2j
(
probability
simplex
unconstrained
policy
~
deterministic
policy
figure
1
the
unconstrained
pol
icy
class
is
the
whole
probabil
ity
simplex
with
vertices
being
deterministic
polices
the
inclu
sion
relation
of
the
neural
net
work
policy
and
the
holder
pol
icy
classes
indicates
that
for
any
holder
continuous
policy
there
is
an
approximation
given
by
neu
ral
network
policy
assumption
a.5
implies
that
with
high
probability
there
exists
an
optimal
action
whose
expected
reward
is
larger
than
those
of
others
by
positive
margin
this
is
an
analogue
of
tsybakov
low-noise
condition
tsybakov
et
al
2004
in
multi-class
clas
sification
problems
which
appears
similarly
in
wang
and
singh
2016
we
illustrate
the
noise
condition
in
binary-action
sce
nario
in
figure
2
we
utilize
temperature
parameter
in
the
softmax
layer
of
the
neural
network
to
better
learn
the
unconstrained
optimal
policy
under
the
holder
continuity
in
assumption
a.4
there
exists
deterministic
optimal
policy
7
i.e
t'(x
aj(z
which
is
one-hot
vector
in
contrast
the
output
of
the
softmax
function
is
randomized
policy
i.e
vector
in
the
interior
of
the
simplex
unless
the
output
of
the
neural
network
is
positive
infinity
accordingly
we
adopt
the
softmax
function
with
tunable
temperature
parameter
to
push
the
learned
policy
to
one-hot
vector
this
idea
has
given
many
empirical
successes
large
noise
small
noise
density
\/m](i
}'az(m)\
/m
figure
2
noise
condition
in
binary-action
scenario
large
noise
corresponds
to
high
den
sities
when
|;4al(z)7,u,42(m)|
is
small
small
noise
corresponds
to
low
densities
near
the
origin
in
reinforcement
learning
koulouriotis
and
xanthopoulos
2008
kuleshov
and
precup
2014
specifically
we
set
t
1
softmaxy(f
with
f(lyy
pry
krn
s
r
where
softmaxy(f
4.12
al
small
temperature
will
push
the
output
of
tn
towards
one-hot
vector
which
can
better
approximate
the
deterministic
policy
7
our
main
result
is
the
following
regret
bound
of
7pg
see
proof
in
section
5.3
theorem
2
suppose
assumptions
and
a.3
a.5
hold
assume
the
network
structures
defined
in
lemma
are
used
to
estimate
the
ha
s
and
the
e
s
if
the
network
parameters
of
al
hnn(h
are
chosen
with
ly
o(logn
pr
o(jaln%
ky
o(|a|n%+
logn
k1
max{b,m
vd
<%
1/h}
riy
m
11
then
the
following
bound
holds
with
probability
no
less
than
c;n~
log®n
r(rt
ttpr
c|a|%
0z
log
nlogl/2
1/h
min
2cmtq+m|a|2exp[(fmt+2n’ﬁ)/h
4.13
t€(0,1
where
c
is
an
absolute
constant
and
depends
on
logd
d
b
m
0
7
1
&
and
the
surface
area
of
m
the
regret
r(r
iprr
consists
of
two
parts
variance
term
77
and
bias
term
7
when
the
temperature
is
fixed
the
variance
77
converges
at
the
rate
n~
7
while
the
bias
t
does
not
vanish
this
is
because
tipg
is
random
policy
as
the
output
of
softmax
function
while
7
is
deterministic
as
one-hot
vector
under
assumption
a.5
furthermore
ttpg
is
asymptotically
consistent
with
7
when
0
if
we
choose
n"7%7
and
212
then
t
converges
at
the
rate
n
and
7
converges
at
the
rate
n~%%
we
have
the
following
corollary
corollary
2
suppose
assumptions
and
a.3
a.5
hold
in
the
setup
of
theorem
2
setting
h=n"2
and
2n
gives
rise
to
sa+2d
_(1ng)a
r(r
ttprr
c|a|
28+
n~
2axd
logs/2
4.14
with
probability
no
less
than
c
nom
log®
n
where
c
is
an
absolute
constant
and
depends
onlogd,d
b,m
0
7
1
@
and
the
surface
area
of
m
4.2
policy
learning
with
continuous
actions
our
analysis
can
be
extended
to
the
continuous-action
scenario
for
simplicity
we
let
the
action
space
be
unit
interval
i.e
0,1
in
such
continuous-action
scenario
policy
7(zx
either
randomized
or
deterministic
is
probability
distribution
on
0
1
for
each
covariate
m
the
expected
reward
of
the
policy
is
defined
as
q(n):j
wx
a)r(x
a)dadp(z
4.15
mi0
where
ip
is
the
marginal
distribution
of
covariate
x
as
mentioned
in
section
3.2
we
tackle
the
continuous-action
scenario
using
discretization
technique
on
the
action
space
this
is
motivated
by
practical
applications
where
continuous
objects
are
often
quantized
the
action
space
is
uniformly
partitioned
into
sub-intervals
i
j—-1)/v,j/v]for
j=1,...,v
where
is
to
be
determined
in
theorem
3
the
discretized
version
of
the
propensity
score
and
the
expected
reward
on
i
are
defined
in
3.9
we
also
consider
discretized
policies
on
a
in
particular
we
identify
all
the
actions
belonging
to
single
sub-interval
i
as
its
midpoint
a
discretized
policy
is
defined
as
vv
7p(@,4
=
pj(@)54,(4
4.16
j=1
12
where
04
is
the
dirac
delta
function
at
a
and
pj(«
denotes
the
probability
of
choosing
action
a
which
satisifes
z]-v:l
pj(x
1
in
fact
7(p)(z
can
be
interpreted
as
vector
in
the
v-dimensional
simplex
since
it
is
only
supported
on
discretized
actions
for
simplicity
we
denote
vector
7p)(x
p1(x),...,py(@)]t
with
pj(z
representing
the
probability
of
choosing
the
action
a
as
an
equivalent
notation
of
p)(z
-
for
the
discretized
policy
in
4.16
the
discretized
expected
reward
is
defined
as
qp
(
i
@
@
7d
@
(
4.17
we
observe
the
analogy
between
4.17
and
3.3
the
discrete
conditional
reward
py
is
replaced
by
the
discretized
conditional
reward
p
and
the
number
of
discrete
actions
a|
becomes
the
number
of
discretized
actions
v
on
the
other
hand
the
expected
reward
of
discretized
policy
is
vv
q(ti(d
j:mj
u(x,a
le‘nj(m)éaj(a)dad]p(m
fm(wm,al),...,mm,av)r,n‘dwm))dp(m
4.18
the
following
lemma
shows
that
if
the
u(a
a
is
lipschitz
in
uniformly
for
any
m
qp)(ed
is
close
to
q(r(p
when
is
large
see
proof
in
appendix
b
lemma
2
assume
there
exists
constant
l
such
that
sup
|;4(m,a)7;4(m,ft)|
l”|a7x|
for
any
aac
0,1
xem
then
for
any
discretized
policy
7p
given
in
4.16
we
have
q
qpp
lv
we
remark
key
difference
between
4.17
and
4.18
to
evaluate
4.18
one
needs
to
accurately
estimate
the
p(z
a;)’s
which
requires
the
action
a
to
be
repeatedly
observed
however
this
is
prohibitive
in
the
continuous-action
scenario
since
an
action
a
is
observed
with
probability
0
in
contrast
4.17
relies
on
the
average
expected
reward
on
sub-interval
which
can
be
estimated
using
standard
nonparametric
methods
in
the
following
section
4.2.1
moreover
thanks
to
lemma
2
we
can
well
approximate
q(1
by
q®)(7(p
up
to
small
discretization
error
this
is
crucial
to
establish
the
regret
bound
in
theorem
3
4.2.1
doubly
robust
policy
learning
with
continuous
actions
after
discretization
we
can
apply
the
doubly
robust
framework
to
learn
an
optimal
discretized
policy
in
the
first
stage
we
estimate
the
y;’s
and
s
in
the
sequel
we
use
the
plain
font
4
to
denote
the
observed
action
of
the
i-th
sample
the
bold
font
a
{0,1}
denotes
the
one-hot
vector
with
the
j-th
element
being
1
if
a
i
similar
to
3.4
3.6
in
the
finite-action
case
we
obtain
estimators
of
the
y;.’s
and
e;,’s
by
minimizing
the
following
empirical
risks
1y
y
o1
fip,(z
argmin
z(y
f(x;))*1{a
i;}
with
ny
zﬂ{ai
€l}
4.19
fern
i=1
i=1
13
and
ny
g(x
=argmin
7<[g(mi)t
l]t,ai>+log(l
8egnn
i=1
1
exp([g(mi)]j
4.20
=1
op(@al
=l
forj<v-1,and
ey
@
where
ay
and
gyy
rv
are
neural
networks
in
the
second
stage
we
learn
an
optimal
discretized
policy
using
the
i
s
and
e]j
s
recall
that
we
define
mapping
i(a
=
i
for
a
i
to
index
which
sub-interval
a
belongs
to
we
use
neural
networks
to
learn
discretized
policy
by
maximizing
the
doubly
robust
empirical
reward
in
3.10
and
3.11
i.e
4.21
c.pr
argmax
q)(r
4.22
neh,‘\;n(h
where
hkn(h
represents
the
proper
network
class
in
3.10
which
is
defined
as
4.12
we
emphasize
that
7ic_pp
is
discretized
policy
and
the
output
pr
is
v-dimensional
vector
in
the
simplex
4.2.2
regret
bound
of
learned
discretized
policy
we
begin
with
several
assumptions
which
are
the
continuous
counterparts
of
assumptions
a.3
a.5
in
the
finite-action
scenario
assumption
b.3
the
propensity
score
and
random
reward
satisfy
i
overlap
e(x,a
for
any
a
where
is
constant
ii
bounded
reward
|y
x
a)|
m
for
any
x,a
x[0,1
where
m
is
constant
assumption
b.4
given
holder
index
1
we
have
both
the
expected
reward
u(-,a
h*(m
and
the
propensity
score
e
a
h*(m
for
any
fixed
action
a
moreover
the
holder
norms
of
u(-,a
and
e(-,a
are
uniformly
bounded
for
any
0,1
i.e
sup
||p(,a)||;e
<my
and
sup
le
al
m
aeg[0,1
aeg[0,1
for
some
constant
m
0
furthermore
there
exists
constant
m3
such
that
sup
|p(x
ay
p(x
ay)l
m3|a
ay|
for
any
ay
a
€[0,1
xzem
there
also
exists
constant
my
such
that
log
j
e(x,a)da/
e(x,a)da
<m
for
any
intervals
i}
i
0,1
of
the
same
length
h[r
let
py(x
e(z,a)da
denote
the
probability
of
choosing
actions
in
given
in
assumption
b.4
the
condition
e(-,a
h*(m
for
any
given
implies
p
h*(m
see
lemma
7
combining
this
and
assumption
b.3
ii
of
e(z
a
17
0
one
deduces
that
log(py
x)/py
(
belongs
to
h
m
with
bounded
holder
norm
see
lemmas
in
appendix
for
formal
justification
for
simplicity
we
denote
=max{1
my
2m
m3
my,—logn}
4.23
14
assumption
b.5
continuous
noise
condition
the
following
two
conditions
hold
i
for
each
fixed
m
p(x,a
is
unimodal
with
respect
to
a
there
exists
unique
optimal
action
a*(x
such
that
p(x
a*(x
maxc
p(x
a
ii
there
exist
constants
and
0
such
that
p[u(x
a*(z
p(x
a
mt
given
|[a
a*(x)|
y
ct(1-y
holds
for
any
0,1
and
any
0
1
where
ip
denotes
the
marginal
distribution
on
x
assumption
b.5
generalizes
the
noise
condition
for
finite
actions
in
assumption
a.5
to
the
continuous-action
scenario
assmption
b.5
i
assures
the
uniqueness
of
the
optimal
action
given
each
covariate
assumption
b.5
ii
means
that
with
high
probability
there
is
gap
between
the
reward
at
the
optimal
action
and
the
rewards
in
its
neighbors
we
establish
regret
bound
of
tic_pr
against
the
unconstrained
optimal
deterministic
policy
ne
argmax
q(m
us
where
q(rr
is
defined
in
4.15
due
to
assumption
b.5
i
7
is
deterministic
with
nig(x
op+(a
the
following
theorem
establishes
the
regret
bound
of
7ic_p
against
7
theorem
3
suppose
assumptions
and
b.3
b.5
hold
set
ay
f(ly,p1
ky
%1
r
in
4.19
and
gnn
lp
pa
kp
k2
r
in
4.20
with
l2adi7d
12ad+7d
ly
=o(logn
p1
o(qmn
7(2aza
)
o(;]mn
72avd
logn|
4.24
x
max{b,m
\/e
12}
ry
=m
and
10ad+74
10ad+74
ly
=o(logn
py
o(n
7ard
)
k
o(n
72
jog
n
x>
max{b,m,vd
12}
r
m
4.25
if
the
network
parameters
in
hkn(h
are
chosen
as
a+7d
a+7d
l1
o(logn
pr1
o(n%“z
)
o(nﬁ
logn
k1
max{b,m
vd
2}
rp=m
4.26
_2a2+4ad
)
the
following
bound
holds
with
probability
at
least
c;n
7@e+
log®
and
we
set
p*a/7(2a+d
2a
r(rts
_prr
cn
log
nlog'21/h
2th‘7
mutosm
exp(f(mt
_4my
/h
4.27
2a
for
any
2(1
1/m)n77‘2“*'“,1
where
c
is
an
absolute
constant
and
depends
on
logd
d
b
m
0
t
1§
a
and
the
surface
area
of
m
15
theorem
is
proved
in
section
5.4
similar
to
equation
4.13
in
theorem
2
the
bound
in
theorem
contains
variance
term
the
first
term
and
bias
term
the
second
term
the
variance
__2a
converges
in
the
rate
of
72<*9
for
fixed
the
bias
term
does
not
vanish
as
goes
to
infinity
20
if
we
set
=n
70
and
4(1
1/m)n
7?0
the
bias
term
converges
in
the
rate
of
n~
72+
under
this
choice
the
behavior
of
r(rt
ttc_pr
is
summarized
in
the
following
corollary
corollary
3
suppose
assumptions
and
b.3
b.5
hold
in
the
setup
of
theorem
3
setting
2a
h=n
72
and
4(1
1/m)n
7229
gives
rise
to
~trae
10052
r(mtg
tie_pr
cn
7c@rlasd
log>
4.28
_2a2+4ad
with
probability
no
less
than
cyn
727
log®
n
where
c
is
an
absolute
constant
and
depends
onlogd
d
b
m
0
7
1
a
and
the
surface
area
of
m
there
are
limited
theoretical
guarantees
for
causal
inference
with
continuous
actions
kennedy
et
al
2017
proposed
doubly
robust
method
to
estimate
continuous
treatment
effects
the
asymptotic
behavior
of
the
method
was
analyzed
while
the
policy
learning
problem
was
not
addressed
to
our
knowledge
theorem
and
corollary
is
the
first
finite-sample
performance
guarantee
of
policy
learning
with
continuous
actions
proof
of
main
results
we
prove
our
main
results
in
this
section
and
the
lemmas
used
in
this
section
are
proved
in
the
appendix
5.1
proof
of
theorem
proof
of
theorem
1
we
denote
7
argmax
q(mn
5.1
71e1'[‘,j,‘\i
which
is
the
optimal
policy
given
by
the
neural
network
class
h‘i\’:‘i‘\
defined
in
4.8
the
regret
can
be
decomposed
as
r(ry
tipr
q(11
q
q(7
q(tipr
5.2
1
(
in
5.2
i
is
the
approximation
error
bias
of
the
optimal
hélder
policy
tze
by
the
neural
network
class
i"ik;ll‘\l
and
ii
represents
the
variance
of
the
estimated
policy
in
i"kal‘\
we
next
derive
the
bounds
for
both
terms
bounding
i
recall
that
n;i
we
can
write
n
softmax([u
ta\]t
where
4
hf(m
for
1,...,|a
according
to
chen
et
al
2019
holder
functions
can
be
uniformly
approximated
by
neural
network
class
if
the
network
parameters
are
properly
chosen
for
any
0
1
there
exists
network
architecture
f(l
k
«,r
with
is
the
holder
continuous
optimal
policy
in
ity
by
defnintion
l=
o(log
%
p=
o(f%
k=
o({%
log%
max{b,m
\/e
2}
r=m
5.3
16
such
that
for
each
4
hf(m
there
exists
ﬁ
f(l,p,k
x,r
satisfying
pilloo
5.4
the
constants
hidden
in
o
depend
on
logd
d
b
m
7
f
and
the
surface
area
of
m
we
denote
softmax([j1
#4
rmi
which
implies
hln
with
lnp=l
pn=|alp
kn=lak
xp=x
rgp=r
5.5
for
l
p,k
k
defined
in
5.3
based
on
5.4
and
the
lipschitz
continuity
of
the
softmax
function
we
have
l
¢
where
|77
5”00
sup
max;|[7(x
nz(m)]ﬂ
with
7t(x
tz;;(:l
denoting
the
j-th
element
of
tt(x
n;;(m
therefore
we
bound
as
11
q
q(f
qureh
q
e[([pa
@
pay
@
t
7
@
t(@
miale
5.6
bounding
ii
we
introduce
an
intermediate
reward
function
qto
decompose
the
variance
term
ii
define
y'fﬂa,(m
tomn
with
=20
it
s
@)oo
(
ral
5.7
ca,(mi
i=n+1
note
that
has
the
same
form
as
while
the
estimated
propensity
score
e
and
expected
reward
71
are
replaced
by
their
ground
truth
e
and
p
respectively
we
decompose
ii
as
i1
q(7
q(7pr
=q(7
qfpr
qt
q
qlfpr
q(fpr
q™
q(7ipr
q(fpr
qat
sup
q(m
q(ma
q(m
q
sup
q1
qlry
q(711
qlrey
n,,nzeh,\’,qn
nl’"zeh]jn
sup
al
al
m)+
sup
ay
105
ny
705
5.8
n,,nzel'[‘,\’,q,‘\
nl,nzen‘,\ﬁ‘\l
where
a(1ty,7
q1
q(112
ay
75
q(my
q\(sz
andaa(nl,nz
q(my
q
the
first
inequality
in
5.8
come
from
5.1
which
implies
q(7pgr
q(7
in
this
decomposition
&
corresponds
to
the
difference
between
and
which
can
be
bounded
using
the
metric
entropy
argument
since
is
unbiased
i.e
e[q
)
q(m
the
second
term
&
corresponds
to
the
error
between
and
which
can
be
bounded
in
terms
of
the
estimation
errors
of
the
e
s
and
the
ha
s
bounding
&
we
first
show
that
e[q
=q(m
1e[a
=e
<ie
%‘;()m)ah
71(93)>
<[,ma1
x
ﬂa‘a‘(m)]t
71(93
=e[([pa
@
pa
@
7(@
ely
2(@
q
5.9
17
which
further
implies
ie[z(ri,tzz
a(my,73
in
5.9
the
second
equality
holds
since
y~
ha(z
e[yl
ie
@
a|:n
ea(m
e[a|9
by
assumption
1
therefore
we
can
write
e1=
sup
a(my,m
e[a(ry
m
5.10
nl;t(zen‘nl‘\l
with
k:ni
<i‘,,n1
-
l
mo(x
i=ny+1
22
ny+1
we
derive
bound
of
£
using
the
following
lemma
which
is
be
proved
by
symmetrization
and
dudley’s
entropy
integral
wainwright
2019
dudley
1967
in
appendix
c
lemma
3
let
it
rl
be
policy
space
on
|.a|
actions
such
that
any
it
maps
covariate
to
we(z
in
the
simplex
of
r
and
s
{(x;,vi)}i_
be
set
of
i.i.d
samples
where
z
is
sampled
from
probability
distribution
ip
supported
on
and
y
r
for
any
z,y
we
define
f‘(m,y
as
function
of
the
sample
x,y
assume
that
there
exists
constant
0
such
that
sup
f(a,p
<
5.11
x,y)e
mxr
for
any
policies
7y
7t
i
define
apmy,m
gm@
~
b
m@
and
5.12
i=1
i=1
d()=
sup
a(my,m
e[a(ry
5
5.13
70y
€lt
with
the
shorthand
f(mi,yi
then
the
following
bound
holds
max|l
oz
1/0
d(i
inf
4/\+—j
jlog
0,11
hip)do
127
28172
5.14
vn
2n
with
probability
no
less
than
2§
over
s
where
||||p
/%
zl(ﬁ,n(m,‘»z
key
observation
is
that
when
taking
i
=t
defined
in
5.7
and
it
i"illfl‘\l
we
have
&
d(it
in
5.10
to
apply
lemma
for
bounding
£
we
only
need
to
verify
the
assertion
5.11
in
fact
due
to
assumption
a.3
we
see
that
y
ya](:n
and
ea](m
are
all
bounded
simple
calculation
yields
sup
,
vxr
2
9)|
<j
=
2m/1
m
therefore
we
bound
&
as
96
m:‘xx
lele
14|
log1/6
<1nf
4+
log
0
ti{
|1l
46
24m
/1
12m
5.15
2n
with
probability
no
less
than
24
18
bounding
&
the
&
term
depends
on
the
difference
between
t
and
ﬁ
where
t
and
i
are
defined
in
5.7
and
3.7
respectively
in
£
we
have
al
o)~
blry
@
mole
t
zi:nl+1
mo
=z
r1,j(@i
=100
j(@
t~
)
j=1
2i:nprl
where
7t
andfi
denote
the
j-th
element
of
7t
and
f
respectively
define
3
y
ay
1
rr1,j(@i
7o
5(2
t
j
€r
for
j=1
al
i=nj+1
then
we
can
write
z(nl,nz
7’&(711,712
as
ay
m0
ay
m2
~
ay
o
5.16
j=1
the
error
term
f
7117
in
aj(my
depends
on
the
estimation
error
of
va
and
e}
based
on
the
source
of
the
error
we
decompose
each
aj(r
;
into
three
terms
vi~
ma,(ti
ajlm
==
m,ﬂm%m,ﬂm»)[[ﬁ
ljayon
+ia
(
i=ny+1
fa,(ti
711,]‘(931‘)*ﬂz,j(l'i))(#a](mi)*fal(mi
i=n;+1
vi~
ha(®i
yi—pa,(ti
7;1
711,]‘(:7
nz,j(mi))ﬂ(u,:a
w
=m
=5y
+
817
0y
100
81
0
)
5.17
where
a=4
o)=y
m,,'(a:i)fnz,jm-))(m,(mnfﬁa,m))[1f
i
i=ny+1
$p
1
j
1z
7e1,j(@i
7o
2
i
pa
mi))[;*,\
2
41
<iznla;=a
ea(xi
en
i
s](_a)(m]-,nz,j):i
ﬂl,j(mi)*nz,j(mi))(ﬁ,q
x
pa
mi))[/\;,;]_
2
m+1<i<nla;i=a
en(xi
(
19
here
s](-l)(nllj,nzj
and
5;2)(n1,j,n2,j
can
be
bounded
using
lemma
3
5;3)(n1,j,n2,j
contains
the
product
of
the
estimation
error
of
ji
and
which
gives
the
doubly
robust
property
according
to
5.16
and
5.17
ja|
2
3
&=
sup
|
§mpma)+
8p
my
71171261'[,:,4,‘\
j=1
1
2
3
sz
sup
711’]-,712
sup
711,]‘,712
sup
tzi’]-,rz
5.18
j=1my
nzehn\
t
nzehnn
t
nzehn,‘\
in
the
rest
of
the
proof
when
there
is
no
ambiguity
we
omit
the
dependency
on
7t
j
7
;
and
use
the
notations
s(-l
sq
and
s(-s
we
next
derive
the
bounds
for
the
s;l
s](-z
and
s](-s
terms
in
the
right
hand
side
of
5.18
respectively
sm
for
s(-l
one
can
show
that
ie[s(-i
=0
nn/
boundingsup
1,72
e[s}
il,lﬁ
700
i
o2
@
1o
~
vaj(mn)[l
a
2z
ny+1
tia;=a
71-:;1(71
d)=
10j(0
pa
i
m,(mn)mlfwﬂ
mi”:o
denote
1
a,—a,}
z
ﬂa
i)~
ha
@
1
ea](mi)]eer
then
we
have
1
sup
sup
s
ty
nzel'ﬂn'\
ty
71261'1‘,\”‘q
sup
e1,j(i
702
(
(
m2
m,nzel'llml
21:;1,+1
the
expression
in
5.19
resembles
the
same
form
as
in
5.13
with
i=tw)(z)and
it
i—i‘i\’;‘il\i
therefore
we
can
estimate
sup
o
ertd
s;l
using
lemma
3
due
to
assumption
a.3
for
any
nn
m
we
have
1))(x
2m/1
after
substituting
2m/n
in
lemma
3
we
have
max
|||
sup
s\
inf
41+
\/%j*f
n\
jlog
o
tl|[|)do
24m/1
l°g'12/
5.20
t
nzel'iml
with
probability
no
less
than
1-26
bounding
sup
s1m11arly
one
can
show
e
s](2
0
denote
tfl:tizehii\'il
@
;
zjz
ﬂal(mi))[m*m
liavaj}
er
20
we
follow
the
same
calculation
in
5.19
to
express
sup
with
i
=2/
and
it
hkﬁ‘\
an
upper
bound
of
sup(4y
t%1)(2)|
can
be
derived
as
follows
with
gy
chosen
in
4.4
its
output
is
bounded
by
m
which
implies
e}
|ale*m)~1
thus
)
e
maelte
s
in
the
same
form
as
in
5.13
alepm
eq;(®
ey
(
since
log#
by
4.1
by
assumption
a.3
and
4.1
we
have
supz’y|y
ya](m)|
2m
hold
for
any
=1,...,|a|
therefore
we
have
sup
t(x)|
2|ale*mm
5.21
z,y)e
mr
using
lemma
and
substituting
2|.4]e?m
give
rise
to
max
|||l
_
96
enid
log1/6
su
<inf
41+
nn
log
0,1l||-|i;)d6
24|
ale“™
m
5.22
<infaxs
n
log
0.1
40
244
1,72l
with
probability
no
less
than
26
3
3
bounding
sup
etk
s;'+
we
next
derive
an
upper
bound
of
sup
etk
as
the
product
of
the
estimation
errors
of
the
714
s
and
the
€y
s
sup
5;3
nl,nzeh‘,\’fl‘\
=
sup
7e1,j(@i
702
2
fin
1
pia
mi))[f*—_
70y
1et
{nl+lgi§zn‘\a,:a
eaj(m
ea](:n
ep
@
(
sniz
|va,(931‘)*#a,(93,‘)|
i=ny+1
since
|my
j
70y
|
1
|1
#a,(mi
#a,(mi
nj
m
ea,(mi
by
cauchy-schwarz
i=n;+1
i=n;+1
1y
sﬂ
1|a|€2m
i=ny+
wmnm}mn)zj%
e
@
ea
@
5.23
where
the
last
inequality
holds
since
ea
21
by
assumption
a.3
and
a
lale*m)~1
we
denote
wj:rvllalezmjni
@,(m»m,(mn)zji
e
(
ea
(
5.24
i=n;+1
i=n;+1
and
write
sup
et
s
<
wj
21
putting
the
s
s
and
terms
together
combining
5.20
5.22
5.23
gives
rise
to
sup
aj(my,mp
sup
sup
sup
s
nl,nzsn‘,\’,q,‘\
ty
r(zehn,l
ty
nzsl'inn
nl,nzeh‘,\’,q,‘\
max
nl
192
entdl
al
om
log1/6
<1nf
81+
nzl
\[log
nv
6
tien
ihip)d6
48]ale*m
oy
with
probability
no
less
than
46
where
we
used
7
according
to
4.1
according
to
5.18
we
can
apply
the
union
probability
bound
for
1,...,|.a|
and
obtain
&=
sup
ay
~
alrry,mp
m,nzel'l‘,\f,l\
192|a|
oy
el
log
1/a
al
<inf
8a|a+
jff
viog
0
1146
48|
a2
;
5.25
with
probability
no
less
than
4|.ao
combining
5.15
and
5.25
we
have
l
192|141
+96
(
1
<inf
84
4)a
\1og
0
t
|l
tog
1/5
+
wj+
72142
12m
ofn/
5.26
j=1
with
probability
at
least
6|.a[6
putting
i
ii
together
putting
our
estimates
of
i
in
5.6
and
ii
in
5.26
together
we
get
192141
+96
max
imle
r(rej
tipr
|aime
+inf
814
+4)a
log
0
tiey
11r)do
iz
sala2em
10810
5.27
+zl‘wj+
|a|“e
5.27
iz
with
probability
at
least
6|.4|6
the
upper
bound
in
5.27
depends
on
the
covering
number
n6
h‘nn
fir
and
the
integral
upper
limit
max__a
|7/lp
which
can
be
estimated
by
the
following
nn
lemmas
see
the
proofs
in
appendix
and
respectively
lemma
4
suppose
assumptions
a.3
and
a.4
hold
and
define
ij\ll\
according
to
4.8
then
2(jam
2m/n)le
priraa|
2)ick
pri/j
al
<0
n(0,tinn
il
lir
5.28
lemma
5
suppose
assumptions
a.3
and
a.4
hold
for
any
1t
hnn
the
following
holds
limllf
2m/y
+]aim)>
5.29
22
setting
the
network
parameter
as
in
5.5
and
using
5.28
we
have
tog
n'(6
ty
i
cubale™
log
log
+log
5.30
with
c
depending
on
logd
d
b
7
1
b
and
the
surface
area
of
m
substituting
5.30
and
5.29
into
5.27
gives
4|
a2e2m
log1/6
r(nﬁ,ndr)si
lmhzl'ijrs
|ale
288|a|
mam+2m/n
+ira1f
12|a[a
cylale
f‘log
log
+10g9)d9
ja|
log1/6
2,2m
s|a|m5+zl‘m]+84|a|
+inf
12|41a
iz
288|412
_a
1
51
1
log
log
+log
31
+c
ne
og
log
~+log~
5.31
with
probability
no
less
than
1
6|a|6
and
c
depending
onlogd
d
b
7
1
b
and
the
surface
area
of
m
setting
=n
3
0=
zm
a=n
implies
4.9
and
4.10
in
theorem
1
1y
5.2
proof
of
corollary
proof
of
corollary
1
corollary
is
proved
based
on
theorem
and
lemma
1
we
first
derive
an
upper
bound
of
the
w;’s
using
lemma
1
taking
an
expectation
on
the
both
sides
of
5.24
gives
rise
to
elw
<5
al
i=n;+1
i=ny+1
a
(
e
@
ea
i
2
i=ny+1
i=n+1
aly\
7
a2
e[lea
=2
12
_3a+d
sa4d
20
™m
m
o)y
2|
alzesd
log®
ny
jnl
7
@
pay
zjhz
@@
sl
2
q—llall,zm
where
the
second
inequality
is
due
to
jensen’s
inequality
and
the
unconfoundedness
condition
in
assumption
1
the
last
inequality
is
due
to
lemma
1
and
c
is
constant
depending
on
logd,d
b
7
and
the
surface
area
of
m
by
markov’s
inequality
for
any
0
e[wf
ip(u
b
sciglnl
log
ny
5.32
where
g
e?m(m
u)r]’%|a|%
applying
union
probability
bound
gives
rise
to
14|
zw
>]a15
|a|g1nl
=i
log®
n
5.33
j=1
23
substituting
5.33
into
4.10
and
setting
c1|a|g1n1
7
we
get
tz[;:rdr
cre®™|
ap
m
o)n
7a
log
1+
cylap
giny
8a+2d
c3e®ma|
m
o
log?n
__an
with
probability
no
less
than
cqn
p1h
log3
n
where
cy
is
an
absolute
constant
c
c3
are
constants
depending
on
log
d
d
b
7
a
§,1
and
the
surface
area
of
m
5.3
proof
of
theorem
proof
of
theorem
2
in
theorem
2
1
is
the
unconstrained
optimal
policy
we
prove
theorem
in
similar
manner
as
we
prove
theorem
1
we
first
decompose
the
regret
using
an
oracle
inequality
r(r
7ipr
q(7
q(tt
q(7
q(tpr)s
5.34
i2
1
where
70
is
the
same
as
in
5.1
in
5.34
i
is
the
bias
of
approximating
7
by
the
policy
class
hk“i‘\i(h
and
ii
is
the
same
as
ii
in
5.2
which
can
be
bounded
similarly
following
the
proof
of
theorem
and
corollary
1
we
can
derive
that
sa+2d
i1
e®m|a|
%657
m
0)n~
log
nlogl/z(l/h
with
probability
no
less
than
cyn™
log3
where
c
is
an
absolute
constant
and
c
depends
on
logd,d,b,7,a
and
the
surface
area
of
m
in
addition
tpg
l‘[kj\ll\‘(h
with
ly
pry
ky
k7
and
ry
given
in
4.9
it
remains
to
show
i
2cmt1
+m|a|2exp[(fmt
2n’23ﬁ)/h
forany
0,1
bounding
i
we
estimate
q(7c
q(7
on
two
regions
the
first
region
is
for
any
given
0,1
{m
em
pp
€)=
max
py
(
mt}
j#jt
@
with
j*(z
argmax
uaj(m
on
y
the
gap
between
amm)(m
the
reward
of
the
optimal
action
and
the
reward
of
the
second
optimal
action
is
smaller
than
m¢t
assumption
a.5
yields
ip(x
ct1
the
second
region
is
xf
{a
te
m’fa],‘e)(m
r:}a(x)ya
x
>mt}
on
which
the
gap
between
p4
(
and
the
reward
of
any
other
action
is
larger
than
mt
for
any
policy
7
we
have
q
e[y
r(x
fm
ule
(
dp
where
p(z
ax(m)""’”a|a|(m)]t
according
to
chen
et
al
2019
theorem
2
for
any
0,1
there
is
neural
network
architecture
l
p
k
k
r
with
=o(logl/e
p=
o(s’%
k=
o(s’%
log
1/8
max{b,m
\/e
2}
r=m
24
such
that
for
each
hay
there
exists
ﬁai
f(l,p,k,x,r
and
||fa
ma]”oo
<e
q(m
4y
and
q(t
~
define
7@
softmaxh(yal,...,yaw
since
7t
argmaanhnn‘h
q(7
we
have
aule
@
t(@)dp@
pu(@
@
r(@)dp(@
5.35
the
first
integral
in
5.35
can
be
bounded
as
@
@
@
dp(x
|p(@)llll7
(
(
ap
(
xt
xt
<2m
1dp(z
2cmh
5.36
xt
where
||p(z)llo
max
|;4al(m)|
and
||ne(x)]|
z‘j’ill
7e(z)];|
for
the
second
integral
we
first
derive
an
upper
bound
of
||7*(x
77(x)||
since
7
is
the
unconstrained
optimal
policy
represented
by
one-hot
vector
7t*(z
aj«(g
we
deduce
exp(y
@v/h
if
x
a
mo
tz
17)771(:1
zk;:p(klak(:);h
liej
exp(finy
/h
if
@
a
tooplin
@m
m=
maxy
exp((pa
x)+e)/h
o~
explls
@£
/
ifj="(@
|l
(
(
|
jalmaxy
exp((jia
@)+e)/h
il
expl(in
@)-c)/h
ifj
=7
@
therefore
||7*(x
||0°s|a|exp(maxk$
mak
)
uaj,(:n)jer)/h
thus
@
@
t(z
dp(z
c||/l(€e)||1||(7t*(m)*ﬁ(ﬂi))llmdﬂ’(ﬂ
xr
m|a|2exp
mt
2¢)/h)dp(z
m]a|
exp
mt
2¢)/h
5.37
xr
combining
5.36
and
5.37
and
setting
n
give
rise
to
q1
q(ft
2cmt9
mi
ap
exp((~mt
20~
%1
/h
which
completes
the
proof
5.4
proof
of
theorem
proof
of
theorem
3
we
denote
argmax
qp)(n
melly
nn(h
25
where
qp)(r
is
defined
in
4.17
the
regret
can
be
decomposed
as
r(rg
epr
q(rg
qp
q
qp
fic_pr
qp
ficpr
q(fc-pr
5.38
13
i13
i1l5
in
5.38
i3
is
the
bias
of
approximating
the
optimal
policy
7
using
the
neural
network
policy
class
hkn(h
in
the
discretized
setting
ii3
is
the
variance
of
the
estimated
policy
in
hnn(h
iii3
characterizes
the
difference
between
the
discretized
policy
reward
and
the
continuous
policy
reward
of
tic_pr
we
next
derive
the
bounds
for
each
part
bounding
i3
by
assumption
b.4
hi
h¥(m
according
to
chen
et
al
2019
holder
functions
can
be
uniformly
approximated
by
neural
network
class
if
the
network
parameters
are
properly
chosen
for
any
0
1
there
exists
network
architecture
l
p
k
k
r
with
=o(log1/e),p
o(s’%),k
o(f’g
log
1/5),1<
=max{b,m
\/e
2},12
=m
5.39
such
that
if
the
weight
parameters
are
properly
chosen
we
have
ji
7(l
p
k
x
r
satisfying
i
oo
<
we
then
define
an
intermediate
policy
softmaxy
s
.
pi1
let
a*(x
argmax
o
1
p(e
a
then
7t¢-(x
a*(x
after
defining
p(x
y
(
py
z)]7
ry
we
can
bound
i5
as
i3
q
qt
q
j
2
a*(z))dp(x
fm
(
dp(z
j
#(%a*(m))dﬂ’(m)*j
@
lia@jeny
la@er
hap(x
ty
(
wiy
ao
ap~
o
fehap
5.40
if
a*(z
i
we
denote
j*(z
and
l(z
i
according
to
assumption
b.4
and
4.23
is
lipschitz
constant
of
the
function
u(x
for
any
fixed
m
since
a*(x
l(x
|u(x
a*(x
hi(z)(x
m/v
for
any
m
hence
t
can
be
bounded
as
t
jmﬂ(m,a*(m
(
@)dp(z
m/v
5.41
we
then
derive
the
bound
for
t
on
two
regions
the
first
region
is
x1,y
alp(@
a"(x
p(z
a
mt
given
|a
a*(z)|
y
and
the
second
region
is
xt[.:y
according
to
assumption
b.5
p(x
ct1(1-7y
26
t
is
decomposed
as
t
<m(ﬂ3
mias@)er
y
ljar@er
ﬁ(@
dp(z
ly
<m(m
vs
@er
vjar@yer
ﬁ(@
dp(x
5.42
ly
the
first
integral
in
5.42
is
bounded
as
@
la@enp
vac@per
|7
t@))dp(@
2cm
1
y
5.43
xty
we
then
derive
an
upper
bound
of
the
second
integral
in
5.42
in
way
similar
to
the
derivation
of
5.37
denote
1
o)<
pe
@
ljsj@
@v/h
exp(qi
/h
exp(in
@)/h
exp(@
@)/h
expl
@)/h
similar
to
5.37
we
have
@
lia@jer,)s
diar(@per
(
dp(x
m(x
e(x
dip(x
ly
xey
@)l
@
dp@
vm
@)dp
5.44
ty
x'}
to
derive
an
upper
bound
of
||z||
we
need
lower
bound
of
g)(x
u,j(a
forany1<j<v
and
j*(z
by
assumption
b.4
for
any
and
i
one
has
e
&
p(a
a
m/v
and
@)=t
it
a
e
aplda
mv
it
where
|ij|
1/v
represents
the
length
of
;
as
result
on
xt[.:y
for
any
j*(x
we
have
hi
@
@
pr
@
p(@
aje
plx
aj
2m/v
p(x
a'(z
p(x
aj
|p(z
a%
(
pl(@
ajr
|
2m/v
mt-3m/v
where
the
last
inequality
holds
for
two
reasons
1
a*(x
x
and
aj
(
l(z
2
we
set
<1/(2y
and
then
j*(x
implies
|a
a*(z)|
1/(2v
y
we
then
deduce
12l
v
1)exp(—(mt
3m/v
2¢)/h
5.45
27
plugging
5.45
into
5.44
we
have
<#(m)r
m
@gery
nar@ery
77(1')>
dip(x
xty
svm
v
1)exp(—(mt
3m/v
2¢)/h
<mv
2exp(—(mt—3m/v
2¢)/h
5.46
substituting
5.41
5.43
and
5.46
into
5.40
if
1/(2y
we
have
i3
+2emti(1
+
mv
exp
mt
3m/v
2¢)/h
5.47
bounding
ii3
ii3
has
the
same
form
as
ii
in
5.2
we
derive
the
upper
bound
by
following
the
same
procedure
while
|a|
is
replaced
v
besides
we
need
to
express
the
estimation
error
of
7[1
s
and
e}
s
in
terms
of
v
note
that
e
1/v
by
lemma
1
we
can
find
e
f(ly,p1,ky
k1
ry
with
olog(ym/v
py
o((gm/v)=7
)
ky
o((m1/
v
log(mi/
v
k1
max{b,m
\/e
12}
=m
such
that
i
pr
|2
co
02
5
vy255
log
51
/v
5.48
with
c
being
constant
depending
on
log
d
b
7
and
the
surface
area
of
m
similarly
we
can
find
g'e
f(l
pa
ky
k2
ry
with
d_
l
o(log(n1/v
p»
o(v*%nfw
ky
o(v*%fr
log(nl/v
k
max{b,m
\/e
2}
r
=m
such
that
4a
2a
e[l[e
ey
i7
com?v
%57
n
log®
my
5.49
with
c
depending
on
log
d
b
m
and
the
surface
area
of
m
following
the
proof
of
corollary
and
using
5.48
and
5.49
we
rewrite
5.33
as
ij
>v
j=1
with
g
e2m(m
6)11’%
and
cj
being
an
constant
depending
on
log
d
b
7,1
and
the
surface
area
of
m
by
replacing
|a|
by
and
by
#/v
in
£
and
&
in
the
proof
of
theorem
1
and
substituting
5.50
one
derives
a+3d
20
czg2
ve
1y
log®
1y
5.50
log1/6
qp
i
q7
c.pr
v5
84e*m
vm
—
+inf
12v
288v
vm+2vm/y
jn_
knlog(e’l(vmjrzvm/q)x
1/2
li(prb/v
2)max(xy
1/h
pry/v)h1#t
do
28
with
probability
no
less
than
m—nl
2a+d
10g3
n
6vs
%ngsd
here
pr
hkn(h
where
the
network
class
hkn(h
has
the
parameters
=l
pp=0(vp
ky
o(vk
xy
=«
=r
with
l,p
k
and
defined
in
5.39
setting
n~
7,8
%
6
c3g2v%n1m
and
a=
v%n;m
gives
rise
to
_d_
_d_
ly
o(logn
pry
o(viizmh),kh
o(vnhmf
logn),kh
=max{b
m
\/e,t
lr
=m
and
i13
c4e®m(m
j)v%n’ﬁ
log2
nlogl/z(l/h
5.51
12a+3d
with
probability
no
less
than
csv
a0
%
log®
n
where
c
is
constant
depending
on
log
d
b
7,1
and
the
surface
area
of
m
cs
is
an
absolute
constant
bounding
iii3
according
to
lemma
2
113
q7
c_pr
q(f
c.pr
m/v
5.52
putting
all
ingredients
together
putting
5.47
5.51
and
5.52
together
and
using
0w
give
rise
to
~
2m
r(mtg
epr
t
cye®m(m
u)v%n’m
log
nlogl/2
1/h
+2cmt9(1-y
mv
exp
mt
3m/v
2n
/h
12a+3d
with
probability
no
less
than
c5v
22a
=252
log®
for
any
and
1/4v
2a
setting
n7
@,y
and
2¢/m
we
get
2a
r(1
c.pr
c4e*m(m
0)n
72asd
log
nlogl/2
1/h
4a
2a
+2cmt9
mn72asd
exp(f(mtféan*m)/h
2a
_2a%+4ad
for
any
2(1
1/m)n
7@
1
with
probability
no
less
than
cgn
7card
log3
n
where
cg
is
an
absolute
constant
in
addition
c.pg
hkn(h
with
ly
prp
kpp
k17
ry
defined
in
4.26
ii
f(ly,p1,ky,%1,ry
for
=1,...,v
with
the
parameters
defined
in
4.24
g€
f(ly
p2
kz
%2
r
with
the
parameters
defined
in
4.25
conclusion
and
discussion
this
paper
establishes
statistical
guarantee
for
doubly
robust
off-policy
learning
by
neural
networks
the
covariate
is
assumed
to
be
on
low-dimensional
manifold
non-asymptotic
regret
bounds
for
the
learned
policy
are
proved
in
the
finite-action
scenario
and
in
the
continuous-action
scenario
29
our
results
show
that
when
the
covariates
exhibit
low
dimensional-structures
neural
networks
provide
fast
convergence
rate
whose
exponent
depends
on
the
intrinsic
dimension
of
the
manifold
instead
of
the
ambient
dimension
our
results
partially
justify
the
success
of
neural
networks
in
causal
inference
with
high-dimensional
covariates
we
finally
provide
some
discussions
in
connection
with
the
existing
literature
sample
complexity
lower
bound
without
low
dimensional
structures
gao
and
han
2020
established
lower
bound
of
the
sample
complexity
for
policy
evaluation
or
treatment
effect
estimation
when
the
covariates
are
in
ir
and
do
not
have
low-dimensional
structures
specifically
they
assume
that
both
the
initial
policy
and
reward
functions
belong
to
holder
space
the
sample
complexity
needs
to
be
at
least
exponential
in
the
dimension
d
this
result
shows
that
the
rate
can
not
be
improved
unless
additional
assumptions
are
made
by
assuming
that
the
covariates
are
on
d-dimensional
manifold
our
sample
complexity
only
depends
on
the
intrinsic
dimension
d
we
remark
that
gao
and
han
2020
studied
the
holder
space
with
holder
index
0,1
while
we
focus
on
the
case
of
1
in
the
case
that
1
if
we
have
in
assumption
a.5
corollary
gives
the
convergence
rate
o(n’ﬁ
log3
n
this
rate
is
better
than
the
minimax
rate
o(n’ﬁ
from
gao
and
han
2020
thanks
to
the
low-dimensional
structures
of
the
covariates
nonconvex
optimization
of
deep
neural
networks
our
theoretical
guarantees
hold
for
the
global
optimum
of
3.4)-(3.8
however
solving
these
optimization
can
be
difficult
in
practice
some
recent
empirical
and
theoretical
results
have
shown
that
large
neural
networks
help
to
ease
the
optimization
without
sacrificing
statistical
efficiency
zhang
et
al
2016
arora
et
al
2019
allen-zhu
et
al
2019
this
is
also
referred
to
as
an
overparameterization
phenomenon
we
will
leave
it
for
future
investigation
references
allen-zhu
z
l1
y
and
liang
y
2019
learning
and
generalization
in
overparameterized
neural
networks
going
beyond
two
layers
in
advances
in
neural
information
processing
systems
aroora
s
du
s
s
hu
w
l1
z
and
wang
r
2019
fine-grained
analysis
of
optimiza
tion
and
generalization
for
overparameterized
two-layer
neural
networks
arxiv
preprint
arxiv:1901.08584
arney
s
and
wager
s
2017
efficient
policy
learning
arxiv
preprint
arxiv:1702.02896
baprista
m
s
calpas
1
l
baptista
m
s
baprrista
c
s
ferreira
a
a
and
heller
m
v
a
2000
low-dimensional
dynamics
in
observables
from
complex
and
higher-dimensional
systems
physica
a
statistical
mechanics
and
its
applications
287
91-99
benkeser
d
ca1r
w
van
der
laan
m
j
et
al
2020
nonparametric
super-efficient
estimator
of
the
average
treatment
effect
statistical
science
35
484-495
benkeser
d
carone
m
laan
m
v
d
and
gilbert
p
2017
doubly
robust
nonparametric
inference
on
the
average
treatment
effect
biometrika
104
863-880
bennert
a
and
karrus
n
2020
efficient
policy
learning
from
surrogate-loss
classification
reductions
arxiv
preprint
arxiv:2002.05153
30
beygelzimer
a
and
langford
]
2009
the
offset
tree
for
learning
with
partial
labels
in
proceedings
of
the
15th
acm
sigkdd
international
conference
on
knowledge
discovery
and
data
mining
cao
w
ts1atis
a
a
and
davipian
m
2009
improving
efficiency
and
robustness
of
the
doubly
robust
estimator
for
population
mean
with
incomplete
data
biometrika
96
723-734
cassgl
c
m
sarndal
c
e
and
wretman
]
h
1976
some
results
on
generalized
difference
estimation
and
generalized
regression
estimation
for
finite
populations
biometrika
63
615-620
chalupka
k
perona
p
and
eberhardt
f
2014
visual
causal
feature
learning
arxiv
preprint
arxiv:1412.2309
chan
k
c
g
yamm
s
c
p
and
zhang
z
2016
globally
efficient
non-parametric
inference
of
average
treatment
effects
by
empirical
balancing
calibration
weighting
journal
of
the
royal
statistical
society
series
b
statistical
methodology
78
673
chen
m
jiang
h
l1ao
w
and
znao
t
2019
efficient
approximation
of
deep
relu
networks
for
functions
on
low
dimensional
manifolds
in
advances
in
neural
information
processing
systens
choly
y
chiu
c
y.-i
and
sontag
d
2016
learning
low-dimensional
representations
of
medical
concepts
amia
summits
on
translational
science
proceedings
2016
41
crump
r
k
horz
v
imbens
g
w
and
mr1nik
o
a
2008
nonparametric
tests
for
treatment
effect
heterogeneity
the
review
of
economics
and
statistics
90
389-405
demirer
m
syrgkanis
v
lewis
g
and
chernozzhukov
v
2019
semi-parametric
efficient
policy
learning
with
continuous
actions
arxiv
preprint
arxiv:1905.10116
dubik
m
langford
j
and
l1
l
2011
doubly
robust
policy
evaluation
and
learning
arxiv
preprint
arxiv:1103.4601
dubley
r
m
1967
the
sizes
of
compact
subsets
of
hilbert
space
and
continuity
of
gaussian
processes
journal
of
functional
analysis
290-330
farias
v
and
l1
a
a
2019
learning
preferences
with
side
information
management
science
65
3131-3149
farrerr
m
h
liang
t
and
misra
s
2018
deep
neural
networks
for
estimation
and
in
ference
application
to
causal
effects
and
other
semiparametric
estimands
arxiv
preprint
arxiv:1809.09953
federer
h
1959
curvature
measures
transactions
of
the
american
mathematical
society
93
418-491
frovlich
m
huser
m
and
wiesenfarth
m
2017
the
finite
sample
performance
of
semi-and
non-parametric
estimators
for
treatment
effects
and
policy
evaluation
computational
statistics
data
analysis
115
91-102
gao
z
and
han
y
2020
minimax
optimal
nonparametric
estimation
of
heterogeneous
treatment
effects
arxiv
preprint
arxiv:2002.06471
31
hartford
j
lewis
g
leyron-brown
k
and
tabpy
m
2017
deep
iv
flexible
approach
for
counterfactual
prediction
in
proceedings
of
the
34th
international
conference
on
machine
learning-volume
70
jmlr
org
heeckmman
j
j
1977
sample
selection
bias
as
specification
error
with
an
application
to
the
estimation
of
labor
supply
functions
tech
rep
national
bureau
of
economic
research
heckman
j
j
and
vyrraci
e
j
2007
econometric
evaluation
of
social
programs
part
i
causal
models
structural
models
and
econometric
policy
evaluation
handbook
of
econometrics
4779-4874
hiwy
j
l
2011
bayesian
nonparametric
modeling
for
causal
inference
journal
of
computational
and
graphical
statistics
20
217-240
horvirz
d
g
and
taompson
d
j
1952
generalization
of
sampling
without
replacement
from
finite
universe
journal
of
the
american
statistical
association
47
663-685
jonansson
e
suatir
u
and
sontag
d
2016
learning
representations
for
counterfactual
inference
in
international
conference
on
machine
learning
karrus
n
2018
balanced
policy
evaluation
and
learning
in
advances
in
neural
information
processing
systems
katrus
n
2020
more
efficient
policy
learning
via
optimal
retargeting
journal
of
the
american
statistical
association
1-13
karrus
n
and
santacatterina
m
2019
kernel
optimal
orthogonality
weighting
balancing
approach
to
estimating
effects
of
continuous
treatments
arxiv
preprint
arxiv:1910.11972
karrus
n
and
znou
a
2018a
confounding-robust
policy
improvement
in
advances
in
neural
information
processing
systems
katrus
n
and
zrou
a
2018b
policy
evaluation
and
optimization
with
continuous
treatments
in
international
conference
on
artificial
intelligence
and
statistics
kennepy
e
h
2020
optimal
doubly
robust
estimation
of
heterogeneous
causal
effects
arxiv
preprint
arxiv:2004.14497
kennnedy
e
h
ma
z
mchugh
m
d
and
smalt
d
s
2017
non-parametric
methods
for
doubly
robust
estimation
of
continuous
treatment
effects
journal
of
the
royal
statistical
society
series
statistical
methodology
79
1229-1245
kim
e
s
hersst
r
s
wistusa
l
1
leg
j
j
blumenschein
g
r
tsao
a
stewart
d
j
hicks
m
e
erasmus
j
gupta
s
et
al
2011
the
battle
trial
personalizing
therapy
for
lung
cancer
cancer
discovery
44-53
kiragawa
t
and
tetenov
a
2018
who
should
be
treated
empirical
welfare
maximization
methods
for
treatment
choice
econometrica
86
591-616
kourourrorss
d
e
and
xanthopoulos
a
2008
reinforcement
learning
and
evolutionary
algo
rithms
for
non-stationary
multi-armed
bandit
problems
applied
mathematics
and
computation
196
913-922
32
kuleshov
v
and
precup
d
2014
algorithms
for
multi-armed
bandit
problems
arxiv
preprint
arxiv:1402.6028
lee
j
2003
introduction
to
smooth
manifolds
graduate
texts
in
mathematics
springer
leg
y
kennedy
e
and
mitraa
n
2020
doubly
robust
nonparametric
instrumental
variable
estimators
for
survival
outcomes
arxiv
preprint
arxiv:2007.12973
liao
w
and
macgaiont
m
2019
adaptive
geometric
multiscale
approximations
for
intrinsically
low-dimensional
data
journal
of
machine
learning
research
20
1-63
lim
b
2018
forecasting
treatment
responses
over
time
using
recurrent
marginal
structural
networks
in
advances
in
neural
information
processing
systems
lorez-paz
d
nishihara
r
chintala
s
scholkopf
b
and
borrou
l
2017
discovering
causal
signals
in
images
in
proceedings
of
the
ieee
conference
on
computer
vision
and
pattern
recognition
luncerorp
j
k
and
davipian
m
2004
stratification
and
weighting
via
the
propensity
score
in
estimation
of
causal
treatment
effects
comparative
study
statistics
in
medicine
23
2937-2960
manoney
m
w
and
dringas
p
2009
cur
matrix
decompositions
for
improved
data
analysis
proceedings
of
the
national
academy
of
sciences
106
697-702
massarr
p
2000
some
applications
of
concentration
inequalities
to
statistics
in
annales
de
la
faculté
des
sciences
de
toulouse
mathématiques
vol
9
maurer
a
2016
vector-contraction
inequality
for
rademacher
complexities
in
international
conference
on
algorithmic
learning
theory
springer
mcdiarmip
c
1989
on
the
method
of
bounded
differences
surveys
in
combinatorics
141
148-188
nivoar
p
smale
s
and
weinberger
s
2008
finding
the
homology
of
submanifolds
with
high
confidence
from
random
samples
discrete
computational
geometry
39
419-441
pevre
g
2009
manifold
models
for
signals
and
images
computer
vision
and
image
understanding
113
249-260
puam
t
t
and
shen
y
2017
deep
causal
inference
approach
to
measuring
the
effects
of
forming
group
loans
in
online
non-profit
microfinance
platform
arxiv
preprint
arxiv:1706.02795
richardson
a
hudgens
m
g
gilbert
p
b
and
fine
p
2014
nonparametric
bounds
and
sensitivity
analysis
of
treatment
effects
statistical
science
review
journal
of
the
institute
of
mathematical
statistics
29
596
robins
j
m
rotnitzky
a
and
zhao
l
p
1994
estimation
of
regression
coefficients
when
some
regressors
are
not
always
observed
journal
of
the
american
statistical
association
89
846-866
rowers
s
t
and
saut
l
k
2000
nonlinear
dimensionality
reduction
by
locally
linear
embed
ding
science
290
2323-2326
rusin
d
b
1974
estimating
causal
effects
of
treatments
in
randomized
and
nonrandomized
studies
journal
of
educational
psychology
66
688
33
sharma
a
horman
j
m
and
warrts
d
j
2015
estimating
the
causal
impact
of
recommenda
tion
systems
from
observational
data
in
proceedings
of
the
sixteenth
acm
conference
on
economics
and
computation
swaminathan
a
and
joachms
t
2015
batch
learning
from
logged
bandit
feedback
through
counterfactual
risk
minimization
the
journal
of
machine
learning
research
16
1731-1755
tenenbaum
j
b
de
silva
v
and
lancrorp
j
c
2000
global
geometric
framework
for
nonlinear
dimensionality
reduction
science
290
2319-2323
tsybakov
a
b
2008
introduction
to
nonparametric
estimation
springer
science
business
media
tsybakov
a
b
et
al
2004
optimal
aggregation
of
classifiers
in
statistical
learning
the
annals
of
statistics
32
135-166
tu
l
2010
an
introduction
to
manifolds
universitext
springer
new
york
van
amsterdam
w
verhoeff
j
de
jong
p
leiner
t
and
enjkemans
m
2019
eliminating
biasing
signals
in
lung
cancer
images
for
prognosis
predictions
with
deep
learning
npj
digital
medicine
1-6
wainwright
m
j
2019
high-dimensional
statistics
non-asymptotic
viewpoint
vol
48
cam
bridge
university
press
wang
y
and
singh
a
2016
noise-adaptive
margin-based
active
learning
and
lower
bounds
under
tsybakov
noise
condition
in
thirtieth
aaai
conference
on
artificial
intelligence
warrd
a
zhou
z
bambos
n
wang
e
and
scheinker
d
2019
anesthesiologist
surgery
assignments
using
policy
learning
in
icc
2019-2019
ieee
international
conference
on
communni
cations
icc
ieee
wasserman
l
2013
all
of
statistics
concise
course
in
statistical
inference
springer
science
business
media
zhang
b
tsiatis
a
a
davipian
m
zhang
m
and
laber
e
2012
estimating
optimal
treatment
regimes
from
classification
perspective
stat
103-114
zuang
c
benaio
s
haror
m
rechr
b
and
vinyats
o
2016
understanding
deep
learning
requires
rethinking
generalization
arxiv
preprint
arxiv:1611.03530
zhang
s
ya0
l
sun
a
and
tay
y
2019
deep
learning
based
recommender
system
survey
and
new
perspectives
acm
computing
surveys
csur
52
1-38
zhao0
y
zeng
d
rush
a
j
and
kosorok
m
r
2012
estimating
individualized
treatment
rules
using
outcome
weighted
learning
journal
of
the
american
statistical
association
107
1106-1118
zuao
y.-q
zeng
d
laber
e
b
song
r
yuan
m
and
kosorok
m
r
2015
doubly
robust
learning
for
estimating
individualized
treatment
with
censored
data
biometrika
102
151-168
zuou
z
athey
s
and
wager
s
2018
offline
multi-action
policy
learning
generalization
and
optimization
arxiv
preprint
arxiv:1810.04778
34
supplementary
materials
for
doubly
robust
off-policy
learning
on
low-dimensional
manifolds
by
deep
neural
networks
proof
of
lemma
proof
of
lemma
1
we
first
derive
the
error
bound
||?ia
f"a]”lz
for
any
1,...,|a
note
that
fai
f(l1,p1,kq
k1
ry
is
the
minimizer
of
3.4
if
we
choose
_d
_d
ly
ollogny
py
o(nf
)
ky
o(n
logny
)
x
max(b,m
va
%
ry
=m
a1
then
according
to
chen
et
al
2019
theorem
1
for
each
j
we
have
e[lifia
pa
2
c(m
02)n
log®
ny
a2
where
na
2:21
lia;=a
and
c
is
constant
only
depending
on
log
d
b
and
the
surface
area
of
m
in
a.2
the
expectation
is
taken
with
respect
to
the
randomness
of
samples
next
we
derive
high
probability
lower
bound
of
1
for
all
j’s
in
terms
of
n
by
assumption
a.3(ii
le(na]/nl
1
by
liao
and
maggioni
2019
lemma
29
we
have
p(|24
24
24
<2
e
g2
le
24
ae(a
=20\
)
=pl
ny
ny
thus
14
#n;/2
holds
with
probability
at
least
2exp
(
#n
)
denote
the
event
e
{ny
iz
p\—38
nny/2}
and
its
complement
by
eic
when
#
so
as
1
is
large
enough
we
have
b
b
il
b
ieiee
6
cy(m
+02)(qnl)’%ﬂ
log®(nyny
2cy
m
+02)exp
23811711
<cy(m?+
02)(71711)’%7
logs(qnl
where
c
is
constant
depending
on
log
d
b
and
the
surface
area
of
m
substituting
4
=nm
into
a.1
gives
rise
to
ﬁai
f(ly,p1,ky
k1
ry
with
ly
py
ky
k1
ry
in
4.3
ja]-1
vectors
whose
elements
are
in
h%(m
we
denote
g
gal’--~’ga|a|,1]t
with
g4
=log
i
accord
14
ing
to
assumption
a.4
g4
=loge
logey
h%
llga
llye
<m
and
g
hltf‘l\fl(m
let
be
the
minimizer
of
3.5
from
maurer
2016
corollary
4
and
the
proof
of
farrell
et
al
2018
theorem
2
setting
gyn
l
p
k
r
gives
rise
to
to
estimate
e[l@
l’allliz
we
use
m
to
denote
the
space
of
the
|a|
1
dimensional
lklogk
al(logl
ig-g"liz
csm
wn—ogbgnﬁw+
sup
inf
lg
85
nn
sr
m)see
with
probability
at
least
exp(—y
where
cj
is
an
absolute
constant
35
according
to
chen
et
al
2019
theorem
2
for
any
¢
0
1
there
exists
neural
network
architecture
l
p
k
x
r
with
vi
l=0|log
p:o(iaisz
k:o
ale
log
|«
max(b
m
vd
%
such
that
for
any
h{
m
there
exists
f(l,p,k
k
r
with
||g
gl
€5
where
||g]l
a]-1
supxepmax;lgi(z)|
setting
&5
|a|
1y
2
=|al
ny
gives
rise
to
gnn
lo
pa
ky
k2
rp
with
ollog(m/ad
p
o(|a|*ziid
n7
a1
han
log(m
/14d
k
max{b,m
vd
%
ry=m
which
implies
4.4).then
with
probability
no
less
than
exp
a~
wn
“
we
deduce
g1
campmai70
log
my
with
c4
depending
on
log
d
b
and
the
surface
area
of
m
denote
the
event
ex
{18
cam
a0
log
my
when
n
so
as
n
is
large
enough
we
obtain
eiiz
1
blig
ejp(e2
elie
is
ip(es
cumplaie
1,7
og
ny
m2
o
exp
a1
#0777
csm2|
al
hi%
log®n
with
c5
depending
on
log
d
b
and
the
surface
area
of
m
exp(g
1+
exp(igl
e[llea
eallf
e[li7@
g7
2
a2t
<e
198
g"nie
csmait
75
define
g
forj=1,...,|a|-1
since
||vrj|l
for
any
j
we
have
2a
da_
3od
similarly
one
can
show
ie[h'eami
eawlle
csm?|a|z=an
log®
n
proof
of
lemma
proof
of
lemma
2
recall
that
qp
(
fm([mm»...,mv<m>]in<z>)dn’(m>
q(m
fm([m
@
pa
@)]t
@
dp
since
l
is
uniform
lipschitz
constant
of
u(z
for
any
m
we
derive
q®)(r0
q(n
jm([m
@)=
@
ay
.
pr
@
@
ay)]t
@
@
l,/
v
36
proof
of
lemma
proof
of
lemma
3
we
first
use
mcdiarmid’s
inequality
lemma
10
to
show
d(it
concentrates
around
e[d(it
and
then
derive
bound
of
ee[d(it
to
simplify
the
notation
we
omit
the
domain
itin
d
we
denote
{10"1.’}17’:1
as
the
counterpart
of
{iﬂ'}g’:1
when
one
sample
xy,tj
is
replaced
by
x4
t}
for
any
with
<k
<n
a’(ry,15
and
d
are
defined
analogously
we
have
/e
id-d<
sup
ary
=
a(ry
sup
b~
1y
70y
@
o
70,1
€ll
701,15
€l1
im0
ol
2b
where
||
||
and
||-||
stand
for
the
£
and
¢
norm
for
vectors
applying
lemma
10
with
=d
we
have
ip(d
e[d
1
exp(-2nt?/(16
c.2
setting
4
gives
rise
to
e[d]+
4/
082
c.3
with
probability
no
less
than
1-6
we
next
derive
bound
of
e[d
by
symmetrization
e[d]=e
sup
5(711,712)7113[5(711,712)]]&13
sup
5(711,712)*&0”(711,712
7y
m2€l
7,m
€t
=ee;|
sup
£0
a(nl,nz
&copy(nl,nz
=2ee¢|
sup
50&(711,712
701,701
701,71
where
acopy
denotes
using
independent
copies
of
samples
and
&,...,&,]t
with
&;’s
being
i.i.d
rademacher
variables
which
take
value
or
1
with
the
same
probability
here
denotes
the
entry-wise
product
of
and
a
i.e
cohm,m
&(fm
(
(
i=1
we
next
apply
lemma
10
with
ieg[supnllnzen
505(711,712
again
we
denote
{10"1-'}1-:1
as
the
counterpart
of
{iﬂ'}?’:1
when
one
sample
xy
iy
is
replaced
by
n;(,fk
for
any
with
<k
<n
a'(10),705
is
defined
analogously
we
get
e¢|
sup
a(ny,my
eg|
sup
565’(711,712
70,10
€lt
701,10
€lt
l
<e¢|
sup
%(rrrk:m(mk)*ﬂz(mk
70,10
€lt
1e
=b
@
ol
c.4
37
applying
lemma
10
with
es[supnl,nzenﬁok(nl,m
gives
rise
to
ieie,s
sup
£0a(ny,my
70,1
€ll
ieg
sup
eoa(nl,nz
zt]gexp(—zntz/(w]z
c.5
701,10
€lt
lob
1/(5
setting
4
gives
rise
to
ele,s
sup
£0a(m
4
<6
c.6
71,1
€ll
ie,s
sup
£0a(ry
my
701,15
€ll
log1/6
2n
the
following
lemma
provides
an
upper
bound
of
eﬁ
sup
56&(711,712
see
proof
in
70,1
appendix
f
lemma
6
let
be
set
of
rademacher
random
variable
and
a(rt;,71
defined
in
5.12
then
the
following
bound
holds
max|lrl
155
sup
foa(nl,n2)]<1nf
zufj’f
jlog
0,11
1ir)d6
c.7
701,10
€t
where
n(g,hk“i‘\i
|[lir
is
the
6-covering
number
see
definition
6
of
it
with
respect
to
the
measure
il
/&
xy
t
m(ai)2
substituting
c.7
into
c.6
yields
max]|7|p
e[d
inf
4/\+
jlog
o
l
|1hi)d0
8
logl/b
c.8
with
probability
no
less
than
1-6
substituting
c.8
into
c.3
give
rise
to
meaan|
log
<inf
4a+tj
jlog
0,1
|i)d6
+12
°g
/b
c.9
with
probability
no
less
than
24
proof
of
lemma
proof
of
lemma
4
we
derive
the
bound
of
the
covering
number
n(q,hll\’?ll\
|[|lr
using
the
covering
number
of
the
neural
network
class
0
l
p
k
%,r
||
||l
let
=}
softmax(,ufqll
ai\l\
and
7
softmax(yfl),...,yiil
be
two
policies
in
hk“i‘\i(lh,ph,kh,kh,rh
such
that
for
each
j
||;4541
7;41;2)”00
0
by
assumption
a.3
and
4.1
||f,-||1
2m/n
+|a|m
for
any
ny
n
therefore
we
have
||n(1
r
n2
n(z
>2
i=ny+1
<
=@
a)|2
aim
2m/m
i=n+1
38
thus
we
obtain
o
ihi
6/aim
+2m/1
tr
l
d.1
1a
since
for
every
hn
with
ly
l
prp
|alp
jaik
k11
&
ry
r
it
contains
|a|
parallel
relu
networks
in
f(l,p,k
k,r
with
an
additional
softmax
layer
we
have
0
hlnai‘\i,h
llo
n(@,f(l,p,k,k,r),||-||w)|a
from
chen
et
al
2019
proof
of
theorem
3.1
we
have
2l2(pr
+2)xclpl
k
nof(lp
25
we
get
l
l+1\mik
2l*(pr+2)x"p
d.2
nl
222
combining
d.1
and
d.2
proves
lemma
4
proof
of
lemma
proof
of
lemma
5
for
any
7t
i—i‘i\’ﬁl\l
o~
tomla
trl
i=n;+1
ny+1
i=n;+1
by
assumption
a.3
and
4.1
||i'||1
2m/n
+|a|m
for
any
ny
n
therefore
we
obtain
||n||rs(2m/r]+|a|m
proof
of
lemma
we
first
define
the
covering
number
of
set
definition
6
let
be
set
equipped
with
metric
p
for
any
0
6-covering
of
is
set
{fi
fn}
such
that
for
any
f
there
exists
f
for
<k
with
p(f
f
6
the
6-covering
number
of
is
defined
as
6
f,p
=inf{n
there
exists
{f}
fy}
which
is
o-covering
of
f}
e1
proof
of
lemma
6
to
bound
e
sup
£0
a(tzi,rz
with
respect
to
the
measure
||-||r
we
construct
70y
€ll
series
of
coverings
of
it
with
resolutions
{9
}1
satisfying
0;,1
the
elements
in
the
i)-th
covering
are
denoted
as
{n(l)}nm
where
the
n’s
are
to
be
determmed
later
thus
for
any
it
there
exists
7
in
the
i)-th
covering
such
that
ly
fmn(a
n0(a
<o
i=1
39
let
711
denote
the
closest
element
of
11
in
the
i)-th
covering
and
n2
is
defined
analogously
we
now
expand
71
1
using
telescoping
sum
1
1
m
ng
+
zr(ihi
n(ll
+n(11)]—[n2
771(21
zn(;l
n
71(21
e2
i=1
i=1
substituting
f.2
into
e
sup
£€®
a(my
712
due
to
the
bi-linearity
of
a
we
have
71,1
€l1
1e§
sup
£®5(n1,n2
701,70
€lt
1
<e¢|sup
z§<,,[m
711
+z
g’](m,.>>
nel’i
i-1
+e¢|sup
zé
1,[n2—n‘2)+
n‘z’”’fﬂ‘z”m‘;’](zi)ﬂ
r3
el
o1
by
the
construction
of
the
coverings
we
immediately
have
cop
r(leh
"
sup
el
fi
l
<o
f4
i=1
<e
we
can
also
check
ji<ﬁ'ﬂ(i+l)(mi
71“”(%‘))2
<f},n‘””(mi
(
(
n‘”(%‘))z
i=1
<\
2
0@
nla
2
gt
7o
i=1
i=1
20(82
+6%
v2n(8i4y
;
e5
using
lemma
11
we
have
sup
zé
bl
n‘f’(:m
nlel'l
<2
i+1
0;)y10g(n
05
tl
hip
nv
0541
tl
ilp
4(0i41
+6i)y1og
0;1
1
ilr
vi
vi
e.6
where
the
metric
in
the
covering
is
||7t||p
<i‘,,7z
>
substituting
f.4
e.6
into
e.3
40
and
invoking
the
identity
6;,1
0
6(d;11
9;42
yield
<25+
ois1
0i)y/10og
6141
ir
eﬁ
sup
§®a
ty
t05
701,71
\/ﬁ
48
viog
87,1
i1
o488
™
<26
0i41—0i42
\‘/)gg
i+
il
|flp
2b’+ﬁj
llogn(t,h,h-hr)dt
choosing
6
maxxy
||7||y
so
that
the
first
covering
only
consists
of
one
element
we
derive
o~
1e§
sup
eoa(thjh
1nf
20+
jlog
6,tl|hir)d6
701,10
€t
some
useful
lemmas
lemma
7
let
f(x,a
be
any
function
defined
on
0
1
assume
there
exists
such
that
sup
|f(allzar
<m
and
sup|f(x,a
f(x,a)|
m|a-a
ya
a€
0,1
g.1
a€[0,1
xzem
then
fu
'
f(x,a)da
h*(m
satisfies
||f“)||hh(m
m|i|
for
any
interval
0,1
where
|i|
is
the
length
of
i
proof
of
lemma
7
to
show
f{
h*(m
it
is
sufficient
to
show
||f(”||ha(u
oo
for
any
chart
u
¢
of
m
for
simplicity
we
denote
fy(2
fd
o0
g7
2
fy(2,4
97
2),4
for
p(u
thenf
j(z
[
fo(z
a)da
we
first
cons1der
1
in
this
case
we
have
ifs)(2
e
l
i
||f
||
su
io
ynz
jlf
zﬁfa%
da
mji|
oo
g.2
zzys
which
implies
fd
hy(m
next
we
consider
1
we
first
show
that
qsfu
'
bsf(p
z,a)da
for
any
|s|
a—17
where
=[st,0]t
let
{h,};2
be
any
sequence
convergmg
to
0
when
s|
1
by
definition
we
have
fy
z+hys
ij
z+h,,s
a
f(,,(z,a)da
since
|f¢(2
a)llne(p(uy
for
any
fixed
0,1
by
the
mean
value
theorem
f({)(z
+hys,a
f({)(z,a
<max|¢9f¢za|<m
zzep(u
41
since
h,s,a
fy(z
lim
°fy(2,a
g.3
i
and
by
the
dominated
convergence
theorem
we
obtain
fyl(z+hys
fy(2
zz)=1lm
—
j°f
n—o0
hl’l
w(z+h,8,a
fy(z,a
lim
w(m
jasf(,)(z,a)da
ln~>oo
similarly
for
any
|s|
a
17
d°f
x
a
can
be
expressed
in
the
form
similar
to
g.3
using
the
taylor
series
following
the
same
procedure
one
can
show
j&gﬁf,(z,a)da
max
sup
|35
m|i|
oo
g.4
sl<fa
uzep(u
for
any
|s|
a
1
therefore
we
have
where
|i|
represents
the
length
of
i
on
the
other
hand
|85f‘”<
9°fy
)
max
mmmm
lsi=faoyepv
flz-ylly
2
a
ffy(y
max
jl
fo
f‘”ly
da
m|i|
co
g.5
a1
oy
llz
3
combining
g.4
and
g.5
gives
||f
||h1x
oo
for
any
chart
u
¢
which
implies
f
h*(m
lemma
8
assume
assumption
2
let
f,g
h*(m
with
inf,cr
g(x
17>
0
let
>0
be
constant
such
that
||
fl3a
1
and
||gll3a
1
m
then
we
have
f/g
h*(m
with
|f/gllya
(
225
m
2b
1
proof
of
lemma
8
to
prove
lemma
8
it
is
sufficiently
to
show
||f/glly«(y
oo
for
any
chart
u
¢
of
m
for
simplicity
denote
folz
¢
2
gp(2
g0
7!(2
for
any
p(u
we
first
consider
1
in
this
case
||z—y||2
mo(2)80(¥
f(2)89(2
fip(2)84
2
fp
w8
=
8oy
gq
z)llz-ylls
|g<{)(y)7g<{
z)|
|f
f(/
y
“
lz-yll5
llz-ylls
l
<2m%
/p
<o
g.6
42
which
implies
f/g
h*(m
we
next
consider
the
case
1
we
first
show
9°(f(2)/g4(2))|
oo
for
|s|
&
1
when
|s|
=1
we
have
as(ﬁp(z
3¢(2
for
any
|s|
a
1
following
this
process
one
can
show
85
fo(2
8¢(2
where
each
g
is
the
product
of
2/l
terms
from
{9§ﬂp,3§f¢||§|
<|s|}
on
the
other
hand
note
that
for
any
with
|s|
1
we
have
as(f(p(z))fas(f«p(y
2s(2
(
fol2)2(2
f5(2)9°85(2
o*fs
20
w)~
f¥)2°gp(w
g(p<
g¢<
y
9°f(2)89(2
85(2)9°
f3
u)g9
4
85
1
2)9°84(2
£5(2
1
v
22(2)83(v
sq—4|g¢(y)g<p(z)[9sf¢(z)g(p(y)*asﬁp(z
2)+
9%
f3(2)8(2
0°
fy
2
+85(y)f3(2)9°g(2
f{
19°84(y
85
(
fy
2
984
85
w
fply
9&{
85
(
¥)9°20(y
85
w89
2
fp
1
84
(
89
2
2)fp(¥)9°
84
y
85(2
f(w
|
asf
g<{
f/
9
g<{
z
85(2
2m2/r]2
sl
zzt
1+|s|
<2
sl
v/
oo
g.7
g(?)\-ﬂ
s%h&p(z)*g(p(yﬂ+|f¢(z)*ﬁ{>(y)|+|9sg<p
gow)l+10°
fy(2
rm(yn
sm—f[4m||z—y||+|35g¢<z)—95g¢<y>|+|95ﬁp<z>—35ﬂp<y>|
analogously
for
any
|s|
a
17
one
can
show
as(ﬁ,xz))fas(f«p(y
)
8¢(2
8o(y
<(m/np
cumilz
yll+
cold°
gy
2
g4
w)l
c10°
fy(2
fy(w
54]s|
for
some
absolute
constants
c
cy
c3
such
that
c
c3
272
18|
thus
we
deduce
|‘95(f<f>(z)/g
2
=0
fp(u)/gply
>)|
su
_
ol<tart
ata-t
sl<fa=1z2yep(u
iz
yll
<m/
20
mb
+(cy
c)m
275
fa=1(m/n
2b
+1
o0
g.8
43
combining
g.7
and
g.8
yields
5+a
/8l
<25
12
m/
2b
+1
o0
g.9
for
any
chart
u
¢
of
which
implies
f/g
h*(m
lemma
9
assume
assumption
2
let
h*(m
with
and
f(z
0
let
be
constant
such
that
||flyey
m
then
we
have
logf
h%(m
with
log
fllya(ny
2215
a2
m
2b
1
proof
of
lemma
9
1t
is
sufficiently
to
show
||log
fll3(ys
oo
for
any
chart
u
¢
of
m
for
simplic
ity
denote
fy(2
fo
~1(z
for
any
u
we
further
denote
such
that
||fll
(
m
for
any
|s|
1
9°log
fy(2
a;‘{‘("z()z
note
that
0°
f(z
h*}(¢(u
according
to
lemma
8
et
i
g.10
fo
for
any
|s|
1
combining
g.10
and
9°fy(2
|<m/y
lol=l2€(u
fo(2
5+[a-2
we
have
|log
f|la(y
oo
with
||10g
fll3e
272
m’2](m/r1)2“k“(23
+1
which
proves
lemma
9
the
following
two
lemmas
are
extensively
used
in
the
previous
proofs
lemma
10
mcdiarmid’s
inequality
mcdiarmid
1989
let
x1
&
be
independent
ran
dom
variables
and
x
be
map
if
for
any
and
ml,...,mn,m
x
the
following
holds
@1
i1
@iy
tits
t
b
t
tig1
0
|
g
then
for
any
0
212
p(f(zy,...,z
e[f
]
exp
cz]~
i=1%
lemma
11
massart’s
lemma
massart
2000
let
be
some
finite
set
in
r
and
y
be
independent
rademacher
random
variables
then
sup
zsix
xex
o1
y/2log|x|
pramt
sup|la]|
zex
44
