europe
pmc
funders
group
author
manuscript
mach
learn
res
author
manuscript
available
in
pmc
2020
january
28
published
in
final
edited
form
as
mach
learn
res
2019
20
causal
learning
via
manifold
regularization
steven
m
hill#
mrc
biostatistics
unit
university
of
cambridge
cambridge
cb2
0sr
uk
chris
j
oates
school
of
mathematics
statistics
and
physics
newcastle
university
newcastle-upon-tyne
ne1
7ru
uk
steven
m
hill
steven.hill@mrc-bsu.cam.ac.uk
chris
j
oates
chris.oates@ncl.ac.uk
duncan
a
blythe
sach
mukherjee
german
center
for
neurodegenerative
diseases
53127
bonn
germany
duncan
a
blythe
duncanblythe
@googlemail.com
sach
mukherjee
sach.mukherjee
@dzne.de
peter
spirtes
these
authors
contributed
equally
to
this
work
sydurosnuepy
joyiny
sropun
da
odoing
abstract
this
paper
frames
causal
structure
estimation
as
machine
learning
task
the
idea
is
to
treat
indicators
of
causal
relationships
between
variables
as
labels
and
to
exploit
available
data
on
the
variables
of
interest
to
provide
features
for
the
labelling
task
background
scientific
knowledge
or
any
available
interventional
data
provide
labels
on
some
causal
relationships
and
the
remainder
are
treated
as
unlabelled
to
illustrate
the
key
ideas
we
develop
distance-based
approach
based
on
bivariate
histograms
within
manifold
regularization
framework
we
present
empirical
results
on
three
different
biological
data
sets
including
examples
where
causal
effects
can
be
verified
by
experimental
intervention
that
together
demonstrate
the
efficacy
and
general
nature
of
the
approach
as
well
as
its
simplicity
from
user’s
point
of
view
keywords
causal
learning
manifold
regularization
semi-supervised
learning
interventional
data
causal
graphs
introduction
causal
structure
learning
is
concerned
with
learning
causal
relationships
between
variables
such
relationships
are
often
represented
using
directed
graphs
with
nodes
corresponding
to
syduosnuey
j0yiny
sipung
dn
2doinyg
cc-by
4.0
see
https://creativecommons.org/licenses/by/4.0/
attribution
requirements
are
provided
at
http://jmlr.org/papers/
v20/18-383
html
code
availability
all
computational
analysis
was
performed
in
r
core
team
2018
source
code
for
mrcl
and
scripts
to
generate
the
empirical
results
presented
in
section
are
available
at
https:/github.com/steven-m-hill/mrcl
author
contributions
cjo
db
and
sm
developed
methodology
based
on
original
ideas
by
sm
smh
performed
the
computational
work
and
cjo
contributed
theory
cjo
and
sm
wrote
the
paper
with
input
from
smh
and
db
sydurosnuepy
joyiny
sropun
da
odoing
syduosnuey
j0yiny
sipung
dn
2doinyg
hill
et
al
page
the
variables
of
interest
consider
set
of
variables
or
nodes
indexed
by
v={1
.
p}
the
aspect
we
focus
on
in
this
paper
is
to
determine
for
each
ordered
pair
7
)
vx
v
whether
or
not
node
exerts
causal
influence
on
node
in
particular
our
focus
is
on
the
binary
detection
problem
of
learning
whether
or
not
node
exerts
causal
influence
on
node
/
rather
than
estimation
of
the
magnitude
of
any
causal
effect
methods
for
learning
causal
structures
can
be
usefully
classified
according
to
whether
the
graph
is
intended
to
encode
direct
or
total
ancestral
causal
relationships
for
example
if
variable
acts
on
which
in
turn
acts
on
c
has
an
ancestral
effect
on
c(via
b
here
the
graph
of
direct
effects
has
edges
b
c
while
the
graph
of
total
or
ancestral
effects
has
in
addition
the
edge
c
methods
based
on
causal
directed
acyclic
graphs
dags
are
natural
and
popular
choice
for
causal
discovery
spirtes
et
al
2000
pearl
2009
the
pc
algorithm
spirtes
et
al
2000
is
an
important
example
of
such
method
using
sequence
of
tests
of
conditional
independence
the
pc
algorithm
estimates
an
underlying
causal
dag
due
to
the
fact
that
the
graph
may
not
be
identifiable
the
output
is
an
equivalence
class
of
dags
encoded
as
completed
partially
directed
acyclic
graph
or
cpdag
here
the
estimand
is
intended
to
encode
direct
influences
ida
intervention
calculus
when
the
dag
is
absent
maathuis
et
al
2009
uses
the
pc
output
to
bound
the
quantitative
total
causal
effect
of
any
node
on
any
other
node
;
these
estimated
effects
can
be
thresholded
to
provide
set
of
edges
fci
fast
causal
inference
spirtes
et
al
2000
and
rfci
really
fast
causal
inference
colombo
et
al
2012
consider
type
of
ancestral
graph
as
estimand
and
allow
for
latent
variables
greedy
interventional
equivalence
search
gies
hauser
and
biihlmann
2012
is
score-based
approach
that
allows
for
the
inclusion
of
interventional
data
methods
for
learning
causal
structures
such
as
those
above
are
often
rooted
in
data
generating
causal
models
in
quite
different
vein
there
have
been
some
interesting
recent
efforts
in
the
direction
of
labelling
pairs
of
variables
as
causal
or
otherwise
such
as
in
lopez-paz
et
al
2015
and
mooij
et
al
2016
these
approaches
are
discriminative
in
spirit
in
the
sense
that
they
need
not
be
rooted
in
an
explicit
data-generating
model
rather
the
emphasis
is
on
learning
how
to
tell
causal
and
non-causal
apart
our
work
is
in
this
latter
vein
we
address
specific
aspect
of
causal
learning—that
of
estimating
edges
in
graph
encoding
causal
relationships
between
defined
set
of
vertices—but
via
machine
learning
approach
that
allows
the
inclusion
of
any
available
information
concerning
known
cause
effect
relationships
the
output
of
our
method
is
directed
graph
that
need
not
be
acyclic
see
spirtes
1995
richardson
1996
hyttinen
et
al
2012
for
discussion
of
cyclic
causality
and
whose
edges
may
encode
either
direct
or
total/ancestral
relationships
as
discussed
below
the
main
differences
between
our
work
and
previous
work
on
labelling
causal
pairs
lopez-paz
et
al
2015
mooij
et
al
2016
are
the
specific
methods
and
associated
theory
that
we
put
forward
the
manifold
regularization
framework
and
the
empirical
examples
in
general
terms
the
idea
is
as
follows
let
denote
the
available
data
and
denote
any
available
knowledge
on
causal
relationships
among
the
variables
indexed
in
v
e.g
based
on
background
knowledge
or
experimental
intervention
we
view
the
causal
learning
task
in
terms
of
constructing
an
estimator
of
the
form
g(2
®
where
gis
directed
graph
with
mach
learn
res
author
manuscript
available
in
pmc
2020
january
28
sydurosnuepy
joyiny
sropun
da
odoing
syduosnuey
j0yiny
sipung
dn
2doinyg
hill
et
al
page
vertex
set
vand
edge
set
eé
with
7
hg
corresponding
to
the
claim
that
variable
has
causal
influence
on
variable
;
to
put
this
another
way
entries
in
binary
adjacency
matrix
encoding
causal
relationships
are
treated
as
labels
in
machine
learning
sense
from
this
point
of
view
the
task
of
constructing
the
estimator
g(%
®
is
essentially
one
of
learning
these
labels
from
available
data
and
from
any
priori
known
labels
derived
from
®
thus
key
difference
with
respect
to
number
of
existing
methods
is
the
nature
of
the
inputs
needed
our
approach
requires
causal
background
information
as
an
input
while
several
existing
methods
such
as
pc
use
only
observational
data
the
casual
background
information
need
not
be
interventional
data
per
se
but
must
encode
knowledge
on
some
causal
relationships
in
the
system
we
consider
both
scenarios
in
empirical
examples
below
note
also
that
in
our
approach
the
causal
status
of
multiple
pairs
is
coupled
via
the
learning
scheme
loosely
speaking
see
below
for
technical
details
it
is
the
position
of
test
pair
on
classification
manifold
relative
to
other
pairs
that
determines
its
status
our
approach
differs
in
several
ways
from
graphical
model-based
methods
in
our
approach
the
same
framework
can
be
used
to
estimate
either
direct
or
ancestral
causal
relationships
depending
on
the
precise
input
we
show
real
data
examples
of
both
tasks
below
this
is
because
the
classifier
can
be
agnostic
to
the
label
semantics
provided
the
bayes
risk
for
the
label
of
interest
is
sufficiently
low
these
labels
can
in
principle
be
learned
in
contrast
to
much
of
the
literature
our
approach
does
not
try
to
provide
full
data-generating
model
of
the
causal
system
but
instead
focuses
on
the
specific
problem
of
learning
edges
encoding
causal
relationships
as
we
see
in
experiments
below
this
can
lead
to
good
empirical
performance
but
the
output
is
in
sense
less
rich
than
full
causal
model
see
the
discussion
our
work
is
motivated
by
scientific
problems
where
good
performance
with
respect
to
this
narrower
task
can
be
useful
in
reducing
the
hypothesis
space
and
targeting
future
work
the
remainder
of
the
paper
is
organized
as
follows
we
first
introduce
some
notation
and
discuss
in
more
detail
how
causal
learning
can
be
viewed
as
semi-supervised
task
we
then
discuss
specific
instantiation
of
the
general
approach
based
on
manifold
regularization
using
simple
bivariate
featurization
using
this
specific
approach—which
we
call
manifold
regularized
causal
learning
mrcl)—we
present
empirical
results
using
three
biological
data
sets
the
results
cover
range
of
scenarios
and
include
examples
with
explicitly
interventional
data
methods
2.1
notation
let
v={1
.
p}
index
set
of
variables
whose
mutual
causal
relationships
are
of
interest
let
denote
directed
graph
with
vertex
set
vand
edge
set
£
where
useful
we
use
v(g
e(g
to
denote
its
vertex
and
edge
sets
and
a(g
to
denote
the
corresponding
pxp
binary
adjacency
matrix
to
make
the
connection
between
causal
relationships
and
machine
learning
more
transparent
we
introduce
linear
indexing
by
4
of
the
pairs
7
)
vx
v
where
needed
we
make
the
correspondence
explicit
denoting
by
4
/1£
the
variable
pair
corresponding
to
linear
index
4
and
by
4(z
/
the
linear
index
for
pair
4
j
suppose
is
the
adjacency
matrix
of
the
unknown
graph
of
interest
let
yj4
{-1
+1}
be
binary
mach
learn
res
author
manuscript
available
in
pmc
2020
january
28
hill
et
al
page
variable
for
convenience
mapped
onto
{—1
+1}
corresponding
to
the
entry
4
j14
in
a
these
yi’s
are
the
labels
or
outputs
to
be
learned
available
data
are
denoted
available
priori
knowledge
about
causal
relationships
between
the
variables
v'is
denoted
®
2.2
causal
semantics
given
data
and
background
knowledge
we
aim
to
construct
an
estimate
g
the
latter
being
directed
graph
that
need
not
be
acyclic
the
information
in
guides
the
learner
two
main
cases
arise
both
of
which
we
consider
in
experiments
below
total
or
ancestral
effects
here
contains
information
on
total
effects
for
example
via
interventional
experiments
as
performed
in
biology
and
the
edges
in
the
estimate
are
intended
to
describe
such
effects
this
means
that
an
edge
4)e
e(é
is
interpreted
to
mean
that
node
7is
inferred
to
be
causal
ancestor
of
node
j
direct
effects
here
contains
information
on
direct
effects
relative
to
the
variable
set
v
and
the
edges
in
the
estimated
graph
are
intended
to
describe
direct
effects
then
an
edge
7
e(é
is
interpreted
to
mean
that
7is
inferred
to
be
direct
cause
of
relative
to
the
variable
set
v
our
immediate
motivation
comes
from
the
experimental
sciences
and
we
focus
in
particular
sydurosnuepy
joyiny
sropun
da
odoing
on
causal
influences
that
can
at
least
in
principle
be
experimentally
verified
even
in
the
presence
of
latent
variables
and
where
causal
cycles
are
possible
as
is
often
the
case
in
biology
or
economics
see
e.g
hyttinen
et
al
2012
accordingly
we
do
not
demand
acyclicity
in
our
empirical
work
in
biology
the
nature
of
the
underlying
chemical/physical
systems
means
that
there
are
many
small
magnitude
causal
effects
that
are
essentially
irrelevant
in
the
scientific
context
and
this
is
characteristic
of
many
problem
settings
in
the
natural
and
social
sciences
this
motivates
pragmatic
approach
assuming
that
estimated
graphs
are
not
very
dense
or
fully
connected
nor
necessarily
transitive
2.3
semi-supervised
causal
learning
with
the
notation
above
the
task
is
to
learn
the
yj4’s
using
and
®
this
is
done
using
semi-supervised
estimator
y14(2
@
we
make
the
connection
to
semi-supervised
learning
explicit
shortly
for
now
assume
availability
of
such
an
estimator
we
discuss
one
specific
approach
below
then
from
the
y4
we
have
an
estimate
of
the
graph
of
interest
as
g
®
v
bg(2
®
recall
that
the
vertex
set
vis
known
with
the
edge
set
specified
via
the
semi-supervised
learner
as
i
eg@
®
vg
@
=
1
background
knowledge
could
be
based
on
relevant
science
or
on
available
interventional
syduosnuey
j0yiny
sipung
dn
2doinyg
data
for
example
in
given
scientific
setting
certain
cause-effect
information
may
be
iwe
emphasize
that
these
are
pragmatic
assumptions
motivated
by
the
nature
of
experimental
data
and
scientific
applications
and
not
intended
to
be
fundamental
statements
about
causality
for
example
hyttinen
et
al
2012
make
the
point
that
cycles
can
be
removed
by
considering
time-varying
data
on
suitable
time
scale
but
that
nevertheless
cycles
are
common
in
causal
scientific
models
in
economics
engineering
and
biology
due
to
the
fact
that
measurements
are
usually
taken
at
wider
intervals
mach
learn
res
author
manuscript
available
in
pmc
2020
january
28
hill
et
al
sydurosnuepy
joyiny
sropun
da
odoing
page
known
from
previous
work
or
theory
alternatively
if
some
interventional
data
are
available
in
the
study
at
hand
this
gives
information
on
some
causal
relationships
whatever
the
source
of
the
information
assume
that
it
is
known
that
certain
pairs
7
j
are
either
causal
pairs
positive
information
or
not
causal
pairs
negative
information
using
the
notation
above
this
amounts
to
knowing
for
some
pairs
4
the
value
of
yj4
in
semi-supervised
learning
terms
the
pairs
whose
causal
status
is
known
correspond
to
the
labelled
objects
and
the
remaining
pairs
are
the
unlabelled
objects
for
each
pair
4
some
of
the
data
or
some
transformation
thereof
will
be
used
as
predictors
or
inputs
denote
these
generically
as
g/4(%
that
is
gj4
is
featurization
of
the
data
with
the
featurization
specific
to
variables
44
j&
let
be
the
set
of
linear
indices
i.e
k
is
variable
pair
&£
be
the
variable
pairs
with
labels
available
via
@
and
\
be
the
set
of
unlabelled
pairs
let
y<
be
binary
vector
comprising
the
my
=1&l|
available
labels
and
y%
be
an
unknown
binary
vector
of
length
vy
|%i
the
available
labels
are
determined
by
the
background
information
and
we
can
write
y<(®
to
make
this
explicit
semi-supervised
learner
gives
estimates
for
the
unlabelled
objects
given
the
data
and
available
labels
that
is
an
estimate
of
the
form
s‘r%
&(d
yx(®
with
these
in
hand
we
have
estimates
for
all
labels
and
therefore
for
all
edges
via
1
formulated
in
this
way
it
is
clear
that
essentially
any
combination
of
featurization
and
semi-supervised
learner
could
be
used
in
this
setting
below
as
practical
example
we
explore
graph-based
manifold
learning
following
belkin
et
al
2006
combined
with
simple
bivariate
featurization
2.4
bivariate
featurization
syduosnuey
j0yiny
sipung
dn
2doinyg
for
distance-based
learning
we
require
distance
measure
between
objects
here
variable
pairs
4
k
the
simplest
candidate
distance
between
variable
pairs
4
£']is
based
only
on
the
bivariate
distribution
for
the
variables
comprising
the
pairs
we
make
this
notion
precise
below
proofs
of
propositions
appearing
in
this
section
are
provided
in
appendix
a
2.4.1
distance
between
variable
pairs—let
denote
the
p-dimensional
random
variable
whose
realizations
z
1=1
.
n,comprise
the
data
set
2
assume
z,=
zmins
zmax
and
that
z
is
endowed
with
the
borel
c-algebra
4
&4
zp
let
be
the
set
of
all
twice
continuously
differentiable
probability
density
functions
generically
denoted
7z
with
respect
to
lebesgue
measure
on
2
&
let
itj
be
the
bivariate
marginal
distribution
for
components
/4
4
vof
z
assumption
each
ty
admits
density
function
mrz
2
if
available
the
densities
74
7z4
could
be
used
to
define
distance
between
the
pairs
4
k
let
dp
0,0
denote
pseudo—metric2
on
#
since
we
do
not
have
access
to
the
underlying
probability
density
functions
we
construct
an
analogue
using
the
available
data
let
=
znin»
zmax]>
denote
the
space
of
possible
bivariate
samples
the
sample
2recall
that
pseudo-metric
satisfies
all
of
the
properties
of
metric
with
the
exception
that
d(x
y
=0
x=y
mach
learn
res
author
manuscript
available
in
pmc
2020
january
28
sydurosnuepy
joyiny
sropun
da
odoing
sydimosnuepy
royiny
sropun
da
2domyg
hill
et
al
page
size
is
i
and
sjy
7
denote
the
subset
of
the
data
for
the
variable
pair
£
that
is
sy
g
al
=1
2
let
x
%
be
density
estimator
de
we
consider
sample
quantities
of
the
form
dy
=dp
xx
x
that
is
given
data
s[k
s[kf
¥
on
two
pairs
4
£
the
de
is
applied
separately
to
produce
density
estimates
x(sjz
and
x(sj'}
that
are
compared
using
to
give
do
six»
six
do
x(spxy
x(sjx'1
this
construction
ensures
that
is
pseudo
metric
without
assumptions
on
the
de
x
proposition
assume
that
dep
is
pseudo-metric
on
#
then
dy
is
pseudo-metric
on.¥
1if
in
addition
is
injective
and
dep
is
metric
on
p
then
dy
is
metric
on
2.4.2
choice
of
distance—for
semi-supervised
learning
we
need
notion
of
distance
under
which
causal
pairs
are
relatively
close
to
each
other
for
measurable
space
0
the
notion
of
distance
that
equipped
with
measure
we
let
f”lq(/
/;\fﬂdp
we
consider
is
g
@
||x
ﬁhlz(az
the
right
hand
side
exists
since
the
integrand
is
continuous
on
compact
set
and
thus
bounded
this
can
be
contrasted
with
the
kernel
embedding
that
was
proposed
for
supervised
causal
learning
in
lopez-paz
et
al
2015
proposition
dp
is
metric
on
2
the
main
requirement
that
we
have
of
the
de
is
that
it
provides
consistent
estimation
in
the
il
1;2(a2
norm
when
2
specifically
consider
sequence
in
¥
indexed
by
the
number
of
data
points
in
particular
suppose
that
is
built
from
independent
data
i.i.d
points
whose
distribution
is
it
the
shorthand
notation
will
be
used
let
7z
be
the
density
function
for
ii
then
xis
said
to
be
consistent
if
llz
k(s(”))iilz(az
=op(l
i.i.d
holds
for
"
~
ii
whenever
2
proposition
suppose
is
consistent
and
that
t1
admit
densities
7,7
2
then
for
iid
_oidid
so
§™
lt
t
where
and
§™
are
not
necessarily
independent
we
have
that
s
5
(
1
0,(1
thus
d
approximates
the
idealized
metric
dz
in
the
limit
of
draws
from
it
and
ti
note
that
in
our
intended
use
case
the
and
will
correspond
to
bivariate
scatter
plots
sj4
six
generated
from
the
same
underlying
z
/=1
.
n
and
hence
and
will
not
be
independent
mach
learn
res
author
manuscript
available
in
pmc
2020
january
28
sydurosnuepy
joyiny
sropun
da
odoing
syduosnuey
j0yiny
sipung
dn
2doinyg
hill
et
al
page
for
the
experiments
in
this
paper
motivated
by
computational
ease
we
used
simple
bivariate
histogram
as
the
de
«
to
this
end
partition
into
an
regular
grid
whose
my
my)th
element
is
denoted
by
m
the
standard
bandwidth
notation
4=
m~
will
also
be
used
for
scatter
plot
7
let
x
y
denote
the
number
of
elements
that
belong
to
the
set
by
.
then
the
histogram
estimator
is
my,my
ks@)=
yy
—
|z
es
7€
z
my,my
=1
o2
172
this
de
is
consistent
in
the
sense
of
proposition
3
indeed
proposition
let
the
bandwidth
parameter
of
the
histogram
estimator
be
chosen
such
that
nl
©0
then
is
consistent
moreover
an
optimal
choice
of
n™
leads
to
i.i.d
sn
l2(ay
v
whenever
~
tl
and
#
we
note
that
this
histogram
de
is
not
rate
optimal
for
the
class
for
comparison
kernel
de:s
attain
rate
of
op
1772/3
over
the
same
class
of
twice
continuously
differentiable
bivariate
densities
considered
here
see
wand
and
jones
1994
however
an
important
advantage
of
the
histogram
de
is
that
the
subsequent
evaluation
of
x(s
is
1
compared
with
o(n
for
the
kernel
de
2.4.3
implementation
of
the
de—the
above
arguments
support
the
use
of
bivariate
histogram
to
provide
simple
featurization
for
variable
pairs
in
practice
for
all
examples
below
the
data
were
standardized
then
truncated
to
3
3]2
following
which
bivariate
histogram
with
bins
of
fixed
width
0.2
was
used
the
dimension
of
the
resulting
feature
matrix
was
then
reduced
to
100
using
pca
2.5
manifold
regularization
recall
that
the
goal
is
to
estimate
binary
labels
y%
for
subset
of
variable
pairs
given
available
data
and
known
labels
y<(®
for
subset
these
are
taken
to
be
obtained
from
available
interventional
experiments
and/or
background
knowledge
for
any
two
pairs
4
£
o
we
also
have
available
distance
dy(s[k
s[k
this
is
task
in
semi-supervised
learning
see
e.g
belkin
et
al
2006
fergus
et
al
2009
and
number
of
formulations
and
methods
could
be
used
for
estimation
in
this
setting
here
we
describe
specific
approach
in
detail
using
manifold
regularization
methods
discussed
in
belkin
et
al
2006
let
x(4
denote
vector
whose
entries
are
the
bin-counts
x
1
/<
m
appearing
in
2
for
scatter
plot
sjg
let
x<
xpl0
a
and
note
that
x4y
2
then
we
make
the
observation
that
for
the
histogram
estimator
5ss
=xl
mach
learn
res
author
manuscript
available
in
pmc
2020
january
28
sydurosnuepy
joyiny
sropun
da
odoing
sydimosnuepy
royiny
sropun
da
2domyg
hill
et
al
page
this
perspective
emphasizes
that
g4(%
x[y
is
the
featurization
that
underpins
this
work
and
that
the
classification
task
can
be
considered
as
the
construction
of
map
¢
{-1
+1}
to
develop
an
approach
to
semi-supervised
classification
in
the
manner
of
belkin
et
al
2006
let
pg~
be
reference
measure
on
and
let
k
be
mercer
kernel
i.e
continuous
symmetric
and
positive
semi-definite
the
reproducing
kernel
hilbert
space
a
associated
to
can
be
defined
via
the
integral
operator
lx(pg
lxpg
where
0=
kr
@y
from
the
fact
that
k'is
mercer
kernel
it
follows
that
is
self-adjoint
positive
semi
definite
and
compact
in
particular
2
is
well-defined
for
0
©0
the
reproducing
kernel
hilbert
space
is
defined
as
7
zz
lz(/}g
and
its
norm
is
||f||%k
c.f
corollary
4.13
in
cucker
and
zhou
2007
l2p2
recall
that
mge
=1ll
is
the
number
of
available
labels
and
my
1%
the
number
of
unlabelled
pairs
let
m=
my+mg
=1%7
be
the
total
number
of
pairs
using
the
distance
function
dy
we
first
define
an
similarity
matrix
with
entries
3
wi,y
exv(—q—”fhxm
xp3
where
o7
must
be
specified
the
squared-exponential
form
is
motivated
by
an
analytic
connection
between
the
heat
kernel
and
the
laplace-beltrami
operator
which
will
be
exploited
in
section
2.5.1
we
will
use
partition
of
the
matrix
corresponding
to
the
sets
%
as
follows
wzy
wy‘z
w=\
ag
auu
w4
where
we
have
assumed
without
loss
of
generality
that
the
variable
pairs
are
ordered
so
that
the
labelled
pairs
appear
in
the
first
mg
places
followed
by
the
my
mg
unlabelled
pairs
correspondingly
let
zz
y=
e{-1,+1
denote
label
matrix
where
+1
indicate
those
pairs
£
for
which
yj4
1
the
vector
y%
is
unknown
and
is
the
object
of
estimation
let
be
the
diagonal
matrix
with
diagonal
entries
dy
zix
1€
wik
k1
define
=d
i.e
the
un-normalized
graph
laplacian
all
matrices
with
o(n72
entries
mach
learn
res
author
manuscript
available
in
pmc
2020
january
28
sydurosnuepy
joyiny
sropun
da
odoing
syduosnuey
j0yiny
sipung
dn
2doinyg
hill
et
al
page
are
denoted
as
bold
capitals
to
emphasize
the
potential
bottleneck
that
is
associated
with
storage
and
manipulation
of
these
matrices
let
be
vector
corresponding
to
classification
function
£
evaluated
at
the
variable
pairs
.
with
the
superscripts
indicating
correspondence
with
the
labelled
and
unlabelled
pairs
intuitively
we
want
the
sign
of
to
agree
with
the
known
labels
y<
and
also
to
take
account
of
the
manifold
structure
encoded
in
l
in
this
work
we
consider
classifier
of
the
form
&x
sign(f(x
where
arises
from
the
laplacian-regularized
least
squares
method
2|2
ly
11
ftlf
arginf
+
a
+
4
eg’%’k
ny
115
following
section
4.2
of
belkin
et
al
2006
here
the
first
term
relates
the
known
labels
to
the
values
of
the
function
£
the
second
term
imposes
smoothness
on
the
label
assignment
in
the
sense
of
encouraging
solutions
where
the
labels
do
not
change
quickly
with
respect
to
the
distance
metric
the
third
term
is
principally
to
ensure
that
the
infimum
remains
well
defined
and
unique
in
the
situation
where
there
is
insufficient
data
for
the
first
penalty
alone
to
be
sufficient
see
remark
in
belkin
et
al
2006
remark
choice
of
loss
/f
is
important
to
comment
on
our
choice
of
squared-error
loss
function
in
4
which
differs
from
the
more
natural
approach
of
using
hinge
loss
for
binary
classification
task
our
motivation
here
is
principally
computational
expedience
the
computational
burden
associated
with
the
m=
oxp
different
scatter
plots
requires
that
light-weight
estimation
procedure
is
used
however
we
note
that
we
are
not
the
first
to
propose
the
use
of
squared-error
loss
in
the
classification
context
it
is
in
fact
standard
approach
to
classification
in
the
situation
when
the
number
of
classes
is>
e.g
wang
et
al
2008
2.5.1
consistency
of
the
classifier
as
explained
in
remark
5
the
use
of
squared
error
loss
function
in
classification
context
is
somewhat
unnatural
it
is
therefore
incumbent
on
us
to
establish
consistency
of
the
proposed
method
to
this
end
we
exploit
the
specific
form
of
the
similarity
matrix
used
in
3
indeed
if
we
re-write
ftle
%m,[kfz]:e
w(f
(
xge
then
it
can
be
established
under
certain
regularity
conditions
that
if
input
data
are
independently
drawn
from
pg
then
5
converges
to
the
quantity
f®a
f
x)dpé,(x
up
mach
learn
res
author
manuscript
available
in
pmc
2020
january
28
sydurosnuepy
joyiny
sropun
da
odoing
syduosnuey
j0yiny
sipung
dn
2doinyg
hill
et
al
page
10
to
proportionality
smoothness
penalty
based
on
weighted
laplace-beltrami
operator
agm
on
the
manifold
induced
by
pg
grigor’yan
2006
the
convergence
occurs
as
m,o2m
oo
theorem
3.1
of
belkin
and
niyogi
2008
this
convergence
of
the
graph
laplacian
to
the
laplace-beltrami
operator
underlies
existing
consistency
results
for
semi-supervised
regression
e.g
cao
and
chen
2012
and
is
exploited
again
to
establish
the
consistency
of
our
classifier
dx
sign(f(x
in
appendix
b
in
summary
the
ability
to
assign
the
correct
label
to
an
unlabelled
pair
4
depends
on
both
the
intrinsic
predictability
of
the
label
as
function
of
the
scatter
plot
iy
as
quantified
by
the
bayes
risk
and
the
smoothness
of
the
bayes
classifier
£
as
quantified
by
the
largest
value
0
1
such
that
2;(7
f,7
lz(py
see
corollary
in
appendix
for
full
detail
2.5.2
implementation
of
the
classifier
given
training
labels
y<
label
estimates
u=
sign(f%
are
obtained
by
minimizing
the
objective
function
described
above
as
explained
in
equation
in
belkin
et
al
2006
this
gives
aom
l
1my
k%y%
ky{,z
+amgl
tlk%
y{
©6
00
where
k¢y
is
the
myy
mkernel
matrix
based
on
the
unlabeled
and
total
data
the
mkernel
matrix
based
on
the
total
data
#
and
,
denotes
an
m-dimensional
identity
matrix
here
yupmvides
point
estimate
for
the
unknown
labels
while
is
real-valued
and
can
be
used
to
rank
candidate
pairs
if
required
the
linear
system
in
6
can
be
solved
at
naive
computational
cost
of
o(z7%
computation
for
large-scale
semi-supervised
learning
has
been
studied
in
the
literature
see
e.g
fergus
et
al
2009
and
number
of
approaches
could
be
used
to
scale
up
to
larger
problems
but
were
not
pursued
in
this
work
for
experiments
reported
below
we
employed
similarity
matrix
with
length
scale
oy
as
in
3
and
kernel
k(x,x
exp[—l
x—x’l
%
20
whose
length-scale
parameter
o
was
set
equal
to
oy
in
the
absence
of
prior
knowledge
about
the
manifold
m
the
scale
oy
was
set
to
the
average
distance
to
the
nearest
50
points
in
the
feature
space
in
practice
estimated
via
subsample
the
two
penalty
parameters
in
4
were
set
to
small
positive
values
1|
a
0.001
we
found
results
were
broadly
insensitive
to
this
choice
following
common
practice
we
worked
with
the
normalized
graph
laplacian
=
ld
zin
place
of
see
remark
of
belkin
et
al
2006
mach
learn
res
author
manuscript
available
in
pmc
2020
january
28
hill
et
al
page
11
empirical
results
we
tested
our
approach
using
three
data
sets
with
different
characteristics
the
key
features
of
each
data
set
are
outlined
below
with
full
description
of
each
data
set
appearing
in
the
respective
subsection
in
all
cases
performance
was
assessed
using
either
held-out
interventional
data
or
scientific
knowledge
d1
yeast
knockout
data
here
we
used
data
set
due
to
kemmeren
et
al
2014
previously
considered
for
causal
learning
in
peters
et
al
2016
meinshausen
et
al
2016
the
data
consist
of
large
number
of
gene
deletion
experiments
with
corresponding
gene
expression
measurements
d2
kinase
intervention
data
from
human
cancer
cell
lines
these
data
due
to
hill
et
al
2017
involve
small
number
of
interventions
on
human
cells
with
corresponding
protein
measurements
over
time
d3
protein
data
from
cancer
patient
samples
these
data
arise
from
the
cancer
genome
atlas
tcga
and
are
presented
in
akbani
et
al
2014
there
are
no
interventional
data
but
the
data
pertain
to
relatively
well-understood
biological
processes
allowing
inferences
to
be
checked
against
causal
scientific
knowledge
sydurosnuepy
joyiny
sropun
da
odoing
an
appealing
feature
of
mrcl
is
the
simplicity
with
which
it
can
be
applied
to
diverse
problems
in
each
case
below
we
simply
concatenate
available
data
to
form
the
data
set
and
available
knowledge/interventions
to
form
®
then
directly
apply
the
methods
as
described
3.1
general
problem
set-up
the
basic
idea
in
all
three
problems
was
as
follows
given
data
on
set
of
variables
for
each
ordered
pair
7
j
of
variables
we
sought
to
determine
whether
or
not
7has
causal
effect
on
in
the
case
of
data
sets
d1
and
d2
the
results
were
assessed
against
the
outcome
of
experiments
involving
explicit
interventions
as
discussed
above
such
experiments
reveal
ancestral
relationships
that
need
not
be
direct
and
the
goal
in
these
examples
was
to
learn
such
relationships
the
availability
of
large
number
of
interventions
in
d1
allowed
wider
range
of
experiments
whereas
d2
is
smaller
data
set
but
from
human
cells
allowing
only
arelatively
limited
assessment
in
the
case
of
d3
where
interventional
data
i.e
interventions
on
the
same
biological
material
that
give
rise
to
the
training
data
were
not
available
but
the
relevant
biological
mechanisms
are
relatively
well
understood
we
compared
results
to
reference
mechanistic
graph
derived
from
the
domain
literature
the
literature
itself
is
in
effect
an
encoding
of
extensive
interventional
experiments
combined
with
biochemical
and
biophysical
knowledge
this
gives
information
on
direct
edges
and
here
the
edges
learned
are
intended
to
represent
direct
causes
relative
to
the
set
of
observed
syduosnuey
j0yiny
sipung
dn
2doinyg
variables
within
the
semi-supervised
set-up
subset
of
pairs
were
labelled
at
the
outset
and
the
remaining
pairs
were
unlabelled
all
empirical
results
below
are
for
unlabelled
pairs
that
is
in
all
cases
assessment
is
carried
out
with
respect
to
causal
and
non-causal
relationships
that
were
not
used
to
train
the
models
mach
learn
res
author
manuscript
available
in
pmc
2020
january
28
sydurosnuepy
joyiny
sropun
da
odoing
syduosnuey
j0yiny
sipung
dn
2doinyg
page
12
3.2
data
set
d1
yeast
gene
expression
data—the
data
consisted
of
gene
expression
levels
log
ratios
for
total
of
piora
6170
genes
some
of
the
data
samples
were
measurements
after
knocking
out
specific
gene
interventional
data
and
the
other
samples
were
without
any
such
intervention
observational
data
with
sample
sizes
of
7™
1479
and
1°
153
respectively
each
of
the
genes
intervened
on
was
one
of
the
pyo
genes
let
#/
be
the
index
of
the
gene
targeted
by
the
/
intervention
that
is
the
a
interventional
sample
was
an
experiment
in
which
gene
«(/
was
knocked
out
let
7=
{«(1
.
£7")}
be
the
subset
of
genes
that
were
the
target
of
an
interventional
experiment
problem
set-up
our
problem
set-up
was
as
follows
we
sampled
subset
c'c
of
the
genes
that
were
intervened
upon
with
|c1
50
and
treated
this
as
the
vertex
set
of
interest
i.e
setting
v=
cand
p=1c
50
the
goal
was
to
uncover
causal
relationships
between
these
variables
since
by
design
interventional
data
were
available
for
all
variables
/€
c
we
used
these
data
to
define
an
interventional
gold
standard
to
this
end
we
used
robust
z-score
that
considered
the
change
in
variable
of
interest
under
intervention
relative
to
its
int
on
gene
/7
for
any
pair
of
genes
7
/€
cwe
say
that
gene
has
causal
effect
on
gene
jif
and
only
if
{ij
|z
observational
variation
let
z
denote
the
expression
level
of
gene
following
intervention
int
using
half
of
the
observational
data
samples
the
remaining
samples
were
used
as
training
m?bs‘/iqr;?bs
7
where
m;?bs
is
the
median
level
of
gene
calculated
data—see
below
iqr?bs
the
corresponding
inter-quartile
range
and
z=15
was
fixed
threshold
that
is
we
say
there
is
an
experimentally
verified
causal
relationship
between
gene
7and
gene
/if
and
only
if
c;;>
7
an
absence
of
causal
effects
precludes
estimation
of
true
positive
rates
hence
we
sampled
c'subject
to
sparsity
condition
that
at
least
2.5%
of
gene
pairs
show
an
effect
let
a(c
be
pxp
binary
matrix
encoding
the
causal
effects
as
described
in
the
foregoing
ie
a(q);;=
indicates
that
has
an
experimentally
verified
causal
effect
on
)
then
given
data
on
genes
c
we
set
up
the
learning
problem
as
follows
we
treated
fraction
of
the
entries
in
a(c
as
the
available
labels
®
thus
here
m=
p
2500
mg
=lp
ml
and
my
m
mg
using
these
labels
and
data
on
the
variables
c
we
learned
causal
edges
as
described
this
gave
estimates
for
the
remaining
unseen
entries
in
a(c
which
we
compared
against
the
corresponding
true
values
the
data
set
comprised
expression
obs
measurements
for
the
genes
in
cfor
n
train
76
observational
data
samples
those
samples
int
not
used
to
calculate
the
robust
z-scores
plus
n
interventional
data
samples
where
genes
outside
the
set
of
interest
were
intervened
upon
that
is
subset
of
the
1429
genes
in
71c
this
set-up
ensured
that
include
neither
any
of
the
interventional
nor
observational
data
that
was
used
to
obtain
the
ground-truth
matrix
a(c
the
total
amount
of
training
data
is
denoted
by
n
32?11
il:;:in
we
considered
myin
200
500
and
1000
corresponding
to
int
n
train
124,424
and
924
respectively
sampled
at
random
mach
learn
res
author
manuscript
available
in
pmc
2020
january
28
sydurosnuepy
joyiny
sropun
da
odoing
syduosnuey
j0yiny
sipung
dn
2doinyg
page
13
results
we
compared
the
proposed
manifold
regularized
causal
learning
mrcl
approach
with
the
following
approaches
penalized
regression
with
an
penalty
lasso
tibshirani
1996
each
variable
cwas
regressed
on
all
other
variables
c
jto
obtain
regression
coefficients
this
is
not
causal
approach
as
such
but
is
included
as
simple
multivariate
baseline
intervention-calculus
when
the
dag
is
absent
ida
maathuis
et
al
2009
2010
lower
bound
for
the
total
causal
effect
of
variable
on
variable
jwas
estimated
for
each
pair
7
j€
c
1#
the
pc
algorithm
pc
spirtes
et
al
2000
this
provides
cpdag
estimate
for
the
variables
c
gies
gies
hauser
and
biihlmann
2012
this
provides
an
essential
graph
estimate
for
the
variables
c
and
allows
inclusion
of
interventional
data
in
principled
manner
as
simple
baselines
we
also
included
pearson
and
kendall
correlation
coefficients
pearson
and
kendall
and
following
suggestion
from
referee
simple
k-nearest
neighbor
approach
based
on
the
featurization
introduced
above
k-nn
we
note
that
the
causal
methods
compared
against
here
differ
in
various
ways
from
mrcl
in
the
nature
of
their
inputs
and
outputs
and
should
not
be
regarded
as
direct
competitors
rather
the
aim
of
the
experiments
is
to
investigate
how
mrcl
performs
on
real
data
whilst
providing
set
of
baselines
corresponding
to
well-known
causal
tools
and
standard
correlation
measures
for
the
methods
resulting
in
score
s
for
all
pairs
7
j€
c
7#
i.e
correlation
or
regression
coefficients
total
causal
effects
or
for
mrcl
the
real-valued
fin
6
the
scores
were
thresholded
and
pairs
7
j
whose
absolute
values
of
the
score
fell
above
the
threshold
were
labelled
as
causal
varying
the
threshold
and
calculating
true
positives
and
false
positives
with
respect
to
the
binary
unseen
entries
in
the
matrix
a(c
resulted
in
receiver
operating
characteristic
roc
curve
figure
shows
the
area
under
the
roc
curve
auc
as
function
of
the
proportion
of
entries
in
a(c
that
were
observed
for
the
three
sample
sizes
results
were
averaged
over
25
iterations
mrcl
showed
good
performance
relative
to
the
other
approaches
for
all
12
considered
combinations
of
i
and
for
the
other
methods
shown
in
figure
1
any
variation
in
performance
with
was
solely
due
to
the
changing
test
set
as
these
methods
do
not
use
the
background
knowledge
®
results
for
pc
which
provides
point
estimate
of
graphical
object
are
shown
as
points
on
the
roc
plane
for
the
12
different
regimes
in
appendix
fig
6
we
considered
also
the
transitive
closure
motivated
by
the
nature
of
the
experimental
data
and
exploiting
the
background
information
via
additional
constraints
mrcl
performs
well
relative
to
the
other
methods
in
all
regimes
see
also
the
discussion
mach
learn
res
author
manuscript
available
in
pmc
2020
january
28
sydurosnuepy
joyiny
sropun
da
odoing
syduosnuey
j0yiny
sipung
dn
2doinyg
hill
et
al
page
14
in
the
above
results
the
pairs
whose
causal
relationship
was
to
be
predicted
were
chosen
at
random
i.e
the
set
of
unlabelled
pairs
was
random
subset
of
the
set
of
all
pairs
in
contrast
in
some
settings
it
may
be
relevant
to
predict
the
effect
of
intervening
on
variable
7
without
knowing
the
effect
of
intervening
on
7on
any
other
variable
for
this
setting
the
unlabelled
set
should
comprise
entire
rows
of
the
causal
adjacency
matrix
a(c
figure
considers
this
case
to
ensure
sufficient
number
of
rows
were
non-empty
we
imposed
the
additional
restriction
on
the
gene
subset
c'that
at
least
half
of
the
rows
had
at
least
one
causal
effect
results
for
pc
are
shown
in
appendix
fig
7
as
points
on
the
roc
plane
as
for
the
random
sampling
case
above
mrcl
offers
an
improvement
over
the
other
methods
k-nn
also
performs
well
relative
to
the
other
approaches
here
we
additionally
compared
mrcl
with
gies
gies
and
mrcl
differ
in
terms
of
their
required
inputs
in
addition
to
data
2
mrcl
requires
binary
labels
on
causal
relationships
via
background
information
®
while
gies
requires
the
interventional
data
itself
and
metadata
specifying
the
intervention
targets
for
row-wise
sampling
to
allow
for
reasonable
comparison
we
ran
gies
providing
the
interventional
data
corresponding
to
the
rows
whose
labels
are
provided
to
mrcl
the
same
data
was
also
provided
as
input
to
the
other
approaches
including
in
data
set
for
mrcl
this
means
the
data
matrices
differ
from
those
above
with
sample
size
dependent
on
p
and
for
mrcl
now
includes
data
that
was
used
to
obtain
background
information
train/test
validity
is
preserved
since
it
remains
the
case
that
all
testing
is
done
with
respect
to
entirely
unseen
interventions
results
appear
in
figure
3
with
pc
and
gies
shown
as
points
on
the
roc
plane
mrcl
appears
to
offer
an
improvement
relative
to
the
other
methods
see
also
the
discussion
note
that
gies
is
not
directly
applicable
to
the
random
sampling
setting
above
since
it
requires
the
interventional
data
with
respect
to
all
other
variables
and
not
just
subset
thereof
3.3
data
set
d2
protein
time-course
data
data—the
data
consisted
of
protein
measurements
for
35
proteins
measured
at
seven
time
points
in
four
different
cell
lines
bt20
bt549
mcf7
and
uacc812
these
are
laboratory
models
of
human
cancer
and
under
eight
growth
conditions
the
proteins
under
study
act
as
kinases
i.e
catalysts
for
biochemical
process
known
as
phosphorylation
and
interventions
were
carried
out
using
kinase
inhibitors
that
block
the
kinase
activity
of
specific
proteins
total
of
four
intervention
regimes
were
considered
plus
control
regime
with
no
interventions
the
data
used
here
were
subset
of
the
complete
data
set
reported
in
detail
in
hill
et
al
2017
and
were
also
previously
used
in
dialogue
for
reverse
engineering
assessments
and
methods
dream
challenge
on
learning
causal
networks
hill
et
al
2016
problem
set-up
treating
each
cell
line
as
separate
independent
problem
the
intervention
regimes
were
used
to
define
an
interventional
gold
standard
in
similar
vein
as
for
data
set
d1
this
followed
the
procedure
described
in
detail
in
hill
et
al
2016
with
an
additional
step
of
taking
majority
vote
across
growth
conditions
to
give
causal
gold
standard
for
each
cell
line
c
for
each
cell
line
¢
we
formed
data
matrix
consisting
of
all
available
data
for
the
p=
35
proteins
except
for
one
of
the
intervention
regimes
the
mach
learn
res
author
manuscript
available
in
pmc
2020
january
28
sydurosnuepy
joyiny
sropun
da
odoing
syduosnuey
j0yiny
sipung
dn
2doinyg
hill
et
al
page
15
intervention
regime
not
included
was
kinase
inhibitor
targeting
the
protein
mtor
this
intervention
was
entirely
held
out
and
used
to
provide
the
test
labels
as
background
knowledge
@
we
took
as
training
labels
causal
effects
under
the
other
interventions
with
this
set-up
the
task
was
to
determine
the
ancestral
causal
effects
of
the
entirely
unseen
intervention
note
that
each
cell
line
was
treated
as
an
entirely
different
data
set
and
task
with
its
own
data
matrix
background
knowledge
and
interventional
test
data
results
figure
shows
aucs
with
respect
to
changes
seen
under
the
test
intervention
for
each
of
the
four
cell
lines
and
each
of
the
methods
there
was
no
single
method
that
outperformed
all
others
across
all
four
cell
lines
mrcl
performed
particularly
well
relative
to
the
other
methods
for
cell
lines
bt549
and
mcf7
k-nn
also
performed
well
for
bt549
was
competitive
for
cell
line
uacc812
but
performed
less
well
for
cell
line
bt20
we
note
also
that
for
cell
lines
bt549
and
mcf7
the
performance
of
mrcl
was
competitive
with
the
best
performers
in
the
dream
challenge
and
with
an
analysis
reported
in
hill
et
al
2017
the
latter
involved
bayesian
model
specifically
designed
for
such
data
in
contrast
mrcl
was
applied
directly
to
data
matrix
comprising
all
training
samples
simply
collected
together
3.4
data
set
d3
human
cancer
data
data—the
data
consisted
of
protein
measurements
for
35
proteins
measured
in
7=
820
human
breast
cancer
samples
from
biopsies
the
data
originate
from
the
cancer
genome
atlas
tcga
project
are
described
in
akbani
et
al
2014
and
were
retrieved
from
the
cancer
proteome
atlas
tcpa
data
portal
li
et
al
2013
https://tcpaportal.org
data
release
version
4.0
pan-can
19
level
data
data
for
many
cancer
types
are
available
but
here
we
focus
on
single
type
breast
cancer
to
minimize
the
potential
for
confounding
by
cancer
type
it
is
at
present
difficult
to
carry
out
interventions
in
biopsy
samples
of
this
kind
however
we
focused
on
the
same
35
proteins
as
in
data
set
d2
whose
mutual
causal
relationships
are
relatively
well-understood
and
used
reference
causal
graph
for
these
proteins
based
on
the
biochemical
literature
as
reported
in
hill
et
al
2017
problem
set-up
we
formed
data
set
consisting
of
measurements
for
the
=35
proteins
for
three
different
sample
sizes
1
zrain
200
ii
bgrain
500
or
iii
all
myyin
820
patient
samples
for
i
and
ii
patient
samples
were
selected
at
random
we
then
used
random
fraction
of
the
reference
graph
as
background
knowledge
testing
output
on
the
unseen
remainder
results
figure
shows
aucs
with
respect
to
the
held-out
causal
labels
as
function
of
the
proportion
of
causal
labels
that
were
observed
for
each
of
the
methods
and
for
the
three
sample
sizes
results
were
averaged
over
25
iterations
mrcl
performed
well
relative
to
the
other
methods
with
performance
improving
with
p
results
were
qualitatively
similar
for
the
three
sample
sizes
with
increases
in
auc
for
ng,j
820
and
i
500
relative
to
dyrain
250
results
for
pc
are
shown
in
appendix
fig
8
as
points
on
the
roc
plane
mach
learn
res
author
manuscript
available
in
pmc
2020
january
28
sydurosnuepy
joyiny
sropun
da
odoing
syduosnuey
j0yiny
sipung
dn
2doinyg
page
16
discussion
in
this
paper
we
showed
how
key
aspect
of
causal
structure
learning
can
be
framed
as
machine
learning
task
although
many
available
approaches
including
those
based
on
dags
and
related
graphical
models
offer
well-studied
framework
we
think
it
may
be
fruitful
to
revisit
some
questions
in
causality
using
machine
learning
tools
in
our
experiments
based
on
three
real
data
sets
we
found
that
mrcl
performed
well
relative
to
range
of
graphical
model-based
approaches
however
two
points
should
be
noted
regarding
these
comparative
results
first
the
various
methods
differ
with
respect
to
their
required
inputs
and
the
nature
of
their
outputs
this
means
that
in
some
cases
specific
methods
may
not
be
an
ideal
fit
to
the
context
of
the
specific
data/task
as
detailed
when
presenting
the
empirical
results
above
second
the
biological
systems
underlying
these
data
sets
are
likely
to
have
features
such
as
causal
insufficiency
and
cycles
that
violate
one
or
more
of
the
assumptions
of
some
of
these
methods
that
said
we
think
biological
data
sets
of
the
kind
we
focused
on
here
offer
perhaps
the
best
opportunity
at
present
to
empirically
study
causal
learning
methods
and
that
causal
learning
tasks
of
the
kind
addressed
here
are
highly
relevant
in
many
applications
in
biology
and
beyond
hence
we
think
that
pursuing
empirical
work
on
such
data
is
valuable
both
from
methodological
and
applied
points
of
view
as
more
interventional
data
become
available
in
the
future
it
will
be
important
to
carry
out
similar
analyses
in
other
contexts
in
order
to
better
understand
the
extent
to
which
our
findings
generalize
to
other
scientific
settings
an
open
question
from
theoretical
point
of
view
is
to
understand
conditions
on
data
generating
processes
needed
to
permit
discriminative
approach
as
pursued
here
and
we
think
this
will
be
an
interesting
direction
for
future
work
one
point
of
view
analogous
to
that
used
in
practical
applications
of
classification
is
to
estimate
the
risk
of
the
learner
and
thereby
report
an
estimate
of
causal
efficacy
without
having
to
directly
consider
requirements
on
the
underlying
system
we
think
this
approach
is
acceptable
when
some
causal
information
is
available
since
one
can
then
empirically
test
problem-specific
efficacy
as
in
our
examples
above
this
then
gives
confidence
with
respect
to
generalization
to
new
interventions
on
the
system
of
interest
but
does
not
address
the
broader
theoretical
question
in
our
approach
information
on
multiple
variable
pairs
is
coupled
via
the
classifier
but
not
by
global
constraints
on
the
graph
in
the
scientific
settings
we
focused
on
we
did
not
consider
further
coupling
via
global
constraints
but
such
constraints
e.g
enforcing
transitivity
could
be
relevant
in
some
applications
and
an
interesting
direction
for
further
work
the
main
advantage
of
our
approach
is
that
it
allows
regularities
in
the
data
to
emerge
via
learning
rather
than
having
to
be
encoded
via
an
explicit
causal
or
mechanistic
model
it
also
naturally
provides
some
uncertainty
quantification
in
the
sense
of
scores
that
can
be
used
to
guide
decisions
or
future
experimental
work
the
main
disadvantage
relative
to
methods
rooted
in
dags
and
related
graphical
models
is
the
lack
of
full
causal
model
albeit
under
relatively
strong
assumptions
dag-based
models
once
estimated
can
be
used
to
shed
light
on
huge
range
of
questions
concerning
causal
relationships
including
direct
and
ancestral
effects
and
details
of
post-intervention
distributions
in
contrast
our
approach
mach
learn
res
author
manuscript
available
in
pmc
2020
january
28
hill
et
al
sydurosnuepy
joyiny
sropun
da
odoing
syduosnuey
j0yiny
sipung
dn
2doinyg
page
17
in
itself
provides
only
estimates
of
binary
causal
relationships
that
said
given
the
efficacy
and
simplicity
of
our
approach
we
think
it
would
be
fruitful
to
consider
coupling
it
to
established
causal
tools
in
two-step
approach
with
our
methods
used
to
learn
an
edge
structure
in
data-driven
manner
and
this
structure
used
to
inform
full
analysis
in
second
step
such
an
approach
would
require
some
care
to
avoid
bias
and
sample
splitting
techniques
that
have
been
studied
in
high-dimensional
statistics
could
be
relevant
wasserman
and
roeder
2009
stddler
and
mukherjee
2017
supplementary
material
refer
to
web
version
on
pubmed
central
for
supplementary
material
acknowledgments
the
authors
are
grateful
to
the
editor
and
reviewers
for
their
constructive
feedback
on
an
earlier
version
of
the
manuscript
the
authors
are
grateful
to
umberto
nog
for
input
on
the
empirical
work
and
to
oliver
crook
for
feedback
on
an
earlier
version
of
the
manuscript
this
work
was
supported
by
the
uk
medical
research
council
university
unit
programme
number
mc_uu_00002/2
cjo
was
supported
by
the
arc
centre
of
excellence
for
mathematics
and
statistics
australia
and
the
lloyd’s
register
foundation
programme
on
data-centric
engineering
at
the
alan
turing
institute
uk
the
results
shown
here
are
in
part
based
upon
data
generated
by
the
tcga
research
network
https://www.cancer.gov/tcga
references
akbani
r
ng
pks
werner
hm
shahmoradgoli
m
zhang
f
ju
z
liu
w
yang
jy
yoshihara
k
li
j
ling
s
et
al
pan-cancer
proteomic
perspective
on
the
cancer
genome
atlas
nature
communications
2014
belkin
m
niyogi
p
towards
theoretical
foundation
for
laplacian-based
manifold
methods
journal
of
computer
and
system
sciences
2008
74(8):1289-1308
belkin
m
niyogi
p
sindhwani
v
manifold
regularization
geometric
framework
for
learning
from
labeled
and
unlabeled
examples
journal
of
machine
learning
research
2006
7:2399-2434
cao
y
chen
d
generalization
errors
of
laplacian
regularized
least
squares
regression
science
china
mathematics
2012
55(9):1859-1868
colombo
d
maathuis
mh
kalisch
m
richardson
ts
learning
high-dimensional
directed
acyclic
graphs
with
latent
and
selection
variables
the
annals
of
statistics
2012
40(1):294-321
cucker
f
zhou
dx
learning
theory
an
approximation
theory
viewpoint
cambridge
university
press
2007
fergus
r
weiss
y
torralba
a
semi-supervised
learning
in
gigantic
image
collections
proceedings
of
the
23rd
annual
conference
on
neural
information
processing
systems
2009
522-530
grigor’yan
a
heat
kernels
on
weighted
manifolds
and
applications
cont
math
2006
398:93-191
hauser
a
biihimann
p
characterization
and
greedy
learning
of
interventional
markov
equivalence
classes
of
directed
acyclic
graphs
journal
of
machine
learning
research
2012
13:2409-2464
hill
sm
heiser
lm
cokelaer
t
unger
m
nesser
nk
carlin
de
zhang
y
sokolov
a
paull
eo
wong
ck
graim
k
et
al
inferring
causal
molecular
networks
empirical
assessment
through
community-based
effort
nature
methods
2016
13(4):310-318
pubmed
26901648
hill
sm
nesser
nk
johnson-camacho
k
jeffress
m
johnson
a
boniface
c
spencer
sef
lu
y
heiser
lm
lawrence
y
pande
nt
et
al
context
specificity
in
causal
signaling
networks
revealed
by
phosphoprotein
profiling
cell
systems
2017
4(1):73-83
pubmed
28017544
hyttinen
a
eberhardt
f
hoyer
po
learning
linear
cyclic
causal
models
with
latent
variables
journal
of
machine
learning
research
2012
13:3387-3439
kemmeren
p
sameith
k
van
de
pasch
la
benschop
jj
lenstra
tl
margaritis
t
o’duibhir
e
apweiler
e
van
wageningen
s
ko
cw
van
heesch
s
et
al
large-scale
genetic
perturbations
mach
learn
res
author
manuscript
available
in
pmc
2020
january
28
sydurosnuepy
joyiny
sropun
da
odoing
sydimosnuepy
royiny
sropun
da
2domyg
hill
et
al
page
18
reveal
regulatory
networks
and
an
abundance
of
gene-specific
repressors
cell
2014
157(3):740
752
pubmed
24766815
lij,lu
y
akbani
r
ju
z
roebuck
pl
liu
w
yang
j-y
broom
bm
verhaak
rgw
kane
dw
wakefield
c
et
al
tcpa
resource
for
cancer
functional
proteomics
data
nature
methods
2013
10(11):1046-1047
lopez-paz
d
muandet
k
scholkopf
b
tolstikhin
i
towards
learning
theory
of
causation
proceedings
of
the
32nd
international
conference
on
machine
learning
2015
1452-1461
maathuis
mh
kalisch
m
biihlmann
p
estimating
high-dimensional
intervention
effects
from
observational
data
annals
of
statistics
2009
37(6a):3133-3164
maathuis
mh
colombo
d
kalisch
m
biihimann
p
predicting
causal
effects
in
large-scale
systems
from
observational
data
nature
methods
2010
7(4):247-248
pubmed
20354511
meinshausen
n
hauser
a
mooij
jm
peters
j
versteeg
p
biihimann
p
methods
for
causal
inference
from
gene
perturbation
experiments
and
validation
proceedings
of
the
national
academy
of
sciences
2016
113(27):7361-7368
mooij
jm
peters
j
janzing
d
zscheischler
j
scholkopf
b
distinguishing
cause
from
effect
using
observational
data
methods
and
benchmarks
journal
of
machine
learning
research
2016
17(32):1-102
pearl
j
causality
cambridge
university
press
2009
peters
j
biihimann
p
meinshausen
n
causal
inference
using
invariant
prediction
identification
and
confidence
intervals
journal
of
the
royal
statistical
society
series
b
2016
78(5):947-1012
core
team
r
language
and
environment
for
statistical
computing
foundation
for
statistical
computing
vienna
austria
2018
url
https://www.r-project.org/
richardson
t
discovery
algorithm
for
directed
cyclic
graphs
proceedings
of
the
twelfth
international
conference
on
uncertainty
in
artificial
intelligence
1996
454-461
spirtes
p
directed
cyclic
graphical
representations
of
feedback
models
proceedings
of
the
eleventh
conference
on
uncertainty
in
artificial
intelligence
1995
491-498
spirtes
p
glymour
cn
scheines
r
causation
prediction
and
search
mit
press
2000
stidler
n
mukherjee
s
two-sample
testing
in
high
dimensions
journal
of
the
royal
statistical
society
series
b
2017
79(1):225-246
tibshirani
r
regression
shrinkage
and
selection
via
the
lasso
journal
of
the
royal
statistical
society
series
b
1996
58(1):267-288
wand
mp
jones
mc
kernel
smoothing
crc
press
1994
wang
j
jebara
t
chang
s-f
graph
transduction
via
alternating
minimization
proceedings
of
the
25th
international
conference
on
machine
learning
2008
1144-1151
wasserman
l
roeder
k
high
dimensional
variable
selection
annals
of
statistics
2009
37(5a
2178
pubmed
19784398
wassermann
l
all
of
nonparametric
statistics
springer
2006
mach
learn
res
author
manuscript
available
in
pmc
2020
january
28
sydurosnuepy
joyiny
sropun
da
odoing
syduosnuey
j0yiny
sipung
dn
2doinyg
hill
et
al
auc
page
19
ntrain
=900
ntrain
1000
02
04
06
0802
04
06
080.2
04
06
0.8
mrcl
kendall
ida
method
pearson
lasso
k-nn
figure
1
results
for
data
set
d1
yeast
data
random
sampling
area
under
the
roc
curve
auc
with
respect
to
causal
relationships
determined
from
unseen
interventional
data
as
function
of
the
fraction
of
labels
available
labels
were
sampled
at
random
results
are
shown
for
three
training
data
sample
sizes
r
results
are
mean
values
over
25
iterations
and
error
bars
indicate
standard
error
of
the
mean
additional
results
for
the
pc
algorithm
appear
in
appendix
see
text
for
details
mach
learn
res
author
manuscript
available
in
pmc
2020
january
28
sydurosnuepy
joyiny
sropun
da
odoing
sydimosnuepy
royiny
sropun
da
2domyg
page
20
02
04
06
0802
04
06
080.2
04
06
0.8
mrcl
kendall
ida
method
pearson
lasso
k-nn
figure
2
rfsults
for
data
set
d1
yeast
data
row-wise
sampling
as
figure
1
except
the
subset
of
labels
available
to
the
learner
were
obtained
by
sampling
entire
rows
of
the
causal
adjacency
matrix
as
before
proportion
were
sampled
the
remaining
rows
were
then
used
as
test
data
additional
results
for
the
pc
algorithm
appear
in
appendix
see
text
for
details
mach
learn
res
author
manuscript
available
in
pmc
2020
january
28
sydurosnuepy
joyiny
sropun
da
odoing
sydimosnuepy
royiny
sropun
da
2domyg
hill
et
al
page
21
p=04
nirain
96
0.5
p=038
b
oc
nirain
106
nirain
=116
0.5
_—-——r
0.5
10
0.5
specificity
mrcl
ida
pg
cnstrnts
pearson
k-nn
pc
cnstrnts
tc
method
kendall
pc
gies
lasso
pc(tc
gies
tc
figure
3
results
for
data
set
d1
yeast
data
comparison
including
gies
row-wise
sampling
roc
curves
are
shown
with
respect
to
causal
relationships
determined
from
unseen
interventional
data
tc
indicates
use
of
transitive
closure
operation
and
cnstrnts
indicates
that
the
background
information
was
included
via
input
constraints
results
for
pc
and
gies
are
shown
as
points
on
the
roc
plane
note
that
due
to
the
nature
of
input
required
by
gies
the
data
matrices
in
this
example
differ
from
the
row-wise
sampling
example
in
figure
see
text
for
details
results
are
averages
over
25
iterations
mach
learn
res
author
manuscript
available
in
pmc
2020
january
28
sydurosnuepy
joyiny
sropun
da
odoing
sydimosnuepy
royiny
sropun
da
2domyg
page
22
bt20
bt549
mcf7
uaccb1
{3’0
qq
@q
figure
4
results
for
data
set
d2
protein
time
course
data
each
panel
is
different
cell
line
with
its
own
training
and
interventional
test
data
auc
is
with
respect
to
an
entirely
held-out
intervention
see
text
for
details
mach
learn
res
author
manuscript
available
in
pmc
2020
january
28
sydurosnuepy
joyiny
sropun
da
odoing
sydimosnuepy
royiny
sropun
da
2domyg
hill
et
al
page
23
ntrain
200
ntrain
500
ntrain
820
auc
02
04
06
0802
04
06
0.80.2
04
06
0.8
mrcl
kendall
ida
method
pearson
lasso
k-nn
figure
5
rfsults
for
data
set
d3
human
cancer
data
data
are
protein
measurements
from
breast
cancer
patient
samples
from
the
cancer
genome
atlas
tcga
auc
is
with
respect
to
reference
graph
based
on
the
causal
biochemical
literature
results
are
mean
values
over
25
iterations
and
error
bars
indicate
standard
error
of
the
mean
see
text
for
details
additional
results
appear
in
appendix
c
mach
learn
res
author
manuscript
available
in
pmc
2020
january
28
