pyif
fast
and
light
weight
implementation
to
estimate
bivariate
transfer
entropy
for
big
data
kelechi
m
ikegwu
linois
informatics
institute
university
of
lllinois
at
urbana-champaign
urbana
il
usa
ikegwu2
@illinois.edu
robert
j
brunner
department
of
accountancy
university
of
lllinois
at
urbana-champaign
urbana
il
usa
bigdog
@illinois.edu
abstract—transfer
entropy
is
an
information
measure
that
quantifies
information
flow
between
processes
evolving
in
time
transfer
entropy
has
plethora
of
potential
applications
in
finan
cial
markets
canonical
systems
neuroscience
and
social
media
we
offer
fast
open
source
python
implementation
called
pyif
that
estimates
transfer
entropy
with
kraskov’s
method
pyif
utilizes
kd-trees
multiple
processes
by
parallelizing
queries
on
said
kd-trees
and
can
be
used
with
cuda
compatible
gpus
to
significantly
reduce
the
wall
time
for
estimating
transfer
entropy
we
find
from
our
analyses
that
pyif’s
gpu
implementation
is
up
to
1072
times
faster
and
it’s
cpu
implementation
is
up
181
times
faster
than
existing
implementations
to
estimate
transfer
entropy
on
large
data
and
scales
better
than
existing
implementations
index
terms—transfer
entropy
parallel
processing
i
introduction
information
theory
provides
framework
for
studying
the
quantification
storage
and
communication
of
information
1
this
theory
defines
entropy
as
the
amount
of
uncertainty
or
disorder
in
random
process
mutual
information
is
another
measure
in
this
theory
which
quantifies
the
amount
of
informa
tion
shared
across
random
variables
while
similar
to
mutual
information
transfer
entropy
te
also
considers
the
dynamics
of
information
and
how
these
dynamics
evolves
in
time
2
put
simply
te
quantifies
the
reduction
in
uncertainty
in
one
random
process
from
knowing
past
realizations
of
another
random
process
this
is
particularly
useful
property
of
te
as
many
real-world
phenomena
from
stock
market
prices
to
neural
signals
are
dynamic
processes
evolving
in
time
te
is
also
an
asymmetric
measure
of
information
transfer
ergo
te
computed
from
process
to
process
may
yield
different
result
than
te
computed
from
to
a
the
information
theoretic
framework
and
these
measures
have
led
to
variety
of
applications
in
different
research
areas
1
5
a
applications
of
transfer
entropy
te
is
particularly
useful
for
detecting
information
transfer
in
financial
markets
5
marschinski
and
kantz
used
te
to
978-1-7281-6861-6/20/$31.00
©2020
ieee
department
of
computer
science
university
of
illinois
at
urbana-champaign
jeff
mcmullin
department
of
accounting
indiana
university
bloomington
in
usa
jemcmull
@indiana.edu
jacob
trauger
urbana
il
usa
jtt2@illinois.edu
perform
index-to-index
analysis
with
the
dow
jones
share
market
djia
and
the
frankfurt
stock
index
dax
4
and
document
the
extent
to
which
one
index
drives
the
behavior
of
the
other
following
marschinski
and
kantz
other
researchers
apply
te
to
examine
related
research
questions
about
financial
markets
these
include
measuring
te
from
market
indexes
such
as
s&p
500
or
the
djia
to
individual
equities
as
well
as
between
individual
equities
5
network
inference
is
another
application
area
of
te
an
objective
of
network
inference
is
to
infer
network
by
inden
tifying
relationships
between
individual
processes
in
the
data
computational
neuroscience
financial
market
analysis
gene
regulatory
networks
social
media
and
multi-agent
systems
are
areas
where
te
has
been
used
to
model
networks
early
approaches
that
used
te
for
network
inference
either
measure
pairwise
te
between
all
pairs
of
variables
in
network
or
threshold
the
te
values
to
select
connections
between
nodes
in
network
12
13
and
14
recent
approaches
have
used
statistical
significance
tests
of
pairwise
te
to
determine
whether
links
exist
15
and
16
s
offers
more
examples
of
te
applications
b
outline
in
the
next
section
we
formally
define
te
the
following
section
discusses
te
estimation
methods
we
then
discuss
our
proposed
implementation
called
pyif
to
estimate
bivariate
te
next
we
describe
comparative
analysis
between
pyif
and
existing
implementations
that
estimate
bivariate
te
lastly
we
conclude
the
paper
with
discussion
and
future
work
ii
definition
of
transfer
entropy
in
2000
schreiber
2
discovered
te
and
coined
the
name
transfer
entropy
although
milian
palus
3
also
indepen
dently
discovered
the
concept
as
well
let
the
function
represent
mutual
information
between
two
probability
distri
butions
lagged
mutual
information
x
y
;
can
be
used
as
time-asymmetric
measure
of
information
transfer
from
to
where
and
are
both
random
processes
is
lag
period
and
is
the
current
time
period
however
lagged
mutual
information
is
unsatisfactory
as
it
does
not
account
for
shared
history
between
the
processes
and
6
te
considers
the
shared
history
between
two
processes
via
conditional
mutual
information
specifically
te
conditions
on
the
past
of
x
to
remove
any
redundant
or
shared
information
between
x
and
its
past
this
also
removes
any
information
in
the
process
about
at
time
that
is
in
the
past
of
7
transfer
entropy
7
where
the
transfer
of
information
occurs
from
to
x
can
be
defined
as
ty
x(t
i(xy
vi
x
1
kraskov
8
shows
that
transfer
entropy
can
be
expressed
as
the
difference
between
two
conditional
mutual
information
computations
ty
x(t
i(xy|
xo—p
yick
i(xy|
xo—k
2
the
intuition
of
this
definition
is
that
te
measures
the
amount
of
information
in
y;_
about
x
after
considering
the
information
in
x,_j
about
x
put
differently
te
quantifies
the
reduction
in
uncertainty
about
x
from
knowing
y;_j
after
considering
the
reduction
in
uncertainty
about
x
from
knowing
x,;_j
iii
estimating
transfer
entropy
there
are
many
techniques
for
estimating
mutual
informa
tion
khan
et
al
explored
the
utility
of
different
methods
for
mutual
information
estimation
10
and
many
of
the
methods
they
considered
are
applicable
to
estimate
te
a
kernel
density
estimator
kernel
density
estimators
can
be
used
to
estimate
te
11
for
bivariate
dataset
of
size
with
variables
and
y
mutual
information
can
be
estimated
as
zl
pxy
tz
y
ix,y
¥)=
px(wi)py
vi
3
where
px
x
and
py
y
are
the
estimated
marginal
prob
ability
density
functions
and
pxy
z;,y
is
the
joint
esti
mated
probability
density
function
for
multivariate
dataset
containing
x1,z2
.
£
where
each
is
in
d-dimensional
space
the
multivariate
kernel
density
estimator
with
kernel
is
defined
by
wz:l
where
is
the
smoothing
parameter
and
in
this
case
is
standard
multlvanate
normal
kernel
defined
by
k(z
2m)~%2¢*5
moon
et
al
outlined
procedure
to
estimate
mutual
lnformatlon
using
marginal
and
joint
probabilities
with
kernel
density
estimators
11
x
b
kraskov
estimator
transfer
entropy
can
be
estimated
using
k-nearest
neighbors
8
note
that
entropy
can
be
estimated
with
lo
x
===
inp(xi
®
=1
kraskov
et
al
expanded
this
definition
to
estimate
entropy
to
x
1
1
n
+in(cq
h(x
h
¥(n
+in(ca
dz
zlnel
6
where
are
the
number
of
data
points
are
the
nearest
neighbors
d
is
the
dimension
of
x
and
4
is
the
volume
of
the
d,-dimensional
unit
ball
for
two
random
variables
and
y
let
be
the
distance
between
z
y
and
it’s
kk
neighbor
be
denoted
by
kz
ky
let
and
be
defined
as
||z
ka;||
and
||y
y;||
respectively
n
is
the
number
of
points
z
such
that
||z
z;||
€,(i)/2
(
is
the
digamma
function
where
y(z
=t(z
1dl(z)/dz
and
i'(x
is
the
ordinary
gamma
function
lastly
1
c
where
c
0.5772156649
and
is
the
euler-mascheroni
con
stant
to
estimate
the
entropy
for
the
random
variable
y
can
be
substituted
into
h(x
joint
entropy
between
and
can
then
be
estimated
as
h(x,y
v(k
(
in(ca,ca
do+dy
zl
in(e(i
8
where
d
is
the
dimension
of
y
and
cd
is
the
column
of
the
d,,-dimensional
unit
ball
using
h(x
h(y
and
h(x,y
mutual
information
can
be
estimated
as
i(x,y
9(k
¥(ny(i
¥(n
72[11
ny
i
where
ny(z
is
the
number
of
points
y
such
that
||y
yil
this
method
has
been
referred
to
as
the
kraskov
esnmalor
in
literature
c
additional
estimators
khan
et
al
also
explored
the
utility
of
edgeworth
approxi
mation
of
differential
entropy
to
calculate
mutual
information
and
adaptive
partitioning
of
the
xy
plane
to
estimate
the
joint
probability
density
which
can
be
used
to
estimate
mutual
information
ultimately
khan
et
al
found
that
kde
esti
mator
and
kraskov
estimator
outperform
other
methods
with
respect
to
their
ability
to
capture
the
dependence
structure
of
random
processes
currently
our
software
supports
estimating
bivariate
te
using
the
kraskov
estimator
with
plans
to
add
other
estimators
in
the
future
iv
pyif
our
proposed
software
implementation
pyif
is
an
open
source
implementation
pyif
currently
only
supports
using
the
kraskov
estimator
to
estimate
te
pyif
utilizes
recent
advancements
in
hardware
to
parallelize
optimize
operations
across
cpus
and
cuda
compatible
gpus
9
and
cpus
in
particular
we
focus
our
efforts
on
the
parallelization
optimization
across
operations
to
obtain
n
n
in
eq
faster
pylif
is
python
only
implementation
which
utilizes
well
known
and
actively
supported
python
libraries
scipy
17
numpy
18
scikit-learn
19
nose
20
and
numba
21
scipy
is
an
open
source
python
library
used
for
variety
of
stem
applications
numpy
is
part
of
scipy’s
ecosystem
and
is
an
open
source
package
that
provides
convenient
ways
to
perform
matrix
manipulations
and
useful
linear
algebra
capa
bilities
the
library
scikit-learn
is
popular
open
source
library
for
machine
learning
and
nose
is
another
open
source
library
that
is
useful
for
testing
code
to
ensure
that
it
will
produce
the
correct
outcome
lastly
numba
is
python
compiler
that
can
compile
python
code
for
execution
on
multicore
cpus
and
cuda-capable
gpus
pyif’s
interface
only
requires
you
to
supply
and
y
two
numpy
arrays
with
nx1
dimensions
optional
arguments
can
be
passed
in
such
as
which
controls
the
number
of
neighbors
used
in
kd-tree
queries
embedding
which
controls
how
many
lagged
periods
are
used
to
estimate
transfer
entropy
and
boolean
argument
gpu
can
be
used
to
specify
if
you
want
to
use
cuda
compatible
gpu
lastly
another
boolean
argument
safetycheck
can
be
used
to
check
for
duplicates
rows
in
your
dataset
this
boolean
argument
is
there
to
help
prevent
more
subtle
error
that
can
occur
when
multiple
data
points
in
bivariate
dataset
have
identical
coordinates
this
essentially
can
lead
to
several
points
that
have
an
identical
distance
to
query
point
which
violates
assumptions
of
the
kraskov
estimator
solution
that
is
used
in
practice
and
that
we
recommend
is
to
add
small
amount
of
noise
to
your
dataset
to
avoid
this
error
v
comparative
analysis
we
compare
pyif’s
ability
to
estimate
transfer
entropy
against
existing
implementations
with
respect
to
computational
performance
we
present
all
of
the
data
and
code
used
to
estimate
te
for
all
implementations
2
each
implementation
in
this
comparative
analysis
estimates
te
on
four
simulated
bivariate
datasets
of
different
sizes
the
estimated
te
values
are
roughly
the
same
for
each
implementation
and
we
forgo
comparing
the
actual
values
since
this
is
random
simulated
ipyif
can
freely
be
downloaded
from
https://github.com/lcdm-uiuc/pyif
2the
data
and
code
can
freely
be
downloaded
from
https://github.com/lcdm-uiuc/publications/tree/master/2020_ikegwu_traguer_
mcmullin_brunner
data
we
make
the
assumption
that
there
is
relatively
little
to
no
information
transfer
between
the
random
processes
we
run
each
of
the
implementations
excluding
transfer
entropy
toolbox
on
nano
cluster
of
eight
supermicro
servers
with
intel
haswell/broadwell
cpus
and
nvidia
tesla
p100/v100
gpus
hosted
by
the
national
center
of
super
computing
ap
plications
at
the
university
of
illinois
at
urbana-champaign
we
used
one
node
which
contains
two
e5-2620
v3
intel
xeon
cpu’s
and
nvidia
p100
gpus
with
3584
cores
we
refer
to
this
analysis
as
analysis
1
we
conduct
the
same
analysis
on
different
hardware
to
com
pare
pyif
to
transfer
entropy
toolbox
because
of
matlab
licensing
issues
with
the
national
center
of
super
computing
applications
we
use
an
engineering
workstation
with
an
intel
xeon
processor
e5-2680
v4
hosted
by
engineering
it
shared
services
at
the
university
of
illinois
at
urbana-champaign
we
use
single
cpu
core
and
up
to
16gb
of
ram
to
estimate
te
with
transfer
entropy
toolbox
and
pyif
this
workstation
does
not
offer
cuda
compatiable
gpus
to
use
for
either
pyif
or
transfer
entropy
toolbox
so
we
forgo
comparing
the
gpu
implementations
this
workstation
has
cpu
time
limit
of
60
minutes
meaning
that
if
any
process
uses
100%
of
cpu
core
for
more
than
60
minutes
the
process
is
terminated
we
refer
to
this
analysis
as
analysis
2
a
idtxl
the
first
implementation
is
the
information
dynamics
toolkit
xi
idtxl
idtxi
is
an
open
source
python
tool
box
for
network
inference
22
currently
idtxi
relies
on
numpy
scipy
cffi
which
is
another
open
source
library
that
provides
interface
for
python
code
hspy
which
is
python
package
that
is
used
to
interface
with
hdf5
binary
data
format
jpype
which
is
python
module
that
provides
java
interface
for
python
code
and
java
jdk
which
is
developer
kit
to
develop
java
applications
and
applets
idtxi
has
additional
functionaility
besides
estimating
te
however
we
only
use
idtxi’s
capability
to
estimate
te
on
bivariate
dataset
b
transent
transent
is
package
that
estimates
transfer
entropy
23
currently
transent
relies
on
rcpp
which
acts
as
interface
to
c++
from
r
transent
also
relies
on
c++
library
called
appromixate
nearest
neigbors
ann
24
which
performs
exact
and
approximate
nearest
neighbor
searches
currently
the
package
has
been
removed
from
cran
however
this
software
can
be
used
and
installed
from
23]’s
github
repo
c
rtransferentropy
rtransferentropy
is
package
that
estimates
transfer
entropy
between
two
time
series
25
currently
the
rtransfer
entropy
package
relies
on
repp
and
the
future
package
which
supports
performing
computations
in
parallel
to
decrease
the
wall
time
we
include
both
the
parallel
implementation
of
23
github
repo
https:/github.com/healthcast/transent
rtransferentropy
and
the
default
implementation
for
com
pleteness
in
the
results
d
transfer
entropy
toolbox
transfer
entropy
toolbox
is
an
open
source
matlab
toolbox
for
transfer
entropy
estimation
26
this
code’s
de
pendencies
include
the
statistics
machine
learning
toolbox
which
provides
functions
to
analyze
and
model
data
the
fieldtrip
toolbox
which
is
used
for
eeg
ieeg
meg
and
nirs
analysis
the
parallel
computing
toolbox
that
performs
parallel
computations
of
multicore
cpus
and
gpus
the
signal
processing
toolbox
that
provides
functions
to
analyze
prepro
cess
and
extract
features
from
sampled
signals
the
tstool
toolbox
which
is
toolbox
for
nonlinear
time
series
analysis
tstool
no
longer
exists
and
cannot
be
download
from
it’s
official
homepage
*
nevertheless
the
developers
of
transfer
entropy
toolbox
include
pre-compiled
mex
files
of
tstool
that
will
work
with
this
implementation
at
the
time
of
writing
this
paper
transfer
entropy
toolbox
has
not
been
updated
since
the
year
2017
e
data
we
create
four
bivariate
datasets
for
this
comparative
analy
sis
each
dataset
contains
two
time
series
with
randomly
gen
erated
values
between
and
1
the
first
dataset
contains
1000
observations
the
second
dataset
contains
10,000
observations
the
third
dataset
contains
100,000
observations
and
the
fourth
dataset
contains
1,000,000
observations
we
used
the
seed
23
for
the
pseudo-random
number
generator
for
reproducibility
we
will
refer
to
the
first
dataset
second
dataset
third
dataset
and
fourth
dataset
as
the
micro
dataset
small
dataset
medium
dataset
and
the
large
dataset
respectively
vi
results
we
report
the
results
for
analysis
in
table
after
estimating
te
using
all
of
the
implementations
outlined
in
the
comparative
analysis
section
we
found
that
pyif
scales
better
on
larger
data
excluding
the
transent
implementation
the
cpu
implementation
of
pyif
or
pyif
cpu
takes
less
time
to
estimate
transferentropy
than
all
other
implementations
the
package
transent
has
better
performance
in
terms
of
speed
than
pyif
cpu
for
the
micro
dataset
and
the
small
dataset
however
pyif
cpu
is
able
to
estimate
transfer
entropy
in
less
time
than
all
other
implementations
for
the
medium
dataset
and
large
dataset
pyif
gpu
outperforms
pyif
cpu
for
the
small
medium
and
large
datasets
figure
visualizes
this
explanation
we
suspect
that
the
optimizations
performed
by
numba
contribute
to
pyif
having
larger
wall
time
than
transent
on
the
micro
and
small
datasets
the
results
for
analysis
are
in
table
ii
although
the
transfer
entropy
toolbox
exceeds
the
cpu
time
limit
for
the
large
dataset
the
results
show
that
pyif
is
able
to
scale
better
than
transfer
entropy
toolbox
for
the
other
three
datasets
pyif’s
wall
times
are
less
than
transfer
entropy
toolbox’s
http://www.dpi.physik.uni
goettingen.de/tstool/
idtxi
100
transent
~e
rtransferentropy
8|
—
rtransferentropy
parallel
pyif
cpu
pyif(gpu
4
_
o
1,000
10,000
100,000
1,000,000
number
of
random
samples
fig
1
this
figure
shows
the
natural
log
time
in
seconds
to
estimate
transfer
entropy
for
each
implementation
excluding
transfer
entropy
toolbox
for
each
dataset
used
in
this
study
pyif
cpu
transfer
entropy
toolbox
g6
sa
22
1,600
10,000
100,000
1,006,000
number
of
random
samples
fig
2
this
figure
shows
the
natural
log
time
in
seconds
to
estimate
transfer
entropy
between
pyif
and
transfer
entropy
toolbox
on
an
engineering
workstation
as
described
in
the
section
comparative
analysis
transfer
entropy
toolbox
exceeded
the
maximum
allowable
cpu
runtime
for
the
large
dataset
wall
times
excluding
the
micro
dataset
figure
visualizes
this
explanation
vii
conclusion
an
important
issue
is
addressed
regarding
big
data
with
respect
to
estimating
bi-variate
transfer
entropy
we
introduce
fast
solution
to
estimate
transfer
entropy
with
small
amount
of
dependencies
on
large
data
our
implementation
pylif
is
up
to
1072
times
faster
utilizing
gpus
and
up
to
181
times
faster
utilizing
cpus
than
existing
implementations
that
estimate
bi-variate
te
pyif
is
also
open
sourced
and
publicly
available
on
github
for
anyone
to
use
for
future
work
we
plan
to
improve
the
existing
code
base
to
increase
the
computational
performance
of
pyif
even
further
in
addition
to
this
we
plan
to
implement
additional
estimators
outlined
in
the
section
entitled
estimating
transfer
entropy
to
estimate
bi-variate
te
this
boost
in
computational
performance
will
enable
researchers
to
estimate
bi-variate
te
much
faster
for
variety
of
research
applications
acknowledgments
this
work
was
partially
funded
by
the
graduate
college
fellowship
program
at
the
university
of
illinois
this
work
table
the
wall
time
to
estimate
transfer
entropy
for
variety
of
implementations
on
the
different
data
sets
described
in
the
section
comparative
analysis
the
higher
the
wall
time
the
longer
it
took
for
the
implementation
to
estimate
transfer
entropy
the
number
in
the
relative
performance
indicates
how
many
times
faster
or
slower
pyif
cpu
is
to
particular
implementation
implementation
wall
relative
time
in
performance
seconds
to
pyif
cpu
micro
dataset
results
1000
obs
idtxi
10.98
428
transent
0.656
0.25
rtransferentropy
12.492
4.87
rtransferentropy
parallel
2.876
1.12
pylif
cpu
2.564
1.00
pyif
gpu
3.282
1.28
small
dataset
results
10,000
obs
idtxi
100.23
31.94
transent
0.968
308
rtransferentropy
102.228
32.57
rtransferentropy
parallel
15.703
5.00
pylif
cpu
3.138
1.00
pyif
gpu
1.98
0.63
medium
dataset
results
100,000
obs
idtxi
1070.749
152.89
transent
21.708
3.03
rtransferentropy
1036.661
152.00
rtransferentropy
parallel
127.281
18.66
pylif
cpu
6.82
1.00
pyif
gpu
3.996
0.58
large
dataset
results
1,000,000
obs
idtxi
43150.129
181.97
transent
1585.942
6.68
rtransferentropy
10592.77
44.67
rtransferentropy
parallel
1188.636
5.01
pylif
cpu
237.122
1.00
pyif
gpu
40.231
0.16
table
11
the
wall
time
and
relative
performance
to
pyif
cpu
to
estimate
transfer
entropy
between
pyif
and
transfer
entropy
toolbox
on
an
engineering
workstation
machine
as
described
in
the
section
comparative
analysis
wall
time
in
seconds
relative
performance
to
pyif
cpu
micro
dataset
results
1000
obs
pyif
cpu
16.049
1.00
transfer
entropy
toolbox
2.5012
0.15
small
dataset
results
10,000
obs
pyif
cpu
4.989
1.00
transfer
entropy
toolbox
21.6880
4.347
medium
dataset
results
100,000
obs
pyif
cpu
20.915
1.00
transfer
entropy
toolbox
616.8712
29.49
large
dataset
results
1,00,000
obs
1455.725
1.00
3600
247
implementation
pyif
cpu
transfer
entropy
toolbox
utilizes
resources
provided
by
the
innovative
systems
labora
tory
at
the
national
center
for
supercomputing
applications
at
the
university
of
illinois
at
urbana-champaign
lastly
we
would
like
to
thank
alice
perng
for
helpful
work
in
analysis
2
11
12
20
21
references
stone
james
v
information
theory
tutorial
introduction
s.i
sebtel
press
2015
print
schreiber
thomas
2000
measuring
information
transfer
physical
review
letters
85
461-4
10.1103/physrevlett.85.461
palus
milan
komarek
vladimir
hrncir
zbynek
sterbovd
katalin
2001
synchronization
as
adjustment
of
information
rates
de
tection
from
bivariate
time
series
physical
review
e
statistical
nonlin
ear
and
soft
matter
physics
63
046211
10.1103/physreve.63.046211
r
marschinski
and
h
kantz
analysing
the
information
flow
between
financial
time
series
the
european
physical
journal
b-condensed
matter
and
complex
systems
30(2):275-281
2002
bossomaier
terry
lionel
barnett
michael
harre
and
joseph
t
lizier
an
introduction
to
transfer
entropy
information
flow
in
complex
systems
cham
springer
2016
internet
resource
a
kaiser
and
t
schreiber
information
transfer
in
continuous
processes
physica
d
166:43-62
2002
p
l
williams
and
r
d
beer
generalized
measures
of
information
transfer
2011
arxiv:1102.1507
a
kraskov
h
stogbauer
and
p
grassberger
estimating
mutual
infor
mation
physical
review
e
69:066138-066153
2004
sanders
jason
and
edward
kandrot
cuda
by
example
an
intro
duction
to
general-purpose
gpu
programming
addison-wesley
pro
fessional
2010
khan
shiraj
bandyopadhyay
sharba
ganguly
auroop
saigal
sunil
erickson
david
protopopescu
vladimir
ostrouchov
george
2007
relative
performance
of
mutual
information
estimation
methods
for
quantifying
the
dependence
among
short
and
noisy
data
physical
review
e
statistical
nonlinear
and
soft
matter
physics
76
026209
10.1103/physreve.76.026209
y
i
moon
b
rajagopalan
and
u
lall
phys
rev
52
2318
1995
c
damiani
and
p
lecca
model
identification
using
correlation-based
inference
and
transfer
entropy
estimation
in
proceedings
of
the
2011
fifth
uksim
european
symposium
on
computer
modeling
and
simu
lation
ems
pages
129-134
ieee
2011
t
l
bauer
r
colbaugh
k
glass
and
d
schnizlein
use
of
transfer
entropy
to
infer
relationships
from
behavior
in
proceedings
of
the
eighth
annual
cyber
security
and
information
intelligence
research
workshop
csiirw
13
new
york
ny
usa
2013
acm
c
j
honey
r
kotter
m
breakspear
and
o
sporns
network
structure
of
cerebral
cortex
shapes
functional
connectivity
on
multiple
time
scales
proceedings
of
the
national
academy
of
sciences
of
the
united
states
of
america
104(24):10240-10245
2007
r
vicente
and
m
wibral
efficient
estimation
of
information
transfer
in
m
wibral
r
vicente
and
j
t
lizier
editors
directed
information
measures
in
neuroscience
understanding
complex
systems
pages
37-58
springer
berlin/heidelberg
2014
m
wibral
r
vicente
and
m
lindner
transfer
entropy
in
neuro
science
in
m
wibral
r
vicente
and
j
t
lizier
editors
directed
information
measures
in
neuroscience
understanding
complex
sys
tems
pages
3-36
springer
berlin/heidelberg
2014
jones
eric
travis
oliphant
and
pearu
peterson
scipy
open
source
scientific
tools
for
python
2001
developers
numpy
numpy
numpy
numpy
scipy
developers
2013
pedregosa
fabian
et
al
scikit-learn
machine
learning
in
python
journal
of
machine
learning
research
12.0ct
2011
2825-2830
pellerin
jason
nose
nose
1.3.7
documentation
https://nose.readthedocs.io/en/latest/
lam
siu
kwan
antoine
pitrou
and
stanley
seibert
numba
llvm
based
python
jit
compiler
proceedings
of
the
second
workshop
on
the
llvm
compiler
infrastructure
in
hpc
acm
2015
22
23
24
25
26
wollstadt
patricia
lizier
joseph
vicente
raul
finn
conor
martinez
zarzuela
mario
mediano
pedro
novelli
leonardo
wibral
michael
2019
idtxi
the
information
dynamics
toolkit
xi
python
package
for
the
efficient
analysis
of
multivariate
information
dynamics
in
networks
journal
of
open
source
software
4
1081
10.21105/j0ss.01081
ann
library
david
mount
sunil
arya
transfer
entropy
packge
ghazaleh
haratinezhad
torbati
and
glenn
lawyer
2015
transfer
entropy
the
transfer
entropy
package
package
version
1.5
https
/icran.r-project.org/package=transferentropy
mount
d.m
ann
programming
manual
department
of
computer
science
university
of
maryland
college
park
md
usa
2006
pp
4-25
simon
b
thomas
d
franziska
j
p
david
j
2019
rtrans
ferentropy
quantifying
information
flow
between
different
time
series
using
effective
transfer
entropy
softwarex
10(100265
1-9
https://doi.org/10.1016/j.s0ftx.2019.100265
lindner
michael
vicente
raul
priesemann
viola
wibral
michael
2011
trentool
matlab
open
source
toolbox
to
analyse
information
flow
in
time
series
data
with
transfer
entropy
bmc
neuroscience
12
119
10.1186/1471-2202-12-119
