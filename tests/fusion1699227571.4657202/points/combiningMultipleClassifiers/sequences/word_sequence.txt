418
ieee
transactions
on
systems
man
and
cybernetics
vol
22
no
3
may/june
1992
methods
of
combining
multiple
classifiers
and
their
applications
to
handwriting
recognition
lei
xu
adam
krzyzak
member
ieee
and
ching
y
suen
fellow
ieee
abstract
method
of
combining
the
classification
powers
of
several
classifiers
is
regarded
as
general
problem
in
various
application
areas
of
pattern
recognition
and
systematic
inves
tigation
has
been
made
possible
solutions
to
the
problem
can
be
divided
into
three
categories
according
to
the
levels
of
infor
mation
available
from
the
various
classifiers
four
approaches
are
proposed
based
on
different
methodologies
for
solving
this
problem
one
is
suitable
for
combining
individual
classifiers
such
as
bayesian
k-nn
and
various
distance
classifiers
the
other
three
could
be
used
for
combining
any
kind
of
individual
clas
sifiers
on
applying
these
methods
to
combine
several
classifiers
for
recognizing
totally
unconstrained
handwritten
numerals
the
experimental
results
show
that
the
performance
of
individual
classifiers
could
be
improved
significantly
for
example
on
the
u.s
zipcode
database
the
result
of
98.9%
recognition
with
0.90%
substitution
and
0.2%
rejection
can
be
obtained
as
well
as
high
reliability
with
95%
recognition
0%
substitution
and
5%
rejection
these
results
compared
favorably
to
other
research
groups
in
europe
asia
and
north
america
1
introduction
ecently
in
the
area
of
character
recognition
the
concept
of
combining
multiple
classifiers
is
proposed
as
new
direction
for
the
development
of
highly
reliable
character
recognition
systems
1
and
some
preliminary
results
have
indicated
that
the
combination
of
several
complementary
clas
sifiers
will
improve
the
performance
of
individual
classifiers
11-[4
we
believe
that
the
combination
of
multiple
classifiers
is
general
problem
that
is
interesting
not
only
to
the
character
recognition
area
but
also
to
various
application
areas
of
pattern
recognition
the
main
reasons
come
from
two
aspects
first
in
almost
any
one
of
the
current
pattern
recognition
application
areas
such
as
character
recognition
speech
recognition
remote
sensing
geophysical
prospecting
and
medical
applications
as
well
as
many
others
1}-{18
29
there
are
number
of
classification
algorithms
available
these
algorithms
are
based
on
different
theories
and
methodologies
broadly
speaking
we
have
now
two
large
groups
of
methods
namely
feature
vector-based
methods
and
syntactic-and-structural
methods
furthermore
each
group
includes
many
algorithms
that
are
based
on
variety
of
methodologies
e.g
for
the
first
group
alone
there
exist
bayes
classifier
k-nn
classifier
various
manuscript
received
october
21
1990
revised
august
13
1991
this
work
was
supported
in
part
by
the
natural
sciences
and
engineering
research
council
of
canada
and
in
part
by
the
national
networks
of
centres
of
excellence
program
of
canada
the
authors
are
with
the
centre
for
pattern
recognition
and
machine
intel
ligence
concordia
university
1455
de
maisonneuve
blvd
west
montreal
pq
h3g
1m8
canada
ieee
log
number
9104551
distance
classifiers
and
neural
network
based
classifiers
.
etc
usually
for
specific
application
problem
each
of
these
classifiers
could
attain
different
degree
of
success
but
maybe
none
of
them
is
totally
perfect
or
even
not
as
good
as
expected
for
practical
applications
so
there
is
need
to
study
the
methodology
of
integrating
the
results
of
number
of
different
classification
algorithms
so
that
better
result
could
be
obtained
the
second
aspect
is
that
for
specific
recognition
problem
usually
numerous
types
of
features
could
be
used
to
represent
and
recognize
patterns
to
make
strong
impression
on
this
fact
some
examples
are
presented
as
follows
1
in
character
recognition
the
usuable
features
may
come
from
density
of
point
measurements
moments
charac
teristic
loci
mathematical
transforms
fourier
walsh
hadamard
)
they
may
also
come
from
skeletons
or
contours
such
as
loop
endpoint
junction
arc
concav
ities
and
convexities
stroke
.
1
2
2
in
applications
related
to
texture
analysis
such
as
remote
sensing
and
scene
analysis
the
usable
features
may
come
from
co-occurrence
matrix
fourier
descriptors
power
spectrum
moments
contrasts
as
well
as
various
structural
primitives
7
29
3
in
waveform
analysis
and
recognition
such
as
seismic
signal
eeg
and
ecg
speech
recognition
and
speaker
identification
underwater
acoustics
as
well
as
recog
nition
of
curve-like
images
the
usable
features
may
come
from
power
spectrum
ar
modeling
function
approximation
zero
crossing
hidden
markov
modeling
and
many
types
of
structural
line
segments
8]-[15
no
doubt
many
other
examples
in
various
pattern
recogni
tion
application
areas
can
still
be
found
these
features
are
represented
in
very
diversified
forms
g
they
may
be
continuous
variables
binary
values
discrete
labels
structural
primitives
it
is
very
difficult
to
lump
them
together
into
one
single
classifier
to
make
decision
as
result
many
classifiers
are
needed
to
handle
the
different
types
of
features
more
specifically
there
are
three
different
cases
where
different
ways
of
processing
are
required
in
the
first
case
the
features
belong
to
types
that
are
drastically
different
e.g
continuous
variables
and
structural
primitives
and
classifiers
based
on
different
theories
and
methodologies
e.g
feature
based
methods
and
syntactic
methods
are
needed
to
treat
such
features
this
case
was
discussed
earlier
in
the
first
aspect
in
the
second
case
the
features
may
be
different
not
in
the
form
of
representation
but
also
in
the
physical
meanings
e.g
for
set
of
features
that
are
represented
in
the
form
of
continuous
variables
suppose
that
some
of
them
are
0018-9472/92803.00
1992
ieee
xu
et
al
methods
of
combining
multiple
classifiers
and
their
applications
to
handwriting
recognition
features
representing
the
pattern’s
volume
some
representing
the
pattern’s
temperature
etc
then
to
lump
these
features
into
vector
for
one
classifier
we
need
first
to
normalize
the
scales
of
these
features
this
job
is
usually
also
quite
difficult
however
it
may
become
much
easier
if
we
integrate
the
results
of
several
classifiers
each
of
which
uses
features
representing
the
same
type
of
physical
property
and
thus
being
of
the
same
scale
finally
in
the
third
case
even
if
normalization
is
not
required
in
lumping
lot
of
variables
into
one
very
high-dimensional
vector
it
may
still
be
good
idea
to
divide
the
high-dimensional
vector
into
several
vectors
with
lower
dimensions
as
input
to
several
classifiers
since
it
is
well
known
that
high-dimension
vectors
will
not
only
increase
computational
complexity
but
will
also
produce
implementation
problems
and
accuracy
problems
of
course
the
combination
of
multiple
classifiers
is
by
no
means
the
only
solution
for
the
problem
involving
variety
of
features
the
hybrid
systems
of
statistical
and
syntactical
or
structural
methods
have
been
developed
5
16
to
cover
the
first
case
mentioned
previously
various
ways
of
feature
selection
and
dimension
reduction
19
have
been
proposed
to
solve
the
problems
associated
with
the
third
case
multistage
system
20
multilevel
hierarchical
classifier
21
especially
tree
classifiers
{22
23
have
been
investigated
extensively
however
even
though
they
may
succeed
up
to
certain
degree
and
certain
aspects
they
too
encounter
many
of
their
own
difficulties
thus
new
direction
of
combining
multiple
classifiers
is
certainly
worthwhile
to
be
explored
further
1t
appears
that
the
study
on
the
combination
problem
is
now
only
at
its
preliminary
stage
it
has
started
in
the
character
recognition
area
due
to
the
complexity
of
handwritten
char
acter
recognition
recently
it
has
been
realized
that
classifiers
based
on
different
methodologies
or
different
features
are
usually
complementary
to
each
other
{16
17
thus
efforts
have
been
made
to
develop
various
complementary
classifiers
or
those
called
expert
modules
1
2
5
for
handwritten
character
recognition
then
it
naturally
raises
the
question
of
obtaining
consensus
on
the
results
of
each
individual
classifier
or
expert
presently
three
kinds
of
effort
have
been
made
toward
this
direction
one
makes
use
of
the
majority
voting
principle
1
2
i.e
each
individual
classifier
represents
one
score
that
is
either
as
whole
assigned
to
one
class
label
or
divided
into
several
labels
the
label
which
receives
more
than
half
of
the
total
scores
is
taken
as
the
final
result
the
second
uses
kind
of
candidate
subset
combining
and
re-ranking
approach
3
4
namely
each
individual
classifier
produces
subset
of
ranked
candidate
labels
and
the
labels
in
the
union
of
all
subsets
are
re
ranked
based
on
their
old
ranks
in
each
subset
the
third
applies
dempster—shafer
d-s
theory
on
the
special
case
of
combining
several
individual
distance
classifiers
6
the
distance
calculated
by
each
individual
classifier
is
transformed
by
some
way
into
confidence
value
between
0
1
which
is
used
as
the
basic
probability
assignment
of
the
only
one
focal
element
the
simplest
combining
rule
based
on
d-s
theory
is
used
in
the
special
case
to
combine
the
contribution
of
each
individual
to
give
the
final
result
the
results
of
these
efforts
on
handwritten
character
recognition
or
on
line
script
recognition
419
are
quite
interesting
and
inspiring
1]-{6
in
this
paper
we
propose
to
conduct
more
systematical
investigation
into
the
problem
of
multiclassifier
combination
generally
speaking
we
could
consider
that
it
consists
of
two
parts
the
first
part
being
closely
dependent
on
the
specific
applications
includes
the
problems
of
how
many
classifiers
are
chosen
for
specific
application
problem
what
kind
of
classifiers
should
be
used
and
for
each
classifier
what
types
of
features
should
be
chosen
as
well
as
other
problems
that
relate
to
the
construction
of
those
individual
and
complemen
tary
classifiers
papers
1
2
have
already
described
lot
of
work
on
the
recognition
of
totally
unconstrained
handwritten
numerals
this
paper
does
not
intend
to
study
the
problems
of
this
part
the
second
part
which
is
general
and
common
to
various
applications
includes
the
problems
related
to
the
question—how
to
combine
the
results
of
different
existing
classifiers
so
that
better
result
can
be
obtained
this
paper
will
concentrate
on
problems
related
to
this
second
part
in
section
ii
we
first
summarize
the
problems
of
combining
multiclassifiers
into
three
categories
according
to
the
levels
of
information
produced
by
various
classifiers
then
in
the
following
section
several
approaches
have
been
proposed
to
tackle
these
problems
these
approaches
include
new
versions
as
well
as
general
form
of
voting
principle
an
averaged
bayes
classifier
and
its
version
combination
approach
in
bayesian
formalism
and
combination
approach
in
demp
ster—shafer
formalism
the
latter
two
especially
are
new
approaches
adapted
from
the
literature
of
evidence
gathering
and
uncertainty
reasoning
the
approaches
proposed
in
this
paper
are
applied
to
the
problems
of
recognizing
totally
un
constrained
handwritten
numerals
the
four
experts
presented
in
1
2
are
used
as
the
individual
classifiers
the
obtained
combination
results
are
significantly
better
than
any
individual
classifier
e.g
on
the
same
database
as
1
2
and
by
combining
the
four
individual
classifiers
given
there
the
result
could
give
98.9%
recognition
0.9%
substitution
and
0.2%
rejection
if
it
is
required
to
suppress
the
substitution
rate
the
results
could
give
95%
recognition
0%
substitution
and
5%
rejection
while
the
individual
classifier
with
the
best
performance
among
the
four
can
only
provide
the
result
of
93.9%
recognition
1.6%
substitution
and
4.5%
rejection
11
the
problem
of
combining
multiple
classifiers
a
three
levels
in
classifier’s
output
information
given
pattern
space
consisting
of
mutually
exclusive
sets
cy
-
cpy
with
each
of
c
vi
{1,2
m}
representing
set
of
specified
patterns
called
class
e.g
10
for
the
problem
of
numerals
recognition
for
sample
from
p
the
task
of
classifier
denoted
e
is
to
assign
one
index
au
{m
1}
as
label
to
represent
that
is
regarded
as
being
from
class
c
if
1
with
+1
denoting
that
has
no
idea
about
which
class
comes
from
or
in
other
words
is
rejected
by
e
regardless
what
internal
structure
classifier
has
and
on
what
theory
and
methodology
it
bases
we
may
simple
regard
classifier
as
420
function
box
that
receives
an
input
sample
and
outputs
label
j
or
in
short
denoted
by
e(x
j
although
is
the
output
information
we
only
want
at
the
final
stage
of
classification
practically
many
of
the
existing
classification
algorithms
usually
supply
or
are
able
to
supply
some
other
related
information
for
example
bayes
classifier
may
also
supply
values
of
post-probabilities
p(i/z),1
1,---,m
for
each
possible
label
in
fact
the
final
label
is
the
result
of
maximum
selection
from
the
values
and
this
selection
certainly
discards
some
information
that
is
considered
useless
for
the
final
output
when
there
is
only
single
classifier
however
such
discarded
information
may
be
useful
for
multiclassifier
combination
depending
on
whether
some
output
information
other
than
one
label
is
used
and
the
other
kind
of
information
is
used
we
will
have
different
types
of
multiclassifier
combination
problems
generally
speaking
the
output
information
that
various
classification
algorithms
supply
or
are
able
to
supply
can
be
divided
into
three
levels
1
the
abstract
level
classifier
only
outputs
unique
label
3
or
for
some
extension
outputs
subset
a
2
the
rank
level
ranks
all
the
labels
in
or
a
subset
a
in
queue
with
the
jabe
at
the
top
being
the
first
choice
3
the
measurement
level
attributes
each
label
in
measurement
value
to
address
the
degree
that
has
the
label
among
the
three
levels
the
measurement
level
contains
the
highest
amount
of
information
and
the
abstract
level
contains
the
lowest
from
the
measurements
attributed
to
each
label
we
could
rank
all
the
labels
in
according
to
rank
rule
e.g
ascending
or
descending
by
choosing
the
label
at
the
top
rank
or
directly
by
choosing
the
label
with
the
maximal
or
minimal
value
at
the
measurement
level
we
can
assign
unique
label
to
z
in
other
words
from
the
measurement
level
to
the
abstract
level
there
is
an
information
reduction
process
or
abstraction
process
many
classification
algorithms
are
able
to
supply
output
information
from
the
measurement
level
e.g
bayes
classifier
supplies
the
post-probabilities
p(i/z),i
and
various
distance
classifiers
supply
the
distance
between
and
each
prototype
sample
of
each
class
as
the
measurements
n
other
words
processing
at
the
measurement
level
is
an
intermediate
stage
of
many
classifiers
however
some
of
classifiers
may
be
able
to
supply
the
output
information
only
from
the
abstract
level
e.g
the
pure
syntactic
classifier
b
three
types
of
problems
for
multiple
classifier
combination
according
to
which
of
the
aforementioned
three
output
information
levels
combination
is
based
upon
various
prob
lems
of
combining
multiple
classifiers
could
be
summarized
into
the
following
three
types
type
1
the
combination
is
made
based
on
the
output
in
formation
of
the
abstract
level
given
individual
classifiers
er
k=1
each
of
which
assigns
input
to
label
j
i.e
produces
an
event
ez(z
ji
the
problem
is
to
use
these
ieee
transactions
on
systems
man
and
cybernetics
vol
22
no
3
may/june
1992
events
to
build
an
integrated
classifier
e
which
gives
one
definitive
label
j
i.e
e(z
j,j
au{m
+1}
type
2
the
combination
is
made
based
on
the
output
information
of
the
rank
level
for
an
input
each
e
produces
subset
ly
with
all
the
labels
in
ly
ranked
in
queue
the
problem
is
to
use
these
events
e(z
ly
k=
1
to
build
an
with
e(z
j,j
au{m
+1}
type
3
the
combination
is
made
based
on
the
output
information
of
the
measurement
level
for
an
input
z
each
ey
produces
real
vector
k
mk(1
-
mx(m
where
my(7
denotes
kind
of
degree
that
considers
that
has
label
i
the
problem
is
to
use
these
events
e(z
m.(k
k=1,---,k
tobuild
an
with
e(z
j
au{m
+1}
the
three
types
of
problems
described
previously
cover
the
different
scopes
of
applications
on
the
problem
of
type
1
the
individual
classifiers
could
be
very
different
from
each
other
in
their
theories
or
methodologies
e.g
ex
may
base
on
statistical
method
while
e
on
syntactic
method
in
fact
any
kind
of
classifier
will
at
least
supply
the
output
information
at
the
abstract
level
so
it
could
be
said
that
the
problem
of
type
covers
all
kinds
of
pattern
recognition
areas
thus
this
type
of
problem
should
be
most
interesting
in
contrast
the
problem
of
type
requires
that
all
the
individual
classifiers
should
be
able
to
supply
the
output
information
at
the
measurement
level
furthermore
if
there
are
any
measurement
vectors
of
different
kinds
say
m.(k
is
vector
of
postprobabilities
while
m
1
vector
of
some
kind
of
distances
the
measurements
should
be
able
to
be
transformed
into
the
same
kind
of
measurement
since
reasonable
combination
operation
on
these
measurements
could
be
made
only
when
they
have
the
same
measure
scale
the
problem
of
type
has
generality
between
type
and
type
3
it
requires
that
all
the
individual
classifiers
be
able
to
supply
the
output
information
at
the
rank
level
thus
its
individual
classifier
could
not
be
pure
syntactic
classifier
that
only
outputs
one
label
but
could
be
any
classifier
that
is
able
to
supply
output
information
at
the
measurement
level
since
ranked
list
ly
could
be
easily
obtained
from
the
correspondent
measurement
vector
m
k
the
combination
problem
studied
in
1
2
belongs
to
type
1
and
the
problem
studied
in
3
4
belongs
to
type
2
in
this
paper
we
will
study
the
problems
of
type
in
section
iil
then
in
the
following
four
sections
we
concentrate
on
the
problem
of
type
because
we
think
that
it
is
the
most
useful
one
due
to
its
generality
1ii
averaged
bayes
classifier
and
its
versions
a
averaged
bayes
classifier
we
use
this
section
to
discuss
the
combination
problem
of
type
3
first
we
look
at
special
case
that
all
individual
classifiers
are
bayes
classifiers
for
bayes
classifier
e
its
classification
of
an
input
is
actually
based
on
set
of
real
value
measure
ments—postprobabilities
p(zcci/z),i=1
o
xu
et
al
methods
of
combining
multiple
classifiers
and
their
applications
to
handwriting
recognition
421
where
c
denotes
that
comes
from
class
c
in
the
convention
of
statistical
pattern
recognition
literature
these
probabilities
are
simply
denoted
by
p(c;/z
vi
a
they
represent
the
probabilities
that
comes
from
each
of
the
classes
under
the
condition
in
principle
these
probabilities
are
not
related
to
each
classifier
e
but
in
practice
that
each
classifies
is
not
really
based
on
those
true
values
of
1
which
are
not
available
instead
for
each
z
estimates
by
itself
set
approximations
of
those
true
values
these
approximations
depend
on
what
features
are
used
and
how
ey
is
trained
to
clarify
such
dependence
we
denote
them
as
follows
pi(r
cifz),i=1
mk=1
k
for
any
e
definitive
decision
is
made
as
ex(x
jwith
pi(z
c;/z
maxieapi(z
ci/x
3
now
we
don’t
care
about
the
results
of
3
instead
we
use
the
approximations
of
2
for
combining
the
classification
tesults
on
the
same
by
all
classifiers
one
simple
approach
here
we
propose
is
to
use
the
following
average
value
as
new
estimation
of
combined
classifier
e
pre
cifr
pz
cifa)i=
1
m
(
k=1
the
final
decision
made
by
this
is
given
by
e(z
j
with
pg(z
c;/z
maxieape(z
ci/z
that
is
bayes
decision
is
based
on
these
newly
estimated
post-probabilities
so
we
call
such
combined
as
an
averaged
bayes
classifier
if
we
expect
that
the
classified
results
are
more
reliable
we
could
use
the
following
equation
to
replace
5
to
take
into
account
the
trade-off
between
the
substitution
rate
and
the
rejection
rate
in
6
shown
at
the
bottom
of
the
page
with
being
threshold
another
alternative
is
to
use
the
median
value
of
py(z
cifz
1,---,m
denoted
by
pn(z
ci/z
to
re
place
the
correspondent
average
value
since
2:‘11
py(z
cifz
1
we
use
the
following
normalized
values
as
the
new
estimations
pn(z
ci/z
pele
cfr
)
b
extensions
to
other
classifiers
the
previous
approach
could
be
extended
to
cover
several
cases
when
some
e;’s
belong
to
another
kind
of
classifiers
first
we
consider
the
case
that
ey
is
nn
classifier
in
this
case
the
classification
process
consists
of
two
steps
the
first
one
is
to
find
the
k
nearest
prototype
samples
to
the
present
input
with
knn
=zk
ki>0
=1
where
k
tepresents
the
number
of
prototype
samples
from
class
c
the
second
step
is
to
classify
into
class
c
according
to
k
max
k
that
is
e(z
j
®
since
the
measurements
ki
1
have
different
scale
from
the
measurements
in
the
form
of
post-probabilities
it
is
not
reasonable
to
use
5
directly
the
following
formula
introduces
one
way
to
transform
k;’s
into
the
approximations
as
when
k
max;ea
ki
pz
cife)=
i1
fon
©
and
then
these
approximations
could
be
put
into
5
for
the
subsequent
combination
computing
second
we
consider
the
case
is
some
kind
of
distance
classifier
i.e
for
each
z
classifies
according
to
some
distance
measures
e.g
euclidean
mahalanobis
and
other
pseudodistances
etc
di(i
between
and
the
centers
or
prototypes
of
each
class
ci
1
m
if
one
could
design
some
functions
pi(i
fild(i),i=1
m
10
for
example
1/d(3
yic
/(i
to
derive
set
of
pi(i)’s
which
obey
the
three
basic
axioms
of
probability
theory
one
could
use
these
pi(i
as
apparent
post-probabilities
and
put
them
into
4
for
combination
generally
any
classifiers
in
which
some
kind
of
apparent
post-probabilities
are
computable
could
be
combined
by
means
of
5
pid
1
iv
combining
multiple
classifiers
by
voting
principle
a
the
earlier
works
from
now
on
this
paper
will
concentrate
on
the
combination
problem
of
type
since
this
type
is
the
most
general
and
useful
one
as
indicated
in
section
i
the
problem
is
to
produce
new
event
e(z
from
the
given
events
ex(z
ji
1
k
where
the
following
equation
may
not
necessarily
hold
12
e1(e
ez(z
+
ex
ena
el@
{m+
1
otherwise
if
pg(z
cj/z
max;es
pe(z
cifz
422
ieee
transactions
on
systems
man
and
cybernetics
vol
22
no
3
may/jjune
1992
that
is
conflicts
may
exist
among
the
decisions
of
classi
fiers
simple
and
common
rule
used
for
resolving
this
kind
of
conflicts
in
human
social
life
is
voting
by
majority
this
ule
has
been
adapted
for
multiclassifier
combination
by
1
2
5
in
the
recognition
of
unconstrained
numerals
in
5
eleven
individual
classifiers
are
proposed
based
on
template
matching
structural
and
statistical
methods
respec
tively
if
six
out
of
11
vote
for
the
same
label
then
the
label
is
taken
as
the
final
result
in
2
two
classifiers
are
used
based
on
structural
features
extracted
from
skeleton
and
contour
respectively
three
rules
specific
for
the
combination
of
the
two
classifiers
are
proposed
these
rules
maybe
regarded
as
the
special
examples
of
the
majority
voting
rule
in
1
two
classifiers
are
added
to
2
among
the
four
each
of
the
last
three
outputs
only
one
label
i.e
e2(z
ja
e3(z
js
e4(x
j4
while
the
first
one
outputs
subset
of
labels
i.e
ey(x
with
#|j|
3
the
ex
2,3
represents
one
vote
that
is
assigned
to
its
output
label
ji
while
e
(
divides
one
vote
into
#|j|
fractions
with
each
label
in
receiving
1/(#]j|
vote
the
decision
is
made
such
that
the
label
that
receives
more
than
half
of
the
votes
i.e
two
is
taken
as
the
final
output
b
variants
of
voting
principle
and
general
expression
for
convenience
we
represent
the
event
ex(z
in
the
form
of
binary
characteristic
function
_j1
wheney(z)=iandi€a
tile
{0
otherwise
a3
the
most
conservative
voting
rule
is
the
following
if
an
ti(z
€cj
>0
e(=
+1
otherwise
a9
that
is
the
combined
classifier
decides
that
comes
from
c
iff
all
the
classifiers
decided
that
comes
from
c
simultaneously
otherwise
it
rejects
2
in
15
n
denotes
the
operator
of
logical
and
or
binary
multiplication
and
in
the
following
15
u
denotes
the
operator
of
logical
or
or
binary
summation
slight
modification
of
14
could
lead
to
version
that
is
less
conservative
the
version
is
shown
in
15
at
the
bottom
of
the
page
which
results
in
an
that
decides
c
as
long
as
some
classifiers
support
c
and
no
other
classifier
supports
different
cj
i
or
in
other
words
15
means
that
the
classifiers
that
reject
have
no
impact
on
the
the
majority
voting
rule
used
in
1
could
be
expressed
by
the
following
formula
e(z
{j
if
tr(z
c
maxiep
tp(z
c
+1
otherwise
16
where
te(zec)=y
tulzeci=1,-,m
7
k=1
by
slightly
modifying
this
formula
more
general
version
is
established
as
follows
e(z
if
te(z
c
maxiea
te(z
ci
ax
+1
otherwise
18
where
1
note
that
16
is
the
special
case
of
18
with
0.5
¢
and
is
arbitrarily
small
equation
12
is
equivalent
to
the
special
case
of
18
with
1.0
in
18
the
thresholding
operation
only
considers
that
the
maximal
votes
of
the
final
selected
label
must
be
large
enough
there
may
exist
cases
that
there
are
more
than
two
labels
that
receive
the
maximal
vote
or
the
vote
of
the
maximal
are
not
considerably
larger
than
the
vote
of
the
second
maximal
in
these
cases
even
the
maximal
vote
of
the
final
selected
label
may
be
quite
large
the
decision
still
may
not
be
reliable
since
there
exists
an
opponent
that
may
also
receive
large
vote
to
tackle
this
problem
new
majority
voting
rule
is
proposed
in
19
shown
at
the
bottom
of
the
page
and
20
max
maxiep
te(z
c
maxy
maxiea
te(z
ci
20
where
1
since
k
the
number
of
classifiers
is
constant
the
votes
of
max
could
be
regarded
as
the
implicit
objections
to
the
label
j
thus
rule
19
in
fact
requires
that
the
pure
supports
received
by
the
finally
selected
label
must
be
large
enough
it
is
not
difficult
to
see
that
rule
15
is
equivalent
to
the
special
case
of
18
with
max
0
all
the
aforementioned
variants
could
be
included
in
general
expression
as
ez
ifte(iigc]‘)=ma.x1
a
k+
dy(z
+1
otherwise
combined
unless
all
the
classifiers
reject
x
@1
13
if3j
a,nf
{t(z
c)u(1
um
ti(z
cp)}
ex
{m
+1
otherwise
15
_l3
if
tg(x
cj
max
and
max
maxg
a
e(z
{m
+1
otherwise
19
xu
et
al
methods
of
combining
multiple
classifiers
and
their
applications
to
handwriting
recognition
where
dy(x
is
function
of
te(z
c
=1
m
rule
17
is
special
case
of
21
with
d(z
and
rule
20
is
special
case
of
21
with
d¢(z
maxs
di(z
could
also
be
another
function
e.g
the
median
value
of
tg(z
c
1
m
observe
that
the
threshold
of
21
consists
of
two
parts
constant
part
that
is
independent
of
z
and
dynamic
one
that
varies
with
input
z
finally
we
should
also
point
out
that
at
the
beginning
of
the
subsection
although
we
only
let
tx(z
c
be
the
event
of
type
ex(z
4
i.e
each
classifier
outputs
single
label
for
the
general
case
z
with
being
subset
of
labels
e.g
expert
no
in
1
all
the
previous
equations
also
apply
by
defining
nonbinary
characteristic
function
for
the
event
ex(z
as
follows
tz
€ci
{w
when
@
0
otherwise
v
the
combination
of
multiple
classifiers
in
bayesian
formalism
a
confusion
matrix
prior
knowledge
and
beliefs
in
the
previous
section
those
voting
methods
that
combine
the
results
of
individual
classifiers
are
only
based
on
the
label
outputted
by
each
classifier
i.e
the
event
ex(x
j
each
of
ex(z
ji’s
is
equally
treated
as
one
vote
without
considering
the
error
of
each
ey
itself
this
and
the
next
section
will
take
these
errors
into
consideration
the
errors
of
each
classifier
are
usually
described
by
its
confusion
matrix
that
is
given
by
51
52
5;21
5(1)\“1
ar
pty
21
22
2m
2(m+1
23
0o
svn
m)z
sw)m
g\/{)(m-h
for
1,2
k
where
each
row
corresponds
to
class
c
and
each
column
corresponds
to
the
event
ex(z
j
thus
an
element
nf
denotes
that
n
samples
of
class
c
have
been
assigned
label
by
ex
the
confusion
matrix
pt}
of
trained
classifier
ex
could
be
obtained
by
using
ey
to
classify
test
sample
set
that
reflects
the
distribution
of
pattern
space
it
follows
from
23
that
the
total
number
of
samples
in
the
test
set
is
m1
ngb
=3
0w
29
i=1
j=1
in
which
the
number
of
samples
in
each
class
c
is
al
=1
25
and
the
number
of
samples
that
are
assigned
by
ey
is
n
=1,--,m+1
26
15
423
for
an
event
ex(z
of
an
error-bearing
classifier
ex
its
truth
i.e
does
come
from
class
c
has
uncertainty
with
the
knowledge
of
its
confusion
matrix
ptj
such
an
uncertainty
could
be
described
by
the
conditional
probabilities
that
propositions
cj
1
are
true
under
the
occurrence
of
the
event
ex(z
j
that
is
k
a®
p(z
cifex(z
j
a
n(k
=l
m
i=1""ij
27
from
another
viewpoint
the
confusion
matrix
pt}
could
be
regarded
as
the
prior
knowledge
of
an
expert
upon
receipt
of
the
evidence—the
occurrence
of
event
z
j
the
expert
expresses
his
beliefs
with
uncertainty
on
each
of
mutually
exclusive
propositions
cj
vi
by
real
numeral
bel
called
belief
value
the
higher
the
bel
he
gives
to
proposition
the
more
likely
it
is
true
with
the
knowledge
of
pty
he
expresses
his
bel(.)’s
on
each
proposition
c
in
the
form
of
conditional
probability
as
given
by
27
viz
bel(z
ci/er(z
en
p(z
ci/er(z
ji)i=1
m
28
that
is
bel
is
defined
as
the
probability
under
the
condition
of
ex(z
jx
and
the
environment
en
where
en
denotes
the
common
classification
environment
that
consists
of
any
evems
that
are
independent
of
any
of
events
ex(z
jk
k=
k
e.g
the
environment
at
least
contains
the
oocurrence
of
specific
input
pattern
such
belief
expression
given
in
28
is
exactly
that
used
by
pearl
24
as
well
as
others
who
adopt
bayesian
formalism
for
evidence
gathering
and
uncertainty
reasoning
in
al
literature
in
his
recent
book
24
pearl
described
that
in
this
formalism
propositions
are
given
numeral
parameters
signifying
the
degree
of
belief
accorded
to
them
under
some
body
of
knowledge
and
the
parameters
are
combined
and
manipulated
according
to
the
rules
of
probability
theory
the
advantages
of
adopting
bayesian
formalism
and
various
methods
for
manipulating
uncertainty
reasoning
along
the
formalism
have
been
studied
extensively
in
24
in
our
case
there
are
propositions
cj
vi
a
the
numeral
parameters
are
the
conditional
probabilities
given
by
27
and
the
body
of
knowledge
consists
of
event
ex(z
matrix
pt
as
well
as
the
environment
en
b
belief
integration
based
on
bayesian
formula
with
classifiers
e;,---,ex
we
will
have
matrices
pt
ptx
when
these
classifiers
are
used
on
the
same
input
z
events
ex(x
ji
1
will
happen
as
discussed
previously
each
ex(z
jx
and
its
corresponding
pt
could
supply
set
of
bel(z
ci/ex(z),en
1
m
each
of
which
supports
one
of
the
propositions
natural
question
is
how
to
integrate
these
individual
supports
424
to
give
the
combined
values
in
29
shown
at
the
bottom
of
the
page
from
28
and
29
we
have
30
shown
at
the
bottom
of
the
page
if
classifiers
ej,---,ex
perform
independent
of
each
other
e.g
we
can
consider
that
classifiers
are
independent
when
they
use
independent
feature
sets
or
they
are
trained
by
independent
training
sets
then
the
events
e;(z
j1,+-+,ex(z
jx
will
be
independent
of
each
other
either
under
the
condition
of
c
as
well
as
en
or
the
condition
of
solely
en
thus
we
have
plei(z
ji
-
ex(2
jx/z
ci
en
p(es(z
j1
-
ex(z
jx/en
i
plen(s
ju/z
g
en
i
plex(z
ju/en
15
p(z
ci/ex(z
ji
p(zeci/en
since
p(ex(x
jx/z
ci
en
p(z
cifex(x
jk
p(ex(z
jk/en
~~
p(z
gi/en
by
putting
the
previous
into
30
we
have
iy
cife(@
i
bel(i
p(z
j,/e‘n)———-——f=1
pa
ci/en
@31
where
p(z
c;/ex(x
ji
could
be
estimated
by
27
with
being
replaced
by
ji
p(z
c;/en
represents
the
probability
that
c
is
true
under
occurrence
of
and
the
common
environment
en
it
should
be
noticed
that
the
occurrence
of
is
necessary
condition
for
events
ex
(
ji
1,--,k
and
thus
should
be
placed
behind
all
the
condition
bars
/
in
27)~(29
and
30
and
39
better
estimation
of
p(x
c;/en
should
be
the
postprobabilities
p(x
c;/z
this
means
that
31
provides
an
alternative
way
to
solve
the
combination
problem
of
type
3
which
we
have
studied
in
section
iii
the
alternative
way
is
not
intended
to
be
further
studied
in
this
paper
for
the
purpose
of
this
paper
i.e
the
combination
problem
of
type
1
the
postprobabilities
p(z
c;/x)’s
are
not
available
for
practical
implementation
we
use
the
following
32
as
an
approximation
of
31
bel(s
[
p(z
cifer(x
jr
k=1
32
1eee
transactions
on
systems
man
and
cybernetics
vol
22
no
3
may/june
1992
with
as
constant
that
ensures
that
2?;1
bel(s
since
cj
1,2
are
mutually
exclusive
and
exhaustive
that
is
we
have
mk
0=
px
ci/ex(z
jv
33
i=1
k=1
finally
depending
on
these
bel(z
values
we
can
classify
into
class
according
to
the
decision
rule
given
here
15
bla
m+1
in
making
the
trade-off
between
the
substitution
rate
and
the
rejection
rate
34
could
be
modified
into
35
b(z
{%4+1
where
is
threshold
if
bel(j
maxiea
bel(i
otherwise
b9
if
bel(j
max;ea
bel(i
a
otherwise
@9
vi
the
combination
of
multiple
classifiers
in
dempster-shafer
formalism
in
this
section
we
study
the
combination
problem
of
type
to
consider
the
errors
of
individual
classifiers
by
adapting
dempster—shafer’s
evidence
theory
the
combination
is
made
in
the
situation
that
only
the
recognition
substitution
and
rejection
rates
of
each
individual
classifier
are
used
as
the
prior
knowledge
these
rates
which
usually
represent
the
performance
indexes
of
classifier
are
easily
obtained
by
testing
the
classifiers
with
test
sample
set
a
dempster-shafer
theory
for
convenience
we
first
briefly
introduce
the
key
points
of
dempster—shafer
theory
given
number
of
exhaustive
and
mutually
exclusive
propositions
a
1
m
which
form
universal
set
={a1
-
am}
asubset
{a
-
4
represents
proposition
denoting
the
disjunction
a
-
4
each
clement
a
corresponds
to
one-element
subset
{4;}
called
singleton
all
the
possible
subsets
of
form
superset
29
ie
each
subset
is
an
element
of
2°
i.e
2°
the
d-s
theory
uses
numeric
value
in
the
range
0
1
inclusive
to
indicate
belief
in
proposition
subset
based
on
the
occurrence
of
an
evidence
e
this
value
conventionally
denoted
by
bel(a
indicates
the
degree
to
which
the
evidence
supports
the
proposition
a
the
value
of
bel(a
is
calculated
from
another
function
called
basic
probability
assignment
bpa
which
represents
the
individual
impact
of
each
evidence
on
the
subsets
of
©
bpa
denoted
m
is
generalization
of
probability
mass
distribution
it
bel(i
bellz
c;/er(z),---,ex(z
en
plz
ci/es(z
j1
+
ex(z
jx
en],i
1
p(ei(z
=41
-
ex(x
jx/z
c
en)p(z
c;/en
29
bel(i
p(z
ci/ei(x
j1
-
ex(z
jx
en
30
p(ei(z
j1
-
ex(z
jx/en
xu
er
al
methods
of
combining
multiple
classifiers
and
their
applications
to
handwriting
recognition
425
assigns
values
in
0
1
to
every
element
of
2©
i.e
each
subset
of
©
instead
of
each
element
of
as
in
probability
theory
such
that
the
numeric
values
sum
up
to
1
usually
m(a
is
used
to
denote
the
value
assigned
to
subset
a
there
are
three
distinct
features
about
bpa
1
m(a
is
the
portion
of
the
total
belief
committed
exactly
to
a
which
cannot
be
further
subdivided
among
the
subsets
of
and
does
not
include
the
portions
of
the
total
belief
committed
to
subsets
of
a
2
the
singletons
{a;}
vi
are
only
parts
of
the
elements
in
2°
so
it
is
possibly
377
m(a
1
and
since
a
and
~a
a
are
only
two
elements
of
28
possibly
m(a4
m(=a
1
this
avoids
the
basic
axioms
of
bayesian
formalism
or
in
other
words
bpa
supplies
an
incomplete
probabilistic
model
3
subset
2°
with
m(4
is
called
focal
element
when
is
the
only
focal
element
in
29
we
have
m(©
m(a
ie
m(©
absorbs
the
unassigned
portions
of
the
total
belief
after
commitment
of
belief
to
various
proper
subsets
of
©
since
subset
represents
the
disjunction
of
all
the
elements
in
a
the
truth
of
implies
the
truth
of
a
i.e
all
the
portions
committed
exactly
to
every
subset
of
will
also
support
a
hence
bei(a
is
given
by
bel(4
m(b
bca
36
with
the
special
cases
1
when
a4
is
singleton
then
bel(a
bel(4
m(a
2
when
o
bel(©
1
when
two
or
more
evidences
exist
there
will
be
two
or
more
sets
of
bpa’s
and
bel(.)’s
given
to
the
subsets
of
the
same
©
under
the
condition
that
each
of
these
evidences
is
independent
of
each
other
dempster’s
combination
rule
could
be
used
to
combine
them
into
new
bpa
and
bel
which
represents
the
combined
impact
of
the
two
evidences
let
bel
bel
and
my
ma
denote
two
belief
functions
and
the
corresponding
bpa’s
respectively
the
dempster
rule
defines
new
bpa
m
@&
my
which
represents
the
combined
effect
of
m
and
mo
ie
for
where
denotes
the
empty
set
ma
=midmy(d
=k
m(x)ma(y
xny=a4,a#0
el=1
mixme(¥)=
mi(x)ma(¥
xny=@
xoy#d
@7
where
©
are
any
subsets
the
is
bpa
if
k™
0
if
k=
0
then
™
my
does
not
exist
and
m
my
are
said
to
be
totally
contradictory
i.e
the
two
evidences
are
in
conflict
if
the
two
evidences
are
not
in
conflict
the
combined
belief
function
denoted
by
bel
bel
may
be
computed
from
m
my
directly
by
36
because
dempster
rule
is
associative
and
commutative
it
could
be
used
to
combine
multiple
evidences
by
sequentially
and
with
an
arbitrary
order
using
37
to
obtain
m=mydme
smg
38
obviously
the
combination
exists
when
any
of
two
among
the
evidences
are
not
in
conflict
moreover
the
values
of
could
also
be
calculated
by
the
following
formula
m@a)=k
j]mx
xin--nxg=ai=1
imx
xameenx
=0
i=1
i‘[m,-(x
xin-nxk#2i=1
kfl=1
39
after
obtaining
m
we
can
calculate
its
correspondent
bel
by
36
b
modeling
multiclassifier
combination
by
ds
theory
in
our
problem
the
exhaustive
and
mutually
exclusive
propositions
are
given
by
a
cj
vi
a
which
respectively
denote
that
input
sample
comes
from
ci
vi
a
and
the
universal
proposition
is
{a
+
an}
when
applied
to
the
same
input
z
classifiers
e
-
ex
will
produce
evidences
ex(z
ji
1,2
with
each
ex(z
jip
denoting
that
is
assigned
label
jk
au{m
1}
by
classifier
ej
given
that
e$’°
55
are
respectively
the
recognition
rate
and
the
substitution
rate
of
e
usually
4p
<1
due
to
the
rejection
action
for
each
ei(z
ji
when
jjik
a
one
could
have
uncertain
beliefs
that
the
proposition
a
=z
cj
is
true
with
degree
®
and
is
not
true
with
degree
%
when
je
+1
ie
is
rejected
by
ex
one
has
no
ideas
about
anyone
of
the
propositions
a
=z
c
vi
a
which
could
be
regarded
as
the
full
support
of
the
universal
proposition
©
we
can
define
bpa
function
m
on
for
evidence
e(x
ji
in
the
following
way
1
when
jr
+1
my
has
only
focal
element
with
m(©
1
since
says
nothing
about
anyone
of
the
propositions
this
is
degenerated
case
2
when
ji
a
m
has
only
two
focal
elements
4
and
=a
{4;,}
with
mg(4
and
mi(—aj
since
e
only
gives
a
and
4
the
support
of
degrees
e8p
respectively
moreover
ey
says
nothing
about
any
other
propositions
so
we
have
mi(8
=1
e
p
as
result
with
the
existence
of
all
the
evidences
ex(z
k=1,---,k
we
will
have
bap’s
my
1
k
our
problem
is
to
use
the
dempster
rule
to
obtain
combined
bpam
=m
®my®
mg
and
to
use
this
new
bpa
to
calculate
bel(a4
and
bel(—a
for
vi
based
on
all
the
426
evidences
hereafter
we
could
form
the
combined
classifier
by
using
decision
rules
derived
from
these
beliefs
before
discussing
how
to
compute
effectively
the
combined
in
the
next
subsection
we
make
two
preparations
first
we
discard
those
evidences
ex(z
jx
with
jx
since
it
follows
from
37
and
39
that
bpa
as
mi(©
has
no
influence
on
the
result
of
the
combination
after
discarding
such
evidences
assume
that
there
are
k
evidences
ex(z
1
k
with
each
jx
a
on
the
k
evidences
without
losing
generality
we
could
further
rule
out
the
following
three
special
cases
1
k
0
it
means
that
all
of
classifiers
reject
the
input
sample
z
obviously
the
final
decision
is
simple
to
reject
too
there
is
one
classifier
ex
which
has
the
recognition
rate
s(rk
1
this
means
that
e
itself
can
classify
any
input
sample
with
absolute
correctness
thus
all
the
other
classifications
are
no
longer
necessary
there
is
one
classifier
e
which
has
the
substitution
rate
esk
1
this
means
that
e
always
makes
wrong
de
cisions
i.e
the
classifier
is
generally
not
much
useful
we
will
not
use
such
classifiers
here
as
result
we
can
concentrate
on
the
general
case
that
there
are
k
evidences
ex(z
ji
1,---,k
with
o<
<0
<1
second
we
show
that
in
the
general
case
previously
the
combination
does
exist
i.e
my,---,mg
are
not
in
conflict
to
show
this
it
is
sufficient
to
show
k=
0
it
follows
from
39
that
we
only
need
to
show
that
there
is
a2
combination
x1
xo
-
with
mi(x1)ma(xp
mi:(xgr
0
for
1,2
k
since
mx(4
e®
1
we
know
that
for
at
least
one
of
mi(—a4j
0
mi(©
should
be
true
let
x1
aj,andfork
=2
k
let
xy
a
if
a
aj
otherwise
let
be
one
of
~aj
such
that
m(xy
0
as
result
we
have
x
xn+
xg
a
with
my(x1)ma(xs
mg
xk
0
therefore
we
proved
k7
0
2
3
c
an
effective
combination
scheme
for
the
computation
of
m
me
+
mk
the
direct
use
of
both
37
and
39
yields
the
computation
cost
that
increases
exponentially
with
m
usually
it
may
be
computationally
prohibitive
especially
when
we
have
large
number
of
classifiers
however
for
our
problem
because
of
special
feature
that
the
bpa
of
each
evidence
only
has
two
focal
elements
one
is
singleton
and
the
other
is
negation
of
this
singleton
we
can
arrive
at
computing
method
with
computation
cost
o(m
the
method
consists
of
two
main
steps
described
in
the
following
1except
the
special
case
of
two
class
problem
in
this
case
the
complement
of
the
decision
of
such
classifier
is
always
right
then
we
reach
the
same
situation
as
2
by
exchanging
the
roles
of
€
e
however
for
problem
involving
more
than
two
classes
the
complement
decision
of
such
classifier
does
not
supply
much
useful
information
eee
transactions
on
systems
man
and
cybernetics
vol
22
no
3
may/iune
1992
in
the
first
step
we
collect
the
evidences
into
groups
with
those
impacting
the
same
proposition
in
each
group
and
then
combine
the
evidences
in
each
group
respectively
for
all
the
evidences
ex(z
jk
1
k
suppose
that
among
jy
+
jx
there
are
ky
min(m
k
different
values
jj
jf
thus
all
the
k
evidences
are
collected
into
k
groups
ei,e»
-
ek
with
each
e(x
ji
being
put
into
group
ej
if
ex(x
jk
j}
for
each
group
ej
since
all
its
evidences
e
(
ji
-
e
2
ji
impact
on
the
same
propositions
and
~aj
we
can
recursively
use
37
to
make
combined
bpa
mg
from
bpa’s
my
-
mk
which
are
provided
by
ek
-
ex
i€
me
me
@mi
+
mg
omy
|ome
|
jemy
40
or
my
mk
mk
m3
=my
my
"
me
=my
mp_1
myp
for
example
mi
a
mi
245
mie
2
az
e
ajy
r
k
my(0
kamy
©)my,(0
ka1
1
ma(~aj
kama
045
mry(©
mi
=45
ket
2
)
©
kaelk)(1
elk2
kpelka)(1
elkr
k
ma
az}
kama
az)lma
©
mu
4
ko
az
i
©
ka1
kel
1
ma(a
=0
for
all
other
©
41
it
is
important
to
notice
that
the
new
bpa
mg
has
also
only
the
same
two
focal
elements
as
before
consequently
it
follows
from
40
that
mg
m,_1
me
could
be
calculated
recursively
in
the
same
way
for
3,---,p
1,p
the
general
recursive
formula
is
given
by
ko
m.(©
kymy_1(©)my
8
mp(~4j
kemy_1(=
az
mi
0
my
~a4
ke
aj
jme—1(8
me(aj
keme-1(aj
my
0
mu
4j
krmi
a
jme_1(©
m,(a
=0
for
all
other
©
ky
as
result
we
obtain
the
new
combined
bpa
mg
myp
which
again
has
only
the
same
two
focal
elements
as
before
xu
et
al
methods
of
combining
multiple
classifiers
and
their
applications
to
handwriting
recognition
therefore
after
the
combination
ej
is
equivalent
to
new
classifier
that
produces
an
event
ex(z
j
with
new
recognition
rate
mpg,(a
and
the
substitution
rate
me,(~4
in
summary
the
first
step
has
converted
the
k
classifiers
ey
ek
and
their
correspondent
evidences
ex(z
jg
k=1,---,k'into
the
k
min(k
m
combined
classi
fiers
by
-
ex
with
their
correspondent
evidences
ex(z
jko
k=1
ky
and
jy
ik
the
second
step
is
to
further
combine
the
bpa’s
mg
k=1,---,k
into
final
combined
bpa
m=mg
@mp
me
43
and
then
to
calculate
the
correspondent
bel(a
bel(~a
for
vi
in
this
case
since
any
two
mg
mg
will
affect
different
focal
elements
the
combined
mg
mg
will
affect
five
focal
clements
instead
of
two
focal
elements
as
each
mg
mg
did
furthermore
the
number
of
focal
elements
of
me
ome
mg
@me
®me
-
me
ome
®mme
will
increase
rapidly
as
more
bpa’s
are
combined
in
such
situation
the
direct
use
of
37
is
computationally
quite
expensive
fortunately
we
have
{355
dk
a=
{1,2
m}
when
ky
m
ie
{j],j5,-,j%,}
is
just
permutation
of
{1,2,---,m}
thus
these
ex
1,---,k
are
just
the
same
as
one
of
the
intermediate
products
in
paper
26
called
simple
evidence
functions
moreover
even
when
k
m
we
can
append
k
functionless
evidences
ej
k=k
+1,---,m
with
mg,(©
1
mek(a]k
=0
me
a
=0fork=ki+1,---,mand
j{
jo
#
jie
jke1
dhy
again
{355
jh
als
permutation
of
{1,2
m}
and
then
e
1
/m
become
the
so
called
simple
evidence
functions
too
on
the
other
hand
it
is
easy
to
see
that
for
any
m
we
have
m
m
mg
k
1
thus
mg
®me
ompy
®meg
omey
=mmg
ome
me
=m
that
is
the
appended
ex
k
1,---,m
have
no
influences
on
the
final
combined
m
so
with
these
facts
427
we
can
directly
borrow
those
formulae
developed
in
26
for
combining
the
simple
evidence
functions
to
serve
our
purpose
from
the
formulae
6)~(8
provided
in
paper
26
by
noticing
that
in
our
case
mg,(8
1
mg,(4
0
me‘;,("a
=0
for
k
+1
m
we
obtain
after
some
derivation
the
following
formulae
for
computing
the
final
combined
beliefs
for
proposition
aj
=aj
=1,2,--+
m
ky
a=
zl
b=
h[l
mp,(4
k=1
ky
=
me.(-4
k=1
me
ajk
mu(4j
44
1+a4)b-0c
1+
a)b
ifk;=m
<m
5
and
46
and
47
shown
at
the
bottom
of
the
page
in
the
formulae
we
have
factors
1/(1
me“(aj"f
1
k1
which
are
not
meaningless
under
the
condition
that
mek(a]-l
<1
k=
1,---,k
for
each
k
recall
40
that
mg
is
calculated
by
mg
my
@---@my
and
that
in
our
problem
mi
4
e
<1
mi,(~4
f
<1
i=1,--,p
it
follows
from
39
that
mg
4j
1
so
we
see
that
all
the
previous
equations
are
always
meanmgful
now
let’s
examine
the
computational
complexity
of
the
whole
procedure
first
the
complexity
for
a
b
is
respec
tively
at
most
of
order
o(m
thus
the
computation
spent
on
all
the
equations
of
44)~(47
is
also
of
order
o(m
second
in
40
and
41
all
the
computations
are
constant
with
respect
to
m
so
the
total
computation
of
the
whole
procedure
is
of
order
o(m
d
decision
rules
recall
that
{j{,j5---,jj}
is
just
permutation
of
{1,2
m}
we
see
that
belief
values
bel(4
bel(~a
1,2,---,m
are
all
given
by
44)(47
with
these
me
4,0
s
6
bel(4
l-meka
+——{‘—75-.a
fkij=morky=m-1&
=m
mg
a
1
tj‘,i
in
all
the
other
cases
mg
a
mp
a
ka~
t=mz
(
b-c
itki=m
a
mp
~ay
bel(~4y
k(4
7
<1_‘,,fzh
j
b
it
k
m&k
k
it
e,,(a
in
all
the
other
cases
428
values
we
can
finally
define
the
combined
classifier
by
the
following
rules
pe=
{3
in
making
the
trade-off
between
the
substitution
rate
and
the
rejection
rate
48
could
be
modified
into
49
bz
{i,“—l
if
bel(4
max;ea
bel(4
otherwise
where
is
threshold
the
previous
rules
didn’t
take
into
consideration
the
beliefs
bel(—a;)’s
which
also
contain
useful
information
for
the
final
decision
the
following
rules
are
proposed
in
order
to
include
this
information
1
the
first
rule
_id
(
m+1
where
and
di
bel(4
bel(-4
1,--+
reflects
the
pure
total
support
by
the
proposition
a4
2
the
rule
tries
to
pursue
the
highest
recognition
rate
under
the
constraint
of
bounded
substitution
rate
equation
51
is
shown
at
the
bottom
of
the
page
3
in
52
which
is
shown
at
the
bottom
of
the
page
where
@3
ag
are
predefined
thresholds
if
bel(a4
max;es
bel(a
otherwise
8
49
if
d
max;en
d
otherwise
0
vil
applications
of
the
combination
approaches
to
the
recognition
of
totally
unconstrained
handwritten
numerals
a
individual
classifiers
and
database
the
four
classifiers
proposed
in
1
where
they
are
called
four
experts
are
used
here
to
show
the
significant
benefits
obtained
by
using
the
combination
approaches
proposed
in
the
earlier
sections
of
this
paper
as
in
1
the
four
classifiers
are
named
expert#1
expert#2
expert#3
and
expert#4
and
are
denoted
by
ey
e
e3
and
e4
the
first
three
are
based
on
the
features
extracted
from
the
skeletons
while
ey
is
based
on
the
features
derived
from
contours
see
1
for
more
details
the
data
used
here
come
from
the
u.s
zipcode
database
of
the
concordia
ocr
research
team
this
database
contains
17
140
run-length
coded
binarized
digits
the
samples
were
originally
collected
from
the
dead
letter
envelopes
by
the
u.s
postal
services
at
different
locations
after
some
pre
processings
see
1
for
details
4000
samples
400
10
digits
i.e
each
of
the
10
numerals
has
400
samples
were
used
for
ieee
transactions
on
systems
man
and
cybernetics
vol
22
no
3
may/june
1992
table
the
results
of
four
experts
recogn
substi
reject
reliab
er
86.05%
225%
11.70%
97.45%
ez
93.10%
295%
3.95%
96.98%
ez
995%
215%
4.90%
97.74%
ey
93.90%
1.60%
4.50%
98.32%
training
the
four
experts
and
then
new
set
of
2000
samples
200
10
digits
was
used
for
testing
them
the
following
results
are
obtained
from
the
testing
set
in
table
i
recogn
substi
reject
and
reliab
are
abbre
viations
of
recognition
substitution
rejection
and
reliability
rates
respectively
the
reliability
rate
is
defined
by
recognition
100%
rejection
in
addition
if
e
assigns
subset
of
labels
to
an
input
the
input
is
regarded
as
being
rejected
in
table
1
moreover
in
the
following
this
nonunique
recognition
of
e
is
also
always
regarded
as
the
rejection
except
for
some
cases
specifically
indicated
in
the
following
three
subsections
we
will
show
the
results
of
experiments
by
the
combination
approaches
proposed
in
sections
iv-vi
then
in
the
last
subsection
we
will
make
some
comparisons
with
the
three
approaches
among
e
€2
€3
€4
only
could
supply
the
output
information
at
the
measurement
level
hence
we
could
only
consider
the
combination
problem
of
type
1
thus
we
have
not
conducted
any
experiments
by
using
the
averaged
bayes
classifier
proposed
in
section
iii
reliability
53
b
experiments
by
the
approach
based
on
dempster—shafer
formalism
extensive
experiments
have
been
carried
out
to
test
the
performance
of
the
approach
proposed
in
section
vl
these
experiments
could
be
divided
into
two
groups
in
the
first
group
we
use
the
recognition
substitution
and
rejection
rates
provided
in
table
as
prior
knowledge
and
use
the
approach
for
combining
the
results
of
the
four
experts
on
all
the
2000
samples
of
the
test
set
given
in
table
1
but
in
the
second
group
we
divide
the
2000
samples
into
two
sets
the
first
1000
samples
are
used
to
test
e
1,2
3,4
in
order
to
obtain
the
estimations
of
the
recognition
substitution
and
rejection
rates
i.e
these
1000
samples
are
used
for
estimating
these
rates
then
the
four
classifiers
are
tested
by
the
remaining
1000
samples
and
combined
by
the
approach
given
in
section
vi
using
the
rates
learned
from
the
first
1000
samples
in
each
group
number
of
experiments
have
been
conducted
using
if
bel{
a
max;ca
{bel(4;)/vi
bel(=a4
a}
blx
+1
otherwise
6d
if
bel(a
max;ea
{bel(4;)/vi
bel{
a
ai
bel(~a4
az}
bla
{m
+1
otherwisje
©2
xu
et
al
methods
of
combining
multiple
classifiers
and
their
applications
to
handwriting
recognition
table
11
results
of
trade-off
as
increases
recogn
substi
reject
reliab
000
98.95%
0.85%
0.20%
99.15%
0.90
97.60%
0.30%
2.10%
99.69%
095
95.85%
0.05%
4.10%
99.95%
099
93.95%
0.00%
6.05%
100.0%
es
93.90%
1.60%
4.50%
98.32%
table
results
of
trade-off
as
increases
according
to
rule
52
recogn
substi
reject
reliab
0.00
98.80%
0.80%
0.40%
99.20%
0.90
96.45%
0.25%
3.30%
99.74%
091
96.15%
0.10%
3.75%
99.90%
092
96.15%
0.10%
3.75%
99.90%
093
95.85%
0.05%
4.10%
99.95%
095
95.45%
0.05%
4.50%
99.95%
0.99
91.80%
0.00%
8.20%
100.00%
eq
93.90%
1.60%
4.50%
98.32%
the
different
decision
rules
given
by
48)(52
with
different
values
of
thresholds
to
provide
different
outcomes
to
allow
trade-offs
between
the
substitution
and
rejection
rates
experiments
of
the
first
group
1
results
from
decision
rule
48
the
results
of
applying
48
to
process
the
combined
beliefs
given
by
44)—(47
are
recogn
98.95%
substi
0.85%
reject
0.20%
relab
99.15%
2
results
from
decision
rule
49
by
using
49
to
replace
48
we
can
determine
the
trade-offs
between
high
recognition
rate
and
high
reliability
rate
the
following
table
shows
some
profiles
about
the
trade-offs
as
increases
in
the
table
ii
the
last
row
is
the
performance
of
e4
alone
refer
to
table
i
we
see
that
eq
gives
the
best
performance
among
the
four
individual
classifiers
here
and
later
we
list
this
performance
again
to
show
the
improvements
obtained
by
the
combined
classifier
e
as
shown
in
the
table
the
combined
is
absolutely
superior
to
e4
and
thus
to
all
other
individual
classifiers
in
all
aspects
the
most
interesting
thing
is
that
could
keep
high
reliability
and
high
recognition
simultaneously
results
from
decision
rule
52
this
rule
produced
the
outcomes
similar
to
those
by
51
the
difference
is
that
it
uses
the
threshold
to
control
the
pure
support
bel(a
bel(—a
for
adjusting
the
trade-offs
between
the
recognition
rate
and
the
reliability
rate
table
iii
is
the
counterpart
of
table
ii
3
experiments
with
the
second
group
as
indicated
earlier
in
this
section
the
experiments
of
this
group
use
the
first
1000
samples
as
training
set
to
obtain
the
initial
recognition
substitution
and
rejection
rates
429
table
iv
results
of
individual
classifiers
on
the
first
1000
samples
recogn
substi
reject
reliab
er
87.0%
1.5%
11.5%
98.31%
ey
94.4%
2.4%
32%
97.52%
e3
95.0%
1.2%
3.8%
99.79%
948%
0.9%
4.3%
99.06%
table
results
of
individual
classifiers
on
the
last
1000
samples
recogn
substi
reject
reliab
851%
3.0%
11.9%
96.59%
ez
91.8%
3.5%
47%
96.33%
ez
90.9%
3.1%
6.0%
96.70%
ey
93.0%
2.3%
4.7%
97.59%
table
vi
outcome
of
by
rule
48
on
the
last
1000
samples
ilo
rej
0
98
100
20
3k
9%
4
97
100
6
0o
70
98
100
9
97
recogn
98.6%
substi
1.2%
reject
0.2%
relab
98.8%
and
then
use
the
last
1000
samples
as
the
testing
set
to
check
the
performance
thus
for
convenience
of
comparing
the
combined
classifier
with
each
individual
classifier
the
following
tables
iv
and
are
given
to
show
the
performances
of
individual
classifiers
on
the
first
1000
samples
and
the
last
1000
samples
respectively
by
comparing
tables
iv
and
v
one
could
see
that
the
performances
of
e
1,2,3,4
on
the
first
1000
samples
are
better
than
those
on
the
last
1000
samples
i.e
we
selected
the
difficult
half
of
the
original
data
set
for
testing
our
combination
approach
in
the
following
we
use
the
rates
given
in
table
v
as
the
prior
knowledge
for
the
combination
test
on
the
last
1000
samples
1
results
from
decision
rule
48
the
results
of
applying
48
to
process
the
combined
beliefs
given
by
44)48
are
shown
in
the
following
confusion
matrix
results
from
decision
rule
49
table
vii
lists
the
results
by
using
49
on
the
last
1000
samples
for
different
values
of
c
table
viii
also
clearly
shows
the
significant
improvements
of
over
e4
as
an
example
we
also
present
the
confusion
matrix
in
table
viii
of
the
combined
with
0.96
as
follows
to
give
some
classification
details
results
from
decision
rule
49
table
ix
corresponds
to
table
iv
2
3
c
experiments
by
the
approach
based
on
bayesian
formalism
several
experiments
were
conducted
to
verify
the
approach
430
table
vi
results
of
trade-offs
as
r
increases
using
the
last
1000
samples
recogn
substi
reject
reliab
0.00
98.6%
12%
0.2%
98.80%
0.90
97.0%
0.4%
2.6%
99.59%
0.92
97.0%
0.4%
2.6%
99.59%
0.94
96.9%
0.4%
2.7%
99.59%
0.96
95.0%
0.0%
5.0%
100.0%
eq
93.0%
2.3%
4.7%
97.59%
table
viii
outcome
of
by
rule
49
with
0.96
using
the
last
1000
samples
ilo
rej
0
94
1
(
97
30
92
o0
92
j
97
6
97
70
93
98
9
91
recogn
95.0%
11
substi
0.0%
reject
5.0%
relab
100.0%
proposed
in
section
v
they
consist
of
three
groups
in
the
first
group
we
used
the
confusion
matrices
of
e
1,2,3,4
on
all
the
2000
testing
samples
as
the
prior
knowledge
and
then
combined
the
results
of
the
four
classifiers
on
the
same
2000
samples
1
experiments
with
the
first
group
the
following
table
shows
the
results
of
the
combinations
obtained
from
34
and
35
for
specified
range
of
values
again
from
the
list
we
see
that
the
combined
is
significantly
better
than
any
of
the
individual
classifiers
experiments
with
the
second
group
in
the
second
group
we
use
the
confusion
matrices
of
e
1,2,3,4
on
the
first
1000
samples
as
the
prior
knowledge
and
then
combine
the
results
of
the
four
classifiers
on
the
last
1000
samples
i.e
the
first
1000
samples
constituted
the
learning
set
and
the
last
1000
samples
the
testing
set
table
xi
is
the
counterpart
of
table
x
in
this
table
the
recognition
rate
of
the
combined
is
lower
than
that
of
e4
and
it
seems
that
the
combined
result
is
even
worse
than
the
individual
classifier
e
however
the
recognition
rate
is
not
the
only
index
for
evaluating
the
performance
of
classification
in
table
xi
the
substitution
rate
is
significantly
reduced
and
the
reliability
is
significantly
increased
thus
from
this
point
of
view
we
see
that
the
performance
is
still
improved
considerably
furthermore
we
can
also
observe
that
as
increases
the
combined
recognition
rate
and
substitution
rate
de
crease
while
the
reliability
increases
thus
there
is
trade-off
when
we
choose
the
value
of
o
in
practice
we
may
have
two
ways
to
do
this
one
is
to
previously
set
an
upper
bound
for
the
substitution
rate
we
adjust
such
that
the
resulting
substitution
rate
is
below
the
2
ieee
transactions
on
systems
man
and
cybernetics
vol
22
no
3
may/june
1992
table
1x
results
of
trade-offs
as
increases
with
rule
49
on
the
last
1000
samples
recogn
substi
reject
reliab
000
986%
11%
03%
98.80%
090
96.1%
03%
36%
99.69%
091
961%
03%
3.6%
99.69%
092
957%
03%
40%
99.69%
093
955%
03%
42%
99.69%
094
950%
00%
50%
100.0%
es
930%
23%
47%
9159%
table
results
of
trade-off
as
increases
recogn
0.00000
99.20%
99.20%
0.90000
98.85%
0.65%
99.50%
0.99000
98.35%
1.45%
99.80%
0.99900
97.75%
2.10%
99.85%
099990
96.35%
3.6%
99.95%
0.99999
94.05%
5.95%
100.00%
o4t
93.90%
4.50%
98.32%
table
xi
results
of
trade-offs
as
increases
on
the
last
1000
samples
recogn
substi
reject
reliab
0.00000
92.3%
0.9%
6.8%
99.03%
0.90000
92.2%
0.7%
7.1%
99.25%
0.99000
91.8%
0.6%
7.6%
99.35%
0.99900
91.7%
0.4%
7.9%
99.57%
0.99990
91.5%
0.3%
8.2%
99.67%
0.99999
91.0%
0.3%
8.7%
99.67%
eq
93.0%
23%
4.7%
97.59%
bound
and
the
recognition
rate
is
as
high
as
possible
the
other
way
is
to
adjust
such
that
the
substitution
rate
is
reduced
as
much
as
possible
until
it
can’t
significantly
reduce
any
more
the
similar
phenomena
can
also
be
observed
from
the
results
of
tables
xii-xvi
the
similar
trade-offs
also
apply
in
fact
this
kind
of
trade-off
applies
also
to
the
results
given
in
the
previous
section
vii-b
and
section
vii-b
experiments
with
the
third
group
in
the
third
group
each
time
we
leave
out
10
samples
one
for
each
digit
as
testing
set
and
use
the
1990
samples
as
the
training
set
to
learn
from
the
confusion
matrices
of
e
1,2,3,4
the
same
process
is
repeated
200
times
until
each
of
the
2000
samples
has
been
taken
as
testing
sample
once
table
xii
is
the
counterpart
of
table
x
it
shows
the
advantage
of
the
combined
over
individual
classifiers
3
d
experiments
by
the
approach
based
on
voting
principle
the
experiments
in
this
subsection
consist
of
two
groups
one
includes
the
results
obtained
from
testing
set
of
all
the
2000
samples
the
other
includes
the
results
on
testing
set
of
the
last
1000
samples
in
fact
the
combination
approach
based
on
voting
principle
does
not
need
the
learning
procedure
for
xu
et
al
methods
of
combining
multiple
classifiers
and
their
applications
to
handwriting
recognition
table
xii
results
of
trade-offs
as
increases
recogn
substi
reject
reliab
0.00000
96.55%
1.45%
2.00%
98.52%
090000
96.25%
1.10%
2.65%
98.87%
099000
95.80%
0.90%
3.30%
99.07%
0.99900
0.99900
0.70%
3.95%
99.27%
099990
94.15%
0.60%
525%
99.37%
099999
9230%
0.60%
7.10%
99.35%
eq
93.90%
1.60%
4.50%
98.32%
table
xiii
the
results
of
voting
rule
18
for
different
thresholds
recog
subst
reject
reliab
00<a<025
9890%
0.90%
0.20%
99.10%
025
a<0.50
9795%
035%
1.70%
99.64%
050
<0.75
9290%
0.00%
7.10%
100.00%
eq
93.90%
1.60%
4.50%
98.32%
table
xiv
results
of
voting
rule
19
for
different
thresholds
recog
subst
reli
a=0.0
98.90%
0.90%
0.20%
00<a<025
9860%
050%
090%
99.50%
025<a<0.50
9545%
0.05%
4.50%
99.95%
0.50
<0.75
88.60%
0.00%
11.4%
100.0%
eq
93.90%
1.60%
450%
98.32%
obtaining
prior
knowledge
the
second
group
is
provided
here
only
to
facilitate
comparison
with
previous
investigations
described
in
section
vii-d
in
both
groups
the
two
general
voting
rules
represented
by
18
and
19
are
used
for
number
of
threshold
values
experiments
with
the
first
group
on
the
2000
samples
1
results
obtained
from
voting
rule
18
table
xiii
gives
the
results
produced
by
18
with
different
values
of
a
it
follows
from
this
table
that
the
combined
by
voting
rule
could
also
improve
individual
classifiers
significantly
note
that
0.0
0.25
means
that
any
value
between
0.0
0.25
produces
the
same
result
because
the
votes
are
digits
among
{0,1,2,3,4}
2
results
obtained
from
voting
rule
20
experiments
with
the
second
group
on
the
last
1000
samples
1
results
obtained
from
voting
rule
19
in
the
following
tables
xv
and
xvi
correspond
to
tables
xiii
and
xiv
respectively
the
difference
is
that
all
the
results
here
are
obtained
from
the
last
1000
samples
results
obtained
from
voting
rule
20
as
an
example
the
confusion
matrix
of
the
combined
with
0.25
0.50
is
given
in
table
xvi
remarks
in
all
the
experiments
described
section
vii-d
the
nonunique
assignment
e
z
is
treated
as
rejection
if
has
more
than
one
element
it
is
worthwhile
to
point
out
2
431
table
xv
results
of
voting
rule
18
on
the
last
1000
samples
recog
subst
reject
reliab
00<a<025
986%
12%
0.2%
98.80%
0.25
<0.50
97.6%
0.5%
1.9%
99.49%
030
<0.75
91.7%
0.0%
8.3%
100.00%
ea
93.0%
23%
47%
97.59%
table
xvi
results
of
voting
rule
20
using
the
last
1000
samples
recog
subst
__
reject
reliab
a=00
98.6%
12%
0.2%
98.80%
00<a<025
983%
0.7%
1.0%
99.29%
025
<050
943%
0.0%
57%
100.00%
050
£0.75
86.1%
0.0%
13.9%
100.00%
eq
93.0%
2.3%
4.7%
97.59%
table
xvii
outcome
of
by
rule
20
with
0.25
0.50
on
the
last
1000
samples
ilo
94
0o
o0
0o
95
33
91
92
55
95
97
770
0o
93
9%
9
89
recogn
9.43%
11
substi
0.0%
reject
5.7%
relab
100.0%
that
if
22
is
used
i.e
as
treated
in
1
then
the
results
could
improve
somewhat
due
to
the
voting
contribution
from
el(z
j
for
example
on
the
2000
samples
we
could
have
recogn
substi
reject
reliab
a=051
93.05%
0.0%
6.95%
100.0%
which
is
better
than
its
counterpart
in
table
xiii
i.e
recogn
substi
reject
reliab
92.9%
0.0%
7.1%
100.0%
in
other
words
e;(z
does
give
some
useful
information
for
combination
in
section
viii
we
will
discuss
how
to
generalize
our
combination
approaches
so
that
the
case
of
ey(z
could
be
included
0.50
0.75
e
comparison
of
the
three
approaches
case
study
the
three
previous
subsections
have
shown
that
each
of
the
three
combination
approaches
improves
the
performance
of
the
individual
classifiers
significantly
in
this
subsection
we
further
compare
the
three
approaches
with
each
other
and
investigate
the
characteristics
of
each
approach
through
reorganizing
some
results
of
the
previous
subsec
tion
we
may
get
four
ordered
lists
tables
xvii-xxiv
will
be
given
in
the
sequel
for
comparing
the
performance
of
the
three
approaches
432
table
xviii
the
maximum
recognition
rates
from
the
2000
sample
set
approach
ds
va
dsy
ecogimax99.2%
>98.95%
98.9%
=98.9%
>98.8%
>96.55%
table
xix
the
maximum
recognition
rates
from
the
last
1000
samples
approach
ds
dsy
va
recogimax
198.6%
98.6%
98.6%
98.6%
92.3%
table
xx
the
best
recognition
rates
under
the
constraint
that
their
correspondent
reliability
rates
100%
from
the
2000
sample
set
approach
ds
dsy
va
tecognmax
94.05%
>93.95%
>92.9%
91.9%
88.6
table
xxi
the
best
recognition
rates
under
the
constraint
that
their
correspondent
reliability
rates
100%
from
the
last
1000
samples
approach
ds
dsy
va
vv
tecognmax
95.0%
95.0%
94.3%
91.7%
in
all
the
tables
recogn
represents
the
maximum
recog
nition
rate
achieved
by
an
approach
i.e
the
recognition
rate
of
the
approach
at
threshold
value
0
in
the
last
three
tables
the
best
recognition
rate
of
an
approach
is
the
highest
one
among
given
subset
of
all
the
recognition
rates
obtained
from
the
experiments
of
the
previous
three
subsections
with
the
correspondent
reliability
rate
of
each
one
in
the
subset
being
not
lower
than
the
given
value
e.g
99%
in
table
xxiv
and
xxv
for
example
in
table
ii
under
the
constraint
that
the
reliability
is
not
lower
than
0.999
we
have
the
best
recognition
rate
95.85%
thus
in
table
xxi
under
item
ds
the
rate
is
95.85%
the
notation
represents
the
approach
based
on
bayesian
formalism
in
section
with
decision
rule
given
by
34
and
35
ds
denotes
the
approach
based
on
dempster—shafer
formalism
in
section
vi
with
decision
rule
48
and
49
dsy
is
version
of
ds
with
the
decision
rule
replaced
by
50
denotes
the
approach
based
on
voting
principle
in
section
iv
with
decision
rule
21
v
is
version
of
with
decision
rule
replaced
by
19
in
part
a
of
all
the
tables
the
training
method
of
b
ds
dsy
is
the
same
as
that
in
the
first
group
of
sections
vii-b
and
vii-c
i.e
the
2000
samples
are
not
only
used
to
learn
from
the
confusion
matrices
or
the
rates
of
individual
classifiers
but
also
to
test
the
combination
approaches
whereas
in
part
b
of
the
tables
the
training
method
is
the
same
as
that
in
the
second
group
of
in
sections
vii-b
and
vii-c
i.e
the
first
1000
samples
are
used
for
learning
and
the
last
1000
samples
for
testing
in
addition
in
tables
xviii
and
xxiv
we
use
b
to
represent
procedure
of
with
the
training
given
in
the
third
group
of
section
vii
i.c
in
each
case
1990
samples
are
used
for
learning
and
the
other
10
samples
one
per
digit
for
testing
the
process
is
repeated
200
times
until
each
of
the
2000
samples
has
been
tested
once
from
tables
xxi-xxv
we
can
summarize
the
following
ieee
transactions
on
systems
man
and
cybernetics
vol
22
no
3
may/june
1992
table
xxit
the
best
recognition
rates
under
the
constraint
that
their
correspondent
reliability
rates
99.9%
from
the
2000
sample
set
approach
dsy
ds
vi
vv
1ecognmax
96.35%
>96.15%
>95.85%
>95.45%
92.9%
table
xxiit
the
best
recognition
rates
under
the
constraint
that
their
correspondent
reliability
rates
99.9%
from
the
last
1000
samples
ds
d34
vy
5.0%
95.0%
94.3%
9l1%
approach
tecognmax
table
xxiv
the
best
recognition
rates
under
the
constraint
that
their
correspondent
reliability
rates
99.0%
from
the
2000
sample
set
ds
vi
dsy
8.9%
98.8%
95.8%
approach
tecognmax
99.2%
98.95%
>98.9%
observations
on
the
characteristics
of
each
approach
1
if
the
confusion
matrices
of
individual
classifiers
are
well
learned
then
the
performance
of
the
approach
based
on
bayesian
formalism
is
the
best
e.g
in
part
a
of
all
the
four
tables
is
ranked
at
the
top
however
the
approach
is
unstable
the
rough
learning
will
degenerate
the
performance
of
the
approach
rapidly
e.g
in
part
b
of
all
the
four
tables
is
either
ranked
the
lowest
or
simply
not
ranked
because
it
could
not
reach
the
required
reliability
2
the
approach
based
on
dempster—shafer
formalism
is
quite
robust
inaccurate
learning
does
not
influence
the
performance
substantially
for
example
from
tables
xviii
and
xix
the
recognition
rates
of
ds
dsy
only
drop
little
while
from
tables
xx
to
xxi
their
recognition
rates
even
go
up
in
addition
we
could
also
see
that
the
performances
of
ds
ds
are
not
much
different
which
means
that
the
decision
rules
given
by
48)~(50
work
equally
well
3
both
the
approach
based
on
d-s
formalism
and
the
approach
based
on
voting
principle
behave
well
on
the
average
the
approach
based
d-s
formalism
is
better
than
the
approach
based
on
voting
principle
especially
when
high
reliability
is
required
see
tables
xx~-xxiii
4
for
the
approach
based
on
the
voting
principle
the
version
v
ie
using
decision
rule
19
performs
better
than
the
version
i.e
using
decision
rule
18
especially
when
high
reliability
is
desired
see
tables
xx-xxiii
in
addition
it
also
follows
from
table
xiii
that
18
is
more
flexible
than
its
special
case
16
for
example
16
which
was
originally
used
in
1
could
only
give
the
recognition
rate
of
92.90%
with
0%
substitution
however
18
could
also
give
other
two
choices
as
shown
in
table
xiii
before
closing
this
section
we
must
emphasize
that
the
previous
observations
are
just
obtained
from
case
study
on
the
2000
sample
data
set
presently
available
they
should
be
tested
with
more
data
sets
strictly
speaking
the
experi
mentally
obtained
recogn
substi
reject
and
reliab
rates
xu
et
al
methods
of
combining
multiple
classifiers
and
their
applications
tg
handwriting
recognition
433
table
xxv
the
best
recognition
rates
under
the
constraint
that
their
correspondent
reliability
rates
99.0%
from
the
last
1000
samples
approach
va
vv
ds
dsy
tecogimax
98.3%
97.60%
>95.0%
95.0%
92.3%
should
all
be
regarded
as
random
variables
and
thus
the
comparisons
of
their
values
should
be
analyzed
statistically
e.g
in
table
xviii
99.2%
98.95%
should
be
tested
under
given
significance
level
this
more
rigorous
analysis
is
certainly
direction
that
deserves
further
pursuit
in
future
here
we
unfortunately
do
not
have
enough
information
to
do
so
first
in
our
experiments
for
each
algorithm
and
under
fixed
o
we
have
only
one
sample
for
random
variables
recogn
substi
reject
and
reliab
e.g
as
given
in
table
xvii
for
0
we
only
have
one
sample
value
99.2%
and
98.95
for
random
variables
recogn.(b
and
recogn.(ds
respectively
to
check
statistically
either
always
recogn.(b
recogn.(ds
or
on
the
average
e[recogn.(b
e[recogn.(ds
we
at
least
need
enough
number
samples
of
recogn.(b
recogn.(ds
for
forming
some
test
statistics
to
conduct
statistical
analysis
second
even
when
there
are
some
ways
to
increase
the
number
of
such
samples
we
also
need
to
known
the
population
distributions
of
recogn.(b
recogn.(ds
we
need
some
further
studies
as
well
as
evidences
before
we
can
appropriately
make
as
sumptions
on
the
distributions
of
recog
substi
reject
and
reliab
for
every
algorithm
third
even
provided
that
we
are
given
the
assumption
that
all
the
distributions
are
gaussian
and
provided
that
we
modeled
the
problem
as
say
simple
hypothesis
testing
hy
e[recogn.(b
e[recogn.(ds
and
h
e[recogn.(b
e[recogn.(ds
we
still
need
to
known
whether
it
is
appropriate
to
make
the
test
under
the
con
dition
with
known
var[recogn.(b)}
var[recogn.(ds
or
unknown
var[recogn.(b
var[recogn.(ds
so
we
see
that
the
aforementioned
interesting
issues
deserve
investigation
with
more
data
sets
and
experiments
in
the
future
viii
conclusion
the
combination
of
several
independent
classifiers
is
gen
eral
problem
that
occurs
in
various
application
areas
of
pattern
recognition
according
to
the
levels
of
output
information
by
various
classifiers
the
problems
of
combining
multiclassifiers
can
be
divided
into
three
types
type
covers
the
individual
classifiers
that
can
output
measurement
values
based
on
which
the
decision
is
made
the
averaged
bayes
classifier
and
its
version
are
proposed
in
section
iii
to
solve
the
problem
of
this
type
this
approach
is
suitable
for
combining
individual
classifiers
such
as
bayesian
classifier
k-nn
classifier
and
various
distance
classifiers
type
covers
the
individual
classifiers
that
can
output
an
ordered
list
of
possible
decisions
i.e
number
of
labels
the
method
proposed
in
4
aims
at
tackling
the
problems
of
this
type
type
covers
the
individual
classifiers
that
output
one
label
one
decision
i.e
zfor
example
iry
to
find
other
data
sets
or
just
divide
the
present
2000
character
samples
into
many
subsets
say
s0
in
one
subset
in
the
latter
case
from
each
subset
one
can
obtain
one
sample
for
recogn
b
recogn.(ds
and
thus
in
total
we
can
have
40
samples
for
recogn.(b
recogn.(ds
any
classifiers
including
those
discussed
in
type
and
type
3
three
approaches
have
been
proposed
in
this
paper
to
solve
the
problems
of
type
1
one
is
based
on
the
voting
principle
that
is
commonly
used
in
social
life
the
other
two
are
developed
in
accordance
with
bayesian
formalism
and
dempster—shafer
formalism—two
well
known
formalisms
used
in
evidence
gathering
and
uncertainty
reasoning
simple
approach
based
on
the
voting
principle
is
the
majority
voting
approach
that
was
originally
proposed
in
1
{21
5
in
our
paper
two
new
versions
of
the
voting
principle
are
also
presented
they
are
proved
better
than
the
simple
majority
voting
approach
used
in
the
experiments
presented
in
section
vii
moreover
general
formula
is
given
it
summarizes
all
those
versions
that
are
based
on
the
voting
principle
although
simple
and
useful
these
approaches
can
not
consider
the
classification
error
of
each
event
ex(z
j
to
fill
this
gap
other
two
approaches
are
developed
they
regard
each
event
ex(x
jr
as
an
evidence
with
uncertain
supports
to
the
possible
decisions
i.c
labels
one
approach
uses
the
confusion
matrix
of
each
individual
classifier
as
the
prior
knowledge
to
manage
this
uncertainty
it
gathers
the
evidences
ex(z
ji
1,---,k
derived
from
bayes
formula
the
other
approach
uses
the
recognition
rate
and
substitution
rate
of
each
individual
classifier
as
prior
knowledge
to
manage
the
uncertainty
dempster’s
combination
formula
is
used
for
gathering
the
evidences
the
experimental
results
on
the
recognition
of
totally
un
constrained
handwritten
numerals
have
shown
that
the
per
formances
of
individual
classifiers
could
be
improved
sig
nificantly
by
the
combination
approaches
proposed
in
this
paper
except
the
one
in
section
iii
which
could
not
be
tested
because
it
was
not
suitable
for
our
application
problem
the
experiments
have
also
shown
the
features
of
each
approach
and
the
details
are
given
in
section
vii-e
based
on
our
case
study
in
section
vii-e
roughly
speaking
the
first
recommendation
would
be
the
approach
based
on
d-s
formalism
since
it
can
obtain
high
recognition
and
reliability
rates
simultaneously
and
robustly
however
this
recommendation
does
not
mean
that
we
should
abandon
the
other
approaches
which
actually
work
also
pretty
well
here
we
should
emphasize
that
the
observa
tions
here
are
obtained
from
our
experiments
on
recognition
of
unconstrained
handwritten
numerals
they
may
be
true
or
may
not
be
totally
true
when
generalized
to
the
problems
or
even
the
same
problem
when
other
data
bases
are
used
however
the
combining
approaches
studied
in
this
paper
are
general
and
not
constrained
to
any
specific
application
problem
we
believe
that
they
will
improve
the
performances
of
individual
classifiers
in
general
we
argue
that
the
research
addressed
to
the
problem
of
combining
multiclassifiers
may
provide
new
insight
to
the
literature
of
statistical
pattern
recognition
previously
the
main
efforts
focus
on
the
design
of
one
good
classifier
and
the
reduction
of
high-dimensional
feature
vector
so
that
desired
classification
rate
can
be
attained
now
we
can
also
change
our
focus
instead
of
designing
one
high
performance
classifier
the
job
is
usually
extremely
difficult
we
can
build
number
of
classifiers
that
use
the
low
dimension
feature
vectors
of
different
and
complementary
types
each
classifier
434
ieee
transactions
on
systems
man
and
cybernetics
vol
22
no
3
may/june
1992
itself
may
not
have
superb
performance
however
the
appropriate
combination
of
these
individual
classifiers
may
produce
performance
of
high
quality
there
are
also
many
new
problems
to
be
studied
we
list
some
of
them
here
1
all
the
approaches
described
in
this
paper
are
based
on
the
assumption
that
individual
classifiers
are
independent
of
each
other
how
to
generalize
these
approaches
or
de
velop
new
approach
to
combine
dependent
classifiers
2
how
many
classifiers
are
appropriate
for
special
prob
lem
with
given
number
of
feature
variables
and
how
to
distribute
these
variables
to
each
classifier
3
in
this
paper
the
recognition
rate
and
the
substitution
rate
of
each
individual
classifier
were
fixed
after
their
training
phase
it
is
well-known
that
these
rates
can
be
changed
by
applying
different
rejection
thresholds
to
the
individual
classifiers
how
to
adjust
these
thresholds
in
the
combination
phase
such
that
the
best
combination
can
be
achieved
4
is
it
possible
to
develop
method
to
analyze
the
recogni
tion
rate
of
the
combined
classifier
theoretically
instead
of
experimentally
even
concentrated
only
on
the
approaches
proposed
in
this
paper
there
are
also
open
problems
to
be
tackled
for
example
on
testing
the
approach
proposed
in
section
iii
and
using
31
directly
to
solve
the
combination
problem
of
type
3
in
addition
at
the
end
of
section
vii-d
we
remarked
that
the
use
of
the
nonunique
assignment
e;(z
{j1,42,---,jp}
by
22
can
further
improve
the
performance
the
approaches
proposed
in
sections
and
vi
can
be
extended
to
cover
this
case
here
we
briefly
introduce
the
key
points
of
such
extensions
1
for
the
approach
given
in
section
v
the
extension
needs
two
modifications
first
use
22
to
let
the
event
ex(x
distribute
its
score
to
learn
from
the
confusion
matrix
of
classifier
e
second
when
classifies
an
unknown
sample
with
ex(x
{j1
2
+
jp}
then
the
following
formula
is
used
to
replace
27
p(z
cifer(z
{j1,j2
"
jp}
k
ij
r=1
2
for
the
approach
given
in
section
vi
the
extension
can
be
made
by
directly
using
40
or
39
for
solving
the
combined
bpa’s
and
bel’s
however
the
computational
complexity
will
increase
exponentially
with
m
if
there
is
only
one
classifier
that
has
an
output
of
nonunique
assignments
as
in
the
case
in
1
and
section
vii
of
this
paper
we
can
borrow
directly
the
implementation
scheme
of
dempster
rule
for
hierarchical
evidence
re
cently
proposed
by
schafer
et
al
28
furthermore
the
scheme
can
also
be
used
directly
for
our
purpose
to
some
special
cases
where
several
classifiers
have
the
output
of
nonunique
assignments
in
these
cases
for
any
two
of
such
classifiers
say
ex(z
j
er(x
i
we
have
the
truth
on
either
inj
=@
orononeof
j
i
or
it
is
not
difficult
to
see
that
these
cases
correspond
to
some
hierarchical
structures
acknowledgment
the
authors
wish
to
thank
christine
nadal
for
providing
the
classification
results
of
the
four
individual
classifiers
e
ez
€3
€4
which
form
the
basis
of
the
experiments
performed
in
section
vii
the
authors
would
also
like
to
thank
the
anonymous
reviewers
for
their
helpful
comments
references
1
c.y
suen
c
nadal
t
a
mai
r
legault
and
l
lam
recognition
of
totally
unconstrained
handwritten
numerals
based
on
the
concept
of
multiple
experts
frontiers
in
handwriting
recognition
c
y
suen
ed
in
proc
int
workshop
on
frontiers
in
handwriting
recognition
montreal
canada
apr
2-3
1990
pp
131-143
2
c
nadal
r
legault
and
c
y
suen
complementary
algorithms
for
the
recognition
of
totally
unconstrained
handwritten
numerals
in
proc
10th
int
conf
pattern
recog
vol
a
june
1990
pp
434—449
3
j.j
hull
commike
and
t
k
ho
multiple
algorithms
for
handwrit
ten
character
recognition
in
frontiers
in
handwriting
recognition
c
y
suen
ed
proc
int
workshop
frontiers
in
handwriting
recognition
montreal
pq
canada
apr
2-3
1990
pp
117-129
4
t
k
ho
j
j
hull
and
s
n
srihari
combination
of
structural
classifiers
in
proc
1990
iapr
workshop
syntactic
and
structural
pattern
recog
june
1990
pp
123-137
5
j.1.hull
s
n
srihari
e
cohen
c
l
kuan
p
cullen
and
p
palumbo
a
blackboard-based
approach
to
handwritten
zip
code
recognition
in
proc
us
postal
service
adv
tech
conf
1988
pp
1018-1032
6
e
mandler
and
j
schuermann
combining
the
classification
results
of
independent
classifiers
based
on
the
dempster/shafer
theory
of
evidences
in
pattern
recognition
and
artificial
intelligence
gelsema
and
kanal
eds
amsterdam
elsevier
science
north-holland
1988
pp
381-393
7
r
m
haralick
statistical
and
structural
approaches
fo
texture
proc
ieee
vol
67
pp
786-804
1979
8
s
levinson
structural
methods
in
automatic
speech
recognition
proc
ieee
vol
73
no
11
pp
1625-1650
1985
9
f
telinek
continuous
speech
recognition
by
statistical
methods
proc
ieee
vol
64
no
4
pp
532-556
1976
10
l
rabiner
and
b
juang
introduction
to
hidden
markov
models
ieee
assp
mag
vol
3
no
1
pp
4-16
jan
1986
11
j
d
gibson
adaptive
prediction
for
speech
encoding
ieee
assp
mag
vol
1
p
12
1984
12
d
o’shaughnessy
speaker
recognition
ieee
assp
mag
vol
3
no
4
pp
4-17
1986
13
e
skordalakis
syntactic
ecg
processing
review
pattern
recog
vol
9
no
4
pp
305-313
1985
14
t
pavlidis
waveform
segmentation
through
functional
approxima
tion
ieee
trans
comput
vol
c-20
pp
59-67
1971
c
h
chen
ed
artificial
intelligence
and
signal
processing
in
un
derwater
acoustic
and
geo-physics
problems
special
issue
pattern
recognition
no
6
1985
16
b
duerr
w
haettich
h
tropf
and
g
winkler
a
combination
of
statistical
and
syntactical
pattern
recognition
applied
to
classification
of
unconstrained
handwritten
numerals
pattern
recog
vol
12
no
3
pp
189-199
1980
17
p
ahmed
and
c
y
suen
computer
recognition
of
totally
uncon
strained
handwritten
zip
codes
int
j
pattern
recog
artificial
intell
vol
1
no
1
pp
1-15
1987
18
c
h
chen
ed
special
section
on
digital
signal
and
waveform
analysis
ieee
trans
pattern
anal
machine
intell
vol
4
no
2
pp
97-141
1982
19
p
a
devijver
and
j
kittler
paitern
recognition
statistical
ap
proach
london
prentice
hall
1982
20
l.lam
and
c
y
suen
structural
classification
and
relaxation
match
ing
of
totally
unconstrained
handwritten
zip-code
numbers
pattern
recog
vol
21
no
1
pp
19-31
1988
21
c.l
kuan
and
s
n
srihari
a
stroke-based
approach
to
handwritten
numeral
recognition
in
proc
us
postal
service
adv
techn
conf
1988
pp
1033-1041
22
p
arentiero
r
chin
and
p
beaudet
an
automated
approach
to
the
design
of
tree
classifiers
jeee
trans
pattern
anal
machine
intell
vol
4
no
1
pp
51-57
1982
15
xu
et
al
methods
of
combining
multiple
classifiers
and
their
applications
to
handwriting
recognition
23
q
r
wang
and
c
y
suen
analysis
and
design
of
decision
tree
based
on
entropy
reduction
and
its
application
to
large
character
set
recognition
ieee
trans
pattern
anal
machine
intell
vol
6
no
4
pp
406417
1984
1
pearl
probabilistic
reasoning
in
intelligent
system
networks
of
pilausible
inference
san
mateo
ca
morgan
kaufmann
publishers
1988
j
gordon
and
e
h
shortliffe
a
method
for
managing
evidential
reasoning
in
hierarchical
hypothesis
space
artificial
intell
vol
23
pp
323-357
1985
j
a
bamett
computational
methods
for
mathematical
theory
of
evidence
in
proc
7th
int
joint
conf
artificial
intell
vancouver
bc
1985
pp
868-875
27
g
shafer
mathematical
theory
of
evidence
ton
university
press
1976
28
g
shafer
and
r
logan
implementing
dempster’s
rule
for
hierarchical
evidence
artificial
intell
vol
33
pp
271-298
1987
29
d
h
ballard
and
c
m
brown
computer
vision
englewood
cliffs
ni
prentice-hall
1982
24
125
26
princeton
nj
prince
lei
xu
received
the
masters
and
ph.d
degrees
in
pattern
recognition
and
signal
processing
from
in
1984
and
1987
respectively
tsinghua
university
from
june
1987
to
june
1988
he
was
postdoc
toral
fellow
at
department
of
mathematics
peking
university
where
he
has
been
an
associate
pro
fessor
since
september
1988
he
was
also
visited
the
department
of
information
technology
and
the
lappecnranta
university
of
technology
finland
at
the
department
of
computer
science
concordia
university
montreal
pq
canada
he
served
as
senior
researcher
and
visiting
scholar
each
for
one
year
respectively
he
is
an
author
of
more
than
60
papers
on
signal
processing
pattern
recognition
artificial
intelligence
computer
vision
and
neural
networks
his
present
research
interests
are
mainly
focused
on
neural
networks
and
computer
vision
he
has
also
experiences
of
serving
as
reviewers
for
neural
networks
ieee
transactions
on
neural
networks
artificial
intelligence
and
several
chinese
high-level
scientific
journals
dr
xu
was
one
of
40
winners
of
the
first
fok
ying
tung
education
foundation
prize
for
young
teachers
of
the
universities
of
the
peoples
republic
china
1988
he
was
also
the
second
of
the
10
winners
of
the
second
beijing
young
scientists
prize
awarded
by
beijing
association
for
science
and
technology
in
1988
435
adam
krzyiak
m’86
was
born
in
szczecin
poland
on
may
1
1953
he
received
the
m.sc
and
ph.d
degrees
in
computer
engineering
from
the
technical
university
of
wroclaw
poland
in
1977
and
1980
respectively
in
1980
he
became
an
assistant
professor
in
the
institute
of
engineering
cybernetics
technical
university
of
wroclaw
poland
from
november
1982
to
july
1983
he
was
postdoctorate
fellow
in
the
school
of
computer
science
mcgill
university
montreal
pq
canada
since
august
1983
he
has
been
with
the
department
of
computer
science
concordia
university
montreal
where
he
is
currently
an
associate
professor
he
has
publications
in
the
areas
of
pattern
recognition
image
processing
identification
and
estimation
of
control
systems
as
well
as
in
various
applications
of
probability
theory
and
statistics
he
edited
the
book
computer
vision
and
pattern
recognition
singapore
world
scientific
1989
dr
krzyzak
is
recipient
of
the
international
scientific
exchange
award
ching
y
suen
m’66-sm’78-f’86
received
the
m.sc
eng
degree
from
the
university
of
hong
kong
and
the
ph.d
degree
from
the
university
of
british
columbia
vancouver
bc
canada
in
1972
he
joined
the
department
of
computer
science
of
concordia
university
montreal
pq
canada
where
he
became
professor
in
1979
and
served
as
chairman
from
1980
to
1984
presently
he
is
the
director
of
cenparmi
the
new
centre
for
pattern
recognition
and
machine
intelligence
of
concordia
during
the
past
15
years
he
was
also
appointed
to
visiting
positions
in
several
institutions
in
different
countries
he
is
the
author/editor
of
10
books
on
subjects
ranging
from
computer
vision
and
shape
recognition
frontiers
in
handwriting
recognition
to
computational
analysis
of
mandarin
and
chinese
his
latest
book
is
entitled
operational
expert
system
applications
in
canada
new
york
pergamon
press
he
is
the
author
of
250
papers
and
his
current
interests
include
pattern
recognition
and
machine
intelligence
expert
systems
optical
character
recognition
and
document
processing
and
computational
linguistics
dr
suen
is
an
active
member
of
several
professional
societies
and
is
fellow
of
the
ieee
he
is
an
associate
editor
of
several
journals
related
to
his
areas
of
interest
he
is
the
past
president
of
the
canadian
image
processing
and
pattern
recognition
society
governor
of
the
international
association
for
pattern
recognition
and
president
of
the
chinese
language
computer
society
he
is
also
the
editor-in-chief
of
computer
processing
of
chinese
oriental
languages
an
international
journal
of
the
chinese
language
computer
society
he
is
the
winner
of
the
1992
itac/nserc
award
for
his
outstanding
contributions
to
pattern
recognition
expert
systsms
and
computational
linguistics
