crossmark
click
for
updates
open
access
citation
najafi
e
darooneh
ah
2015
the
fractal
patterns
of
words
in
text
method
for
automatic
keyword
extraction
plos
one
10(6
€0130617
doi:10.1371/journal.pone.0130617
editor
francisco
j
esteban
university
of
jaén
spain
received
june
25
2014
accepted
may
21
2015
published
june
19
2015
copyright
2015
najafi
darooneh
this
is
an
open
access
article
distributed
under
the
terms
of
the
creative
commons
attribution
license
which
permits
unrestricted
use
distribution
and
reproduction
in
any
medium
provided
the
original
author
and
source
are
credited
data
availability
statement
the
sample
text
is
available
from
http://www.gutenberg.org/files/22764/
and
the
glossary
of
the
text
is
available
from
http://
literature.org/authors/darwin-charles/the-origin-of
species/glossary.html
funding
the
authors
have
no
support
or
funding
to
report
competing
interests
the
authors
have
declared
that
no
competing
interests
exist
plos
one
doi:10.1371/journal.pone.0130617
june
19,2015
research
article
the
fractal
patterns
of
words
in
text
method
for
automatic
keyword
extraction
elham
najafi
amir
h
darooneh
department
of
physics
university
of
zanjan
zanjan
iran
e.najafi@znu.ac.ir
abstract
text
can
be
considered
as
one
dimensional
array
of
words
the
locations
of
each
word
type
in
this
array
form
fractal
pattern
with
certain
fractal
dimension
we
observe
that
important
words
responsible
for
conveying
the
meaning
of
text
have
dimensions
consider
ably
different
from
one
while
the
fractal
dimensions
of
unimportant
words
are
close
to
one
we
introduce
an
index
quantifying
the
importance
of
the
words
in
given
text
using
their
fractal
dimensions
and
then
ranking
them
according
to
their
importance
this
index
mea
sures
the
difference
between
the
fractal
pattern
of
word
in
the
original
text
relative
to
shuffled
version
because
the
shuffled
text
is
meaningless
i.e
words
have
no
importance
the
difference
between
the
original
and
shuffled
text
can
be
used
to
ascertain
degree
of
frac
tality
the
degree
of
fractality
may
be
used
for
automatic
keyword
detection
words
with
the
degree
of
fractality
higher
than
threshold
value
are
assumed
to
be
the
retrieved
keywords
of
the
text
we
measure
the
efficiency
of
our
method
for
keywords
extraction
making
com
parison
between
our
proposed
method
and
two
other
well-known
methods
of
automatic
keyword
extraction
introduction
language
is
the
human
capability
for
communication
via
vocal
or
visual
signs
language
can
be
regarded
as
complex
system
1
where
words
are
constituents
which
interact
with
each
other
to
form
particular
patterns
such
patterns
represent
human
thoughts
feelings
will
and
knowledge
which
are
called
meaning
human
language
is
unique
among
other
communication
systems
because
there
are
lots
of
words
to
express
the
immaterial
and
intellectual
concepts
in
addition
the
existence
of
synonymy
polysemy
and
so
on
increases
its
complexity
texts
as
the
written
form
of
language
inherit
its
complexity
text
can
be
partially
understood
through
regularities
in
spatial
distribution
of
words
and
their
frequencies
research
has
shown
that
reg
ularity
in
text
can
be
expressed
as
power
law
relationship
one
of
the
most
well-known
power
laws
is
zipf's
law
which
shows
that
if
we
rank
the
words
in
text
from
the
most
com
mon
to
the
least
the
frequency
of
each
word
is
inversely
proportional
to
its
rank
2
related
law
heaps
law
shows
another
universal
feature
of
texts
the
number
of
distinct
words
in
text
1/18
plos
one
the
fractal
patterns
of
words
in
text
i.e
number
of
word
types
changes
with
the
text
size
i.e
the
number
of
tokens
in
the
form
of
power
law
3
another
level
of
regularity
is
evident
only
through
the
pattern
of
words
throughout
text
text
is
not
just
random
collection
of
words
we
can
only
call
this
collec
tion
text
if
it
has
meaning
in
other
words
the
words
in
text
must
be
placed
in
specific
order
to
impart
meaning
many
power
laws
cannot
capture
this
fact
any
random
shuffling
pro
cess
drastically
destroys
the
meaning
of
text
but
zipf's
law
remains
unchanged
and
heaps
law
changes
only
very
slightly
4
the
particular
arrangement
of
words
in
specific
order
arises
for
two
reasons
first
gram
matical
rules
determine
where
words
should
be
placed
within
sentence
and
specify
the
posi
tion
of
verbs
nouns
adverbs
and
other
parts
of
speech
grammatical
rules
make
short
range
correlations
between
the
sequences
of
words
in
sentence
secondly
text
derives
meaning
from
how
the
words
are
arranged
throughout
this
ordering
is
called
semantic
ordering
and
acts
across
the
whole
range
of
the
text
hence
the
long-range
correlation
can
be
seen
between
the
positions
of
any
word
the
broad
meaning
of
text
also
means
that
different
word
types
have
different
importance
in
text
we
can
distinguish
between
two
kinds
of
content
words
in
text
those
which
are
related
to
the
subject
of
the
text
i.e
the
important
words
and
all
oth
ers
that
are
irrelevant
to
it
for
text
in
cosmology
words
like
universe
space
big-bang
and
inflation
are
important
words
other
words
such
as
is
fact
happening
etc
are
irrelevant
to
the
topic
of
the
text
finding
an
index
for
quantifying
the
importance
of
words
in
given
text
is
crucial
to
detecting
keywords
automatically
and
provides
very
useful
starting
point
for
text
summarization
document
categorization
machine
translation
and
other
matters
related
to
automatic
information
retrieval
automating
these
processes
is
of
increasing
importance
given
the
increasing
size
of
available
information
yet
limited
man-power
in
the
current
paper
we
use
the
concept
of
fractal
to
assign
an
importance
value
to
every
word
in
given
text
fractal
is
mathematical
object
e.g
set
of
points
in
euclidean
space
that
has
repeating
patterns
at
every
scales
it
means
at
any
magnification
there
is
smaller
piece
of
the
object
that
is
similar
to
the
whole
this
property
is
called
self-similarity
the
fractal
dimension
shows
how
detail
of
fractal
pattern
changes
with
scale
it
is
used
as
an
index
of
complexity
the
fractal
dimension
of
set
is
equal
or
less
than
the
topological
dimension
of
space
that
the
set
is
embedded
in
it
we
claim
that
the
positions
of
word
type
within
the
text
array
form
fractal
pattern
with
specified
dimension
that
is
positive
value
less
than
or
equal
to
one
based
on
this
fact
an
index
is
presented
for
ranking
the
vocabulary
words
of
given
text
the
difference
between
the
pattern
of
word
in
the
original
text
versus
randomly
shuf
fled
version
shows
its
importance
words
with
greater
differential
between
the
original
and
shuffled
texts
are
more
important
we
compare
this
approach
with
other
more
well-known
methods
of
keyword
extraction
in
the
following
section
we
review
previous
research
reporting
kind
of
fractal
structure
in
texts
in
order
to
show
that
our
method
is
novel
then
we
review
some
basic
ideas
for
keyword
extraction
which
are
useful
for
understanding
the
different
principles
currently
at
work
in
the
field
finally
we
describe
our
method
and
how
it
could
be
evaluated
and
report
the
results
for
sample
book
background
and
related
works
fractal
structures
in
texts
in
1980
g
altmann
made
formula
for
quantifying
of
the
menzerath’s
law
5
menzerath
altmann
law
says
there
is
relation
between
size
of
construct
and
size
of
its
constituents
system
like
language
has
different
levels
or
constructs
such
as
syllables
words
syntactic
plos
one
doi:10.1371/journal.pone.0130617
june
19
2015
2/18
plos
one
the
fractal
patterns
of
words
in
text
constructions
clauses
sentences
and
semantic
constructs
according
to
menzerath-altmann
law
when
the
size
of
construct
increases
the
size
of
its
constituents
decreases
and
this
holds
at
every
level
thus
certain
kind
of
self-similarity
exists
for
each
level
6
7
fractal
dimen
sion
can
be
calculated
for
each
level
the
fractal
dimension
of
given
text
is
the
average
value
of
fractal
dimension
of
levels
8
for
quantitative
calculations
texts
are
usually
mapped
into
time
series
text
can
be
consid
ered
as
one
dimensional
array
where
elements
can
be
either
characters
words
or
sentences
ausloos
built
two
time
series
by
replacing
each
word
in
the
text
by
their
length
or
frequency
9
10
he
quantified
the
complexity
in
written
text
by
examining
the
fractal
pattern
of
its
corre
sponding
length
and
frequency
time
series
discovering
that
resulting
fractal
patterns
may
be
used
as
an
authorship
indicator
furthermore
these
length
and
frequency
time
series
also
gave
indications
of
the
semantic
complexity
of
the
text
eftekhari
worked
on
letters
instead
of
words
as
the
constituents
of
text
finding
that
if
let
ter
types
in
text
are
ranked
from
the
most
common
to
the
least
the
frequency
of
each
letter
type
would
be
inversely
proportional
to
its
rank
11
i.e
simillar
to
zipf's
law
if
frequency
of
letter
types
is
plotted
versus
their
ranks
in
double
logarithmic
scale
straight
line
is
obtained
he
called
the
slope
of
this
line
zipf's
dimension
he
also
suggested
method
for
calculating
fractal
dimension
of
texts
declaring
that
if
letter
types
are
ranked
in
alphabetical
order
and
fre
quency
of
letter
types
is
plotted
against
their
ranks
the
slope
of
such
diagram
would
be
fractal
dimension
of
the
literature
nevertheless
since
the
data
which
is
used
is
too
disperse
he
used
the
so-defined
fractal
dimension
he
also
showed
that
texts
exhibit
changes
in
fractal
dimen
sion
similar
to
corresponding
zipf’s
dimension
which
vary
according
to
the
text’s
size
principles
for
keyword
extraction
the
first
method
based
on
zipf’s
analysis
of
word
frequency
for
keyword
extraction
was
pro
posed
by
luhn
12
he
plotted
the
zipf
diagram
of
words
then
eliminated
words
with
high
and
low
frequencies
and
declared
that
the
words
remaining
in
the
mid-range
frequencies
are
the
most
important
words
of
text
there
are
some
problems
with
this
method
it
omits
some
important
words
which
have
very
low
frequencies
and
may
also
mistakenly
take
some
com
mon
words
with
mid-range
frequencies
as
keywords
to
overcome
this
deficiency
ortufio
et
al
proposed
method
based
on
the
concept
that
important
words
form
clusters
13
they
used
standard
deviation
of
distance
between
consecutive
occurrences
of
particular
word
as
measure
of
word
clustering
words
with
large
standard
deviations
tend
to
form
clusters
and
so
are
more
important
carpena
et
al
improved
this
method
and
introduced
the
value
for
mea
suring
the
importance
of
words
14
based
on
their
clustering
distributions
we
review
this
method
in
the
appendix
section
in
contrast
to
our
own
another
method
based
on
clustering
was
proposed
by
zhou
and
slater
15
they
used
the
density
fluctuations
of
words
as
mea
sure
of
clustering
the
method
was
useful
to
reduce
significance
of
common
words
mihalcea
and
tarau
used
method
based
on
the
graph
theory
for
detecting
the
keywords
16
the
text
is
regarded
as
graph
with
word
types
nodes
with
edges
occuring
between
two
words
where
they
are
adjacent
in
the
text
to
extract
keywords
they
introduced
the
concept
of
textrank
cal
culated
similarly
to
pagerank
which
is
used
in
the
google
search
engine
for
ranking
the
web
pages
textrank
works
by
counting
the
number
and
weight
of
links
to
node
to
determine
importance
of
the
node
the
more
important
nodes
are
likely
to
receive
more
links
from
other
nodes
words
with
higher
values
of
textrank
are
more
important
herrera
and
pury
suggested
an
entropic
method
for
word
ranking
based
on
the
relative
frequency
of
words
in
each
part
of
the
text
17
this
method
is
also
reviewed
in
the
appendix
in
contrast
to
our
own
mehri
and
darooneh
used
several
entropic
metrics
to
extract
keywords
18
in
particular
they
found
that
plos
one
doi:10.1371/journal.pone.0130617
june
19
2015
3/18
plos
one
the
fractal
patterns
of
words
in
text
cumulative
distribution
of
distances
between
consecutive
occurrences
of
word
type
follows
p:[1+(q_1);3x]ﬁ
1
where
is
distance
between
consecutive
occurrences
of
word
type
bis
constant
and
is
positive
value
they
ranked
words
according
to
value
the
value
of
in
the
case
of
important
words
is
larger
than
the
case
of
common
words
19
methods
the
degree
of
fractality
text
is
certain
arrangement
of
words
in
one
dimensional
array
that
carries
meaning
any
ran
dom
shuffling
of
the
words
across
the
text
significantly
reduces
its
meaning
hence
the
ordering
of
the
words
is
important
for
representation
of
the
meaning
in
other
words
the
meaning
shows
kind
of
regularity
in
text
this
regularity
also
manifests
itself
in
pattern
of
occurrences
of
each
word
in
the
text
array
if
we
consider
the
text
array
as
one
dimensional
space
the
spatial
pattern
of
occurrences
of
any
vocabulary
word
will
form
fractal
set
or
simply
fractal
we
can
assign
fractal
dimension
to
any
word
in
given
text
using
the
practical
method
of
box
counting
using
this
method
the
fractal
dimension
of
word
is
generally
between
and
1
in
box-counting
the
space
is
divided
into
boxes
each
box
that
contains
component
of
the
fractal
set
is
called
filled
box
the
fractal
law
is
power
law
relationship
between
the
number
of
filled
boxes
and
the
box-size
20
to
calculate
the
fractal
dimension
of
word
by
box-counting
method
the
text
array
is
divided
into
boxes
of
size
s
we
place
each
consecutive
words
in
box
the
number
of
such
boxes
is
n
n/s
where
is
the
length
of
the
text
if
the
considered
word
appears
in
one
of
the
boxes
that
box
is
filled
box
nj(s
stands
for
the
number
of
filled
boxes
power
law
relation
ship
exists
between
the
number
of
filled
boxes
and
the
box
size
as
follows
n,(s
xxs7®
is
the
fractal
dimension
of
the
word
fractal
dimension
is
obtained
by
measuring
the
slope
of
log-log
plot
of
nj(s
versus
s
it
is
worth
noting
that
here
the
box
size
is
an
integer
number
and
in
practice
we
expect
to
see
the
power
law
behavior
for
the
large
box
sizes
as
we
noted
earlier
the
fractal
dimension
for
any
word
is
between
and
1
when
all
occur
rences
of
word
are
distributed
uniformly
across
the
text
all
of
the
boxes
have
the
same
proba
bility
of
containing
token
of
the
word
therefore
in
this
particular
case
the
number
of
filled
boxes
has
the
maximum
possible
value
in
other
cases
some
of
the
boxes
may
contain
more
than
one
occurrence
this
results
in
some
of
the
other
boxes
remaining
empty
and
the
number
of
filled
boxes
is
less
than
this
limiting
value
in
shuffled
text
all
of
the
words
are
distributed
uniformly
for
small
scales
when
the
num
ber
of
boxes
is
greater
than
the
frequency
of
word
type
the
number
of
filled
boxes
is
expected
to
be
approximately
equal
to
the
frequency
of
the
word
type
by
increasing
the
box
size
the
number
of
filled
boxes
will
be
decreased
in
large
scales
the
fact
that
the
number
of
filled
boxes
is
maximum
makes
the
slope
of
the
log-log
plot
of
nj(s
versus
close
to
one
the
upper
limit
for
slope
the
following
equation
indicates
our
conjecture
on
the
number
of
filled
boxes
for
word
in
the
shuffled
text
against
the
box
size
consistent
with
the
above
facts
_l
®3
1+
(
s—1
ni(s
where
is
frequency
of
the
word
w
plos
one
doi:10.1371/journal.pone.0130617
june
19
2015
4/18
o~
plos
one
the
fractal
patterns
of
words
in
text
the
fractal
dimension
is
the
slope
of
the
line
of
best
fit
on
the
log-log
plot
of
the
number
of
filled
boxes
against
the
box
size
in
practice
the
choice
of
the
fitting
range
is
very
important
and
definitely
has
influence
on
the
value
of
the
fractal
dimension
unfortunately
there
is
no
way
to
automatically
choose
the
most
appropriate
fitting
range
instead
of
the
fractal
dimen
sion
we
propose
an
index
which
is
used
to
quantify
the
fractality
of
the
word
pattern
in
another
way
the
degree
of
fractality
is
defined
as
55
where
is
particular
word
the
degree
of
fractality
ds
measures
the
difference
between
the
pattern
of
occurrences
of
word
in
the
original
and
shuffled
text
we
use
the
logarithm
in
the
definition
of
this
index
to
avoid
domination
of
the
values
for
small
box
sizes
the
degree
of
fractality
is
suitable
quantity
for
ranking
the
words
of
text
in
computing
the
degree
of
frac
tality
we
only
need
to
find
the
number
of
filled
boxes
for
any
scale
unlike
the
process
of
com
putation
of
the
fractal
dimension
data
regression
is
not
required
moreover
we
are
not
faced
with
the
problem
of
determining
the
fitting
range
for
each
word
the
larger
value
for
the
degree
of
fractality
means
the
distribution
pattern
of
word
has
more
differences
with
the
uniform
distribution
evaluation
of
the
method
the
degree
of
fractality
gives
an
importance
value
for
every
word
type
in
given
text
using
this
value
we
are
able
to
list
the
words
from
greatest
to
least
importance
the
top-ranked
words
of
the
list
are
assumed
as
keywords
comparison
with
manually
created
list
of
keywords
allows
for
an
approximate
evalua
tion
of
the
efficiency
of
our
method
it
is
important
to
know
how
the
list
of
the
relevant
key
words
is
prepared
for
given
book
in
our
experience
we
assume
that
the
manually
created
glossary
of
book
is
good
candidate
for
providing
the
relevant
keywords
of
the
book
the
glossary
of
book
should
be
prepared
by
author
or
some
experts
of
the
field
thus
it
is
reliable
to
be
selected
as
our
reference
data
the
following
two
issues
are
important
when
we
have
comparison
between
the
list
of
rele
vant
and
retrieved
keywords
first
it
is
important
to
compute
how
many
words
are
common
in
the
two
lists
if
both
of
them
have
the
same
size
second
what
fraction
of
the
retrieved
list
should
be
selected
to
include
all
the
relevant
keywords
in
binary
classification
analysis
recall
and
precision
are
two
metrics
which
consider
the
above
issues
respectively
the
recall
and
pre
cision
are
calculated
as
follows
according
to
herrera
and
pury’s
suggestion
17
these
are
well-known
metrics
for
evaluation
of
keyword
extraction
methods
r=
5
nyoss
ny
©
last
where
ny
is
the
size
of
list
of
relevant
keywords
glossary
n
is
the
number
of
common
keywords
in
two
lists
which
have
the
rank
less
than
ngjos
and
nia
stands
for
the
last
position
of
relevant
keywords
in
the
list
of
retrieved
words
it
is
worth
noting
again
that
these
metrics
cannot
precisely
determine
the
accuracy
of
the
keyword
detection
methods
according
to
our
experience
they
depend
on
the
data
processed
selected
book
its
genre
and
on
how
the
list
of
relevant
keywords
is
prepared
plos
one
doi:10.1371/journal.pone.0130617
june
19
2015
5/18
plos
one
the
fractal
patterns
of
words
in
text
there
is
another
method
for
calculating
recall
and
precision
that
is
suggested
by
mehri
and
darooneh
18
in
this
method
words
with
degree
of
fractality
higher
than
threshold
value
are
selected
as
retrieved
keywords
the
threshold
value
is
choosen
such
that
some
percentage
of
ranked
list
of
words
is
selected
as
the
retrieved
keywords
in
each
step
then
number
of
key
words
which
is
the
same
between
glossary
and
this
new
list
is
counted
recall
and
precision
are
calculated
as
follows
r=
ny
@
ne
p==
8
again
ny
is
the
size
of
glossary
and
n
is
the
number
of
keywords
which
are
the
same
between
glossary
and
selected
percentage
of
retrieved
list
n
is
the
size
of
the
retrieved
list
to
the
whole
vocabulary
size
in
percent
results
universal
properties
of
texts
to
explain
more
details
we
apply
our
method
to
on
the
origin
of
species
by
charles
darwin
21
the
book
is
about
evolution
of
populations
through
process
of
natural
selection
digi
tal
copy
of
this
text
is
freely
available
on
project
gutenberg
22
we
only
keep
the
main
body
of
the
text
and
leave
the
others
e.g
contents
index
no
other
preprocessing
tasks
are
per
formed
except
deletion
of
the
non-alphabetic
characters
the
book
has
total
of
191740
tokens
and
contains
8842
distinct
word
types
we
examined
two
famous
regularities
of
texts
for
this
book
the
zipf's
and
heaps
law
fig
shows
zipf's
law
for
the
book
frequency
of
each
word
type
is
plotted
against
word
rank
on
double-logarithmic
scale
straight
line
is
obtained
with
slope
of
1.01
fig
shows
heap’s
law
for
the
book
size
of
vocabulary
is
plotted
versus
size
of
text
on
double-logarithmic
scale
straight
line
is
obtained
with
slope
of
0.73
as
outlined
earlier
the
spatial
distribution
or
pattern
of
ocurrences
of
any
word
in
given
text
exhibits
self-similarity
the
box
counting
is
practical
procedure
for
measuring
this
prop
erty
in
this
procedure
the
text
is
divided
into
boxes
of
size
s
that
varies
from
to
the
text
size
=1
means
each
box
contains
only
one
word
means
each
box
contains
two
words
and
so
on
box
is
called
filled
if
it
contains
some
instances
of
the
considered
word
we
chose
powers
of
for
our
box
sizes
as
an
example
fig
illustrates
division
of
small
part
of
our
sample
book
into
boxes
with
size
2
and
8
in
this
example
the
appears
in
3
3
and
boxes
for
2
4
and
respectively
distribution
of
word
is
self-similar
if
we
see
the
same
pattern
for
the
word
in
all
scales
in
all
5
in
fig
the
distribution
of
hybrid
one
of
the
vocabulary
words
in
our
sample
book
is
shown
in
three
different
scales
1
256
and
1024
as
is
seen
in
this
figure
distribution
of
hybrid
is
the
same
in
these
scales
ranking
the
words
and
keyword
detection
all
words
have
self-similar
pattern
in
the
text
but
with
different
fractal
dimensions
if
the
word
is
uniformly
distributed
along
the
text
its
fractal
dimension
is
close
to
one
for
words
which
are
clustered
in
text
the
fractal
dimension
is
substantially
less
than
one
fig
shows
dis
tribution
of
two
words
of
the
instance
book
hybrid
and
rarely
both
of
them
have
the
same
plos
one
doi:10.1371/journal.pone.0130617
june
19
2015
6/18
plos
one
the
fractal
patterns
of
words
in
text
10000
\\
\\
\
0:\
pn
1000
(
e}
100
n\
\\
10
10
100
1000
rank
fig
1
zipf's
law
for
the
book
the
origin
of
species
frequency
of
each
word
is
inversely
proportional
to
its
rank
in
form
of
power
law
the
zipf
curve
follows
straight
line
with
slope
of
1.01
when
plotted
on
double-logarithmic
scale
doi:10.1371/journal.pone.0130617.g001
10000
1000
100
nwf
10
100
1000
10000
100000
1000000
size
of
text
fig
2
heap’s
law
for
the
book
the
origin
of
species
size
of
vocabulary
increases
as
size
of
text
increases
in
form
of
power
law
the
heap
curve
follows
straight
line
with
slope
of
0.73
when
plotted
on
double-logarithmic
scale
doi:10.1371/journal.pone.0130617.9002
plos
one
doi:10.1371/journal.pone.0130617
june
19
2015
7/18
o~
plos
one
the
fractal
patterns
of
words
in
text
movable
lip
ior
lid
iof
the
|ce|l
correspondingl
with
theiiower
and
movable
mandible|of
the
movable
lip
or
lid
iofthe
cell
corresponding
|with
the
lower
and
|movab|e
mandible
of
the
movable
lip
or
lid
of
the
cell
corresponding
with
the
lower
and
movable
mandible
of
the
fig
3
schematic
of
how
an
instance
text
is
devided
into
boxes
the
number
of
words
that
is
placed
in
box
is
the
box-size
box-size
for
first
row
is
equal
to
and
for
the
second
and
third
rows
are
and
respectively
doi:10.1371/journal.pone.0130617.9003
s=1
|i
50000
100000
150000
200000
s=256
100
200
300
400
500
600
700
800
s=1024
50
100
150
200
fig
4
spatial
distribution
of
hybrid
in
the
book
the
origin
of
species
for
three
different
scales
as
seen
distributions
in
all
scales
1
=256
and
1024
are
statistically
the
same
they
have
similar
clusters
doi:10.1371/journal.pone.0130617.9004
frequency
45
occurrences
of
hybrid
form
cluster
in
the
text
while
rarely
has
uniform
distribution
in
fig
we
compute
the
fractal
dimension
for
these
words
hybrid
has
dimension
0.4
and
dimension
of
rarely
is
0.8
we
also
plot
the
results
for
other
pair
of
words
cell
and
actually
with
28
occurrences
in
the
book
for
both
of
them
cell
is
clustered
as
same
as
hybrid
and
actu
ally
has
uniform
pattern
like
rarely
in
the
shuffled
text
all
words
are
distributed
more
uniformly
and
clustered
words
do
not
occur
fig
illustrates
the
result
of
box
counting
for
hybrid
in
our
sample
book
and
its
shuffled
version
our
conjecture
on
the
number
of
filled
boxes
in
the
shuffled
text
is
also
plotted
show
ing
that
our
conjecture
has
good
agreement
with
the
shuffled
data
the
patterns
of
words
that
have
uniform
distributions
change
only
slightly
after
the
shuf
fling
process
indicating
that
the
words
uniformly
distributed
in
the
original
text
are
plos
one
doi:10.1371/journal.pone.0130617
june
19
2015
8/18
plos
one
the
fractal
patterns
of
words
in
text
hybrid
50000
100000
150000
i
50000
100000
150000
fig
5
spatial
distribution
of
two
words
hybrid
and
rarely
in
the
book
the
origin
of
species
according
to
subject
of
the
book
hybrid
is
an
important
and
the
rarely
is
an
irrelevant
word
both
of
them
have
the
same
frequency
equal
to
45
rarely
is
distributed
in
the
text
uniformly
but,hysrip
is
clustered
doi:10.1371/journal
pone.0130617.9005
100
606004
eoe
00
ﬁ\s
|q
\\
\\
<&
14
10
\\\
h\
2y
%o
ny
ot
cell
o0
\g
actually
hybrid
ee
rarely
10
100
1000
10000
100000
1000000
box
size
fig
6
results
of
box
counting
for
hybrid
and
rarely
the
dashed
line
and
dash
dotted
line
demonstrate
the
power
law
regression
the
fractal
dimension
is
about
0.4
for
hysrid
and
is
close
to
0.8
for
rarely
the
box
counting
result
of
cell
and
actually
is
also
showed
the
fractal
dimension
is
about
0.4
for
cell
and
is
close
to
0.8
for
actually
doi:10.1371/journal.pone.0130617.g006
plos
one
doi:10.1371/journal.pone.0130617
june
19
2015
9/18
plos
one
the
fractal
patterns
of
words
in
text
100
eee-e8-0-0.0.e
5
b0
|9
hybrid
hybrid-shuffled
text
shuffled_conjectured
10
100
1000
10000
100000
1000000
box
size
fig
7
results
of
box
counting
for
distribution
of
hybrid
in
the
original
and
shuffled
text
hverid
is
an
important
word
in
the
book
the
origin
of
species
so
there
is
considerable
difference
between
box
counting
of
this
word
in
the
original
and
shuffled
text
doi:10.1371/journal.pone.0130617.g007
unimportant
the
difference
between
patterns
of
word
in
the
original
and
shuffled
text
can
be
considered
an
indication
of
its
importance
the
degree
of
fractality
which
is
defined
in
eq
measures
this
difference
fig
shows
the
degree
of
fractality
for
two
words
hybrid
and
cell
it
is
clear
from
this
figure
that
cell
is
more
important
than
hysrid
the
degree
of
fractality
of
hybrid
is
8.21
and
is
12.71
in
the
case
of
cell
now
we
can
rank
all
of
the
words
according
to
the
degree
of
fractality
table
reports
the
list
of
twenty
top-ranked
words
and
also
the
first
twenty
frequent
words
for
comparison
according
to
the
subject
of
the
book
words
such
as
slaves
illegitimate
saliva
and
pedicellariae
are
important
words
they
also
have
higher
degree
of
fractality
in
comparison
with
other
words
the
irrelevant
words
like
the
of
and
and
1n
have
lower
degree
of
fractality
though
they
are
very
frequent
in
the
book
it
is
useful
to
point
out
that
function
words
have
the
lowest
degree
of
fractality
overall
but
unimportant
con
tent
words
still
have
lower
fractality
than
important
keywords
for
small
texts
word
frequency
becomes
increasingly
important
for
taking
into
account
the
effect
of
frequency
we
multiply
log(m
by
the
degree
of
fractality
causing
the
most
changes
in
degree
of
fractality
rank
in
the
middle
of
the
list
while
words
at
the
top
of
the
list
have
small
change
in
their
rank
other
choices
may
change
the
rank
of
the
words
in
all
parts
of
the
list
significantly
table
presents
another
retrieved
list
of
words
according
to
this
combined
measure
now
words
like
slaves
wax
hybrids
and
instincts
are
placed
in
the
top
in
this
new
ranking
list
the
word
hybrid
changes
its
place
from
321
to
48
the
word
rarely
also
moves
from
2203
rank
to
1011
in
addition
to
the
degree
of
fractality
there
exist
several
methods
that
assign
an
importance
value
to
any
word
in
given
text
we
can
list
the
words
in
descending
order
of
their
impor
tance
in
this
list
the
words
that
are
placed
in
the
top
ranks
are
assumed
to
be
keywords
by
choosing
threshold
value
we
can
identify
the
list
of
keywords
in
the
following
section
we
evaluate
our
proposed
method
for
the
keyword
detection
task
plos
one
doi:10.1371/journal.pone.0130617
june
19
2015
10/18
plos
one
the
fractal
patterns
of
words
in
text
100
10
number
of
filled
boxes
32
128
512
2048
8192
32768
131072
box
size
fig
8
area
which
is
bounded
between
two
curves
for
cell
and
hybrid
in
the
box
counting
diagram
the
curves
correspond
to
box
counting
result
for
these
two
words
in
the
original
and
shuffled
text
the
area
corresponds
to
cell
is
bigger
than
the
case
of
hybrid
cell
is
more
important
than
hyerid
in
the
book
the
origin
of
species
doi:10.1371/journal.pone.0130617.g008
evaluation
of
our
method
the
best
way
to
evaluate
the
efficiency
of
our
approach
to
keyword
detection
is
comparing
its
results
with
other
methods
we
use
two
metrics
in
this
comparison
precision
and
recall
these
tell
us
to
what
extent
the
retrieved
list
of
keywords
conforms
to
the
manually
selected
list
as
described
in
the
previous
section
in
this
work
we
would
like
to
compare
our
method
with
two
efficient
methods
in
keyword
extraction
the
value
14
and
entropy
17
these
methods
are
selected
according
to
our
experience
we
found
that
value
has
the
maximum
amount
of
recall
compared
with
other
methods
and
entropy
has
maximum
amount
of
precision
com
pared
with
others
18
23
these
methods
are
reviewed
in
further
detail
in
the
appendix
to
do
the
assessment
we
use
the
glossary
written
by
w
s
dallas
24
note
that
the
choice
of
glos
sary
has
the
potential
to
considerably
alter
the
result
of
comparisons
two
points
are
relevant
before
proceeding
to
the
comparison
first
the
glossary
of
the
book
contains
not
only
words
but
also
some
phrases
to
deal
with
multi-word
keywords
of
the
glos
sary
we
separate
them
into
single
words
for
example
we
convert
the
phrase
ganoid
fishes
to
two
separate
words
ganooid
and
fishes
in
the
glossary
second
in
any
method
value
is
assigned
to
each
vocabulary
word
then
we
can
sort
the
words
from
the
highest
value
to
the
lowest
we
give
rank
to
the
first
word
in
the
sorted
list
the
second
word
takes
rank
and
so
on
unlike
in
zipfian
ranking
this
ranking
process
allows
for
rank
ties
in
other
words
if
some
words
have
the
same
assigned
value
they
should
have
the
same
rank
as
an
example
in
table
the
words
forward
and
months
have
equal
values
in
this
case
we
assign
them
equal
plos
one
doi:10.1371/journal.pone.0130617
june
19
2015
11/18
plos
one
the
fractal
patterns
of
words
in
text
table
1
list
of
the
twenty
top-ranked
words
according
to
degree
of
fractality
left
and
the
first
twenty
frequent
words
right
from
the
book
the
origin
of
species
words
with
high
degree
of
fractality
are
important
words
according
to
subject
of
the
book
and
common
words
have
low
degree
of
fractality
the
string
un
which
is
placed
in
the
second
row
of
list
of
top-ranked
words
is
french
determinant
which
appears
four
times
in
single
sentence
so
it
is
highly
clustered
and
has
high
value
of
fractality
because
we
do
not
per
form
any
pre-processing
to
eliminate
foreign
words
this
word
appears
in
the
list
words
frequency
fractality
words
frequency
fractality
slaves
34
17.42
the
13368
2.54
un
16.70
of
9071
2.67
illegitimate
21
16.52
and
5482
2.79
saliva
16.42
in
4973
297
pedicellariae
15
16.03
to
4477
2.79
floated
18
15.98
3143
271
pupae
13
15.72
that
2612
2.77
wax
42
15.65
as
2122
3.16
vibracula
12
15.54
have
2051
279
masters
17
15.52
be
2045
278
avicularia
13
15.28
is
1975
2.80
dried
15.11
species
1745
242
movable
10
15.10
by
1665
2.82
segment
15.04
which
1646
2.76
caudicle
14.59
are
1556
2.69
neuters
12
14.93
or
1489
3.22
cuckoo
32
14.89
it
1462
3.04
lamellae
20
14.67
on
1432
3.12
dun
14.60
with
1383
3.02
bucket
14.59
for
1381
2.98
doi:10.1371/journal.pone.0130617.t001
rank
2128
and
the
next
word
in
the
list
will
have
rank
2130
there
are
two
approaches
for
cal
culating
recall
and
precision
in
herrera
and
puri
approach
17
they
do
not
indicate
any
threshold
after
ranking
words
according
to
an
importance
index
the
last
word
of
the
glossary
in
the
ranked
list
is
found
then
the
number
of
words
from
the
ranked
list
which
include
all
the
glossary
words
are
selected
as
keywords
in
this
approach
they
introduce
cut-off
frequency
they
keep
only
the
words
with
frequencies
greater
or
equal
to
the
cut-off
frequency
both
in
ranked
list
and
in
the
glossary
and
omit
all
other
words
with
lower
frequencies
for
example
cut-off
frequency
equal
to
means
only
words
with
frequencies
more
than
are
kept
and
other
words
are
omitted
the
number
of
words
from
ranked
list
and
from
the
glossary
for
various
choices
of
cut-off
fre
quency
are
written
in
table
4
in
fig
9
the
recall
and
precision
are
plotted
against
the
cut-off
frequency
according
to
fig
9
recall
for
combined
measure
is
higher
than
other
methods
for
cut-off
frequencies
greater
than
5
this
means
that
the
proposed
fractal
method
is
superior
to
the
others
as
method
for
keyword
extraction
the
precision
of
combined
measure
is
higher
than
value
for
all
cut-off
frequencies
if
we
rank
the
words
according
to
their
fractality
we
will
find
power
law
relationship
between
the
fractality
of
word
and
its
rank
therefore
it
is
rational
to
choose
the
words
with
rank
lesser
than
specific
value
as
the
retrieved
keywords
list
instead
of
using
the
fractality
threshold
in
mehri
and
darooneh
approach
18
after
ordering
words
due
to
their
fractality
plos
one
doi:10.1371/journal.pone.0130617
june
19,2015
12/18
plos
one
the
fractal
patterns
of
words
in
text
table
2
list
of
the
twenty
top-ranked
words
according
to
combined
measure
from
the
book
the
ori
gin
of
species
these
words
are
important
according
to
the
subject
of
the
book
the
word
is
related
to
some
classification
of
species
such
as
18
{10
f14
.
and
some
proper
names
is
kept
because
non-alpha
betical
characters
are
removed
in
our
method
words
frequency
slaves
34
wax
42
hybrids
135
instincts
87
sterility
100
cuckoo
32
illegitimate
21
floated
18
instinct
63
masters
17
lamellae
20
pedicellariae
15
cell
28
nest
55
46
pupae
13
cells
58
fertility
80
spheres
19
clover
15
fractality
17.42
15.65
10.89
11.85
11.27
14.89
16.52
15.98
10.62
15.52
14.67
16.03
12.71
10.23
10.62
15.72
9.84
9.08
13.46
14.55
combined
measure
26.68
25.40
23.20
23.00
2253
22.40
21.85
20.07
19.11
19.10
19.09
18.85
18.39
17.80
17.66
17.51
17.36
17.27
17.22
17.11
doi:10.1371/journal.pone.0130617.t002
table
3
list
of
ten
words
and
their
ranks
from
the
book
the
origin
of
species
words
with
equal
com
bined
measures
take
equal
ranks
words
forward
months
saved
treat
observers
gone
inferiority
agree
icebergs
laying
really
doi:10.1371/journal.pone.0130617.t003
plos
one
doi:10.1371/journal.pone.0130617
june
19
2015
combined
measure
3.31199
3.31199
3.31115
3.31115
3.30809
3.30749
3.30647
3.30564
3.30447
3.30447
3.30164
rank
2128
2128
2130
2130
2132
2133
2134
2135
2136
2136
2138
13/18
plos
one
the
fractal
patterns
of
words
in
text
table
4
number
of
vocabulary
words
and
number
of
glossary
words
for
various
cut-off
frequencies
n
and
n
are
the
number
of
vocabulary
words
from
the
book
and
number
of
glossary
words
for
each
cut-off
frequency
respectively
cut-off
frequency
10
n
8842
5351
4092
3428
2957
2624
2352
2141
1968
1855
ny
229
157
126
109
89
79
72
65
57
54
doi:10.1371/journal.pone.0130617.t004
#
combined
measure
-
®
value
-
¢
entropy
0.08
0.06
recall
0.09
0.02
o4
=4
=4
cutcff
frequency
0.045
combined
measure
value
entropy
0.09
0.035
0.03
0.025
0.0z
0.015
precisiocn
0.01
0.005
ot
10
12
cutcff
frequency
fig
9
results
of
calculating
recall
and
precision
with
herrera
and
purri
approach
for
the
book
the
origin
of
species
for
10
cut-off
frequencies
the
fractal
method
has
the
highest
value
of
recall
in
all
frequencies
and
higher
value
of
precision
than
value
method
t
doi:10.1371/journal.pone.0130617.g009
plos
one
doi:10.1371/journal.pone.0130617
june
19
2015
14/18
plos
one
the
fractal
patterns
of
words
in
text
1z
@
fractality
®
combined
measure
19
value
¥
entropy
recall
0.z
0.4
0.6
0.8
1.2
cutoff
frequency
0.09
0.08
g
#
fractality
n
\
#
combined
measure
gor
4y
#
value
\
.
\
x
entropy
vy
006
¥‘:\
aos
oot
0.04
\,.::.h
ha
003y
ot
==
a--......._.__
a0z
rx--=
0.01+
o177
71
0.z
0.4
0.6
0.8
1.2
t
cutcoff
frequency
fig
10
results
of
calculating
recall
and
precision
with
mehri
and
darooneh
approach
for
the
book
the
origin
of
species
the
fractal
method
has
the
highest
value
of
recall
and
precision
in
all
vocabulary
fractions
doi:10.1371/journal.pone.0130617.g010
percentage
of
words
from
the
top
of
the
ranked
list
are
selected
as
keywords
in
the
first
step
the
top
percent
of
the
ranked
list
are
selected
as
keywords
the
first
percent
of
8842
in
the
next
step
the
top
percent
of
the
list
are
selected
as
keywords
and
so
on
also
in
this
approach
all
of
the
glossary
words
are
selected
as
relevant
keywords
in
all
steps
in
fig
10
the
recall
and
precision
are
plotted
using
mehri
and
darooneh
approach
according
to
this
figure
recall
for
fractality
for
our
method
is
higher
than
other
methods
for
all
retrieved
list
fractions
the
plos
one
doi:10.1371/journal.pone.0130617
june
19
2015
15/18
plos
one
the
fractal
patterns
of
words
in
text
precision
of
fractality
for
our
method
is
higher
than
others
for
retrieved
list
fractions
of
more
than
percent
the
validity
of
our
method
also
extends
to
other
books
the
first
three
minutes
by
steven
weinberg
25
and
brief
history
of
time
by
stephen
hawking
26
the
value
of
recall
for
our
method
is
higher
than
for
value
and
entropy
the
precision
value
obtained
is
higher
than
other
methods
for
cut-off
frequencies
of
more
than
for
weinberg’s
book
and
more
than
in
the
case
of
hawking’s
book
conclusion
the
pattern
of
occurrences
of
word
in
text
can
be
considered
as
fractal
object
with
dimen
sion
between
and
0
we
found
that
words
related
to
the
subject
of
the
text
have
non-uniform
spatial
distributions
and
their
dimensions
are
considerably
less
than
one
in
contrast
the
irrele
vant
words
are
distributed
uniformly
with
dimension
close
to
one
we
introduced
the
con
cept
of
degree
of
fractality
which
measures
the
difference
between
distribution
pattern
of
word
in
the
original
text
and
randomly
shuffled
version
while
in
the
shuffled
texts
all
of
the
words
are
uniformly
distributed
across
the
text
the
original
text
exhibits
clustering
of
impor
tant
words
in
particular
we
used
the
degree
of
fractality
in
combination
with
function
of
fre
quency
for
ranking
words
in
the
origin
of
species
by
charles
darwin
the
top
words
in
the
ranked
list
of
the
words
was
selected
as
the
retrieved
keywords
of
the
text
the
retrieved
list
of
keywords
was
checked
against
the
glossary
of
the
book
for
this
checking
we
used
two
metrics
precision
and
recall
which
are
defined
in
the
context
of
the
binary
classification
analysis
com
pared
with
two
other
representative
methods
in
this
area
the
entropy
and
value
our
approach
is
more
effective
as
method
for
automatic
keyword
extraction
future
work
should
aim
to
examine
the
effectiveness
of
our
method
in
keyword
detection
for
smaller
texts
this
method
could
also
be
applied
to
key-phrase
extraction
finally
the
gen
eral
framework
behind
our
method
could
be
extended
to
explore
the
hidden
secrets
of
genome
for
instance
by
developing
way
for
data
mining
non-coding
dna
appendix
description
of
related
methods
of
word
ranking
value
the
value
method
is
based
on
noticing
distribution
of
the
words
in
text
and
word
cluster
ing
14
to
quantify
the
clustering
of
word
the
parameter
the
standard
deviation
of
the
normalized
distance
between
consecutive
occurrence
of
word
is
defined
by
g=v<2>—<s>2
9
where
is
the
normalized
distance
between
consecutive
occurrences
=d/
>,and
is
the
average
distance
between
occurrences
can
be
normalized
with
respect
to
standard
devi
ation
of
the
distance
between
consecutive
occurrences
of
words
in
random
text
which
has
geometrical
spatial
distribution
of
word
types
6
v/1
p
where
m/n
is
the
probabil
ity
of
occurrence
of
word
type
with
frequency
equal
to
in
text
with
total
words
10
t~
m
o
m
m
nor
plos
one
doi:10.1371/journal.pone.0130617
june
19
2015
16/18
o~
plos
one
the
fractal
patterns
of
words
in
text
where
g
>=2
are
the
mean
value
of
the
normalized
nor
zw+1
nm
v(2
standard
deviation
and
standard
deviation
of
the
distribution
of
0
in
random
text
respec
tively
means
the
word
is
distributed
randomly
in
text
and
means
the
word
forms
cluster
entropy
entropy
is
another
parameter
used
to
rank
the
words
of
text
17
for
this
purpose
text
with
words
is
devided
into
parts
the
ith
part
contains
n
words
which
~
n
n
so
the
rala
i=1
m0
where
m;(w
and
m(w
are
the
frequency
of
word
type
in
the
ith
part
and
in
the
whole
texf
respectively
where
tive
frequency
of
occurrence
of
the
word
type
in
the
part
is
f,(w
>~
m
m
with
this
explanation
the
probability
measure
over
the
partitions
can
be
defined
i=1
as
sfe
2
j=1
the
following
relation
is
the
shannon’s
information
entropy
for
discrete
distribution
p;(w
(
w)zp
jn(p
(
13
there
is
problem
with
this
relation
it
is
zero
for
words
with
frequency
equal
to
1
to
take
into
account
the
effect
of
frequency
the
following
relation
seems
to
be
better
choice
m()[1
§(o
e,(0
e,(0
14
where
e
w
is
the
entropy
of
the
word
type
in
random
text
2
p
acknowledgments
we
acknowledge
valuable
comments
from
referees
which
substantially
improved
the
paper
author
contributions
conceived
and
designed
the
experiments
en
ahd
performed
the
experiments
en
ahd
analyzed
the
data
en
ahd
contributed
reagents/materials/analysis
tools
en
ahd
wrote
the
paper
en
ahd
references
1
larsen-freeman
d
cameron
l
complex
systems
and
applied
linguistics
oxford
oxford
university
press
2008
2
zipf
gk
human
behavior
and
the
principle
of
least
effort
an
introduction
to
human
ecology
cam
bridge
addison-wesley
press
1949
3
heaps
hs
information
retrieval
computational
and
theoretical
aspects
new
york
academic
press
1978
4
sano
takayasu
h
takayasu
m
progress
of
theoretical
physics
supplement
no
194
2012
202
209
plos
one
doi:10.1371/journal.pone.0130617
june
19
2015
17/18
plos
one
the
fractal
patterns
of
words
in
text
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
altmann
g
prolegomena
to
menzerath’s
law
glottometrika
2
1980
110
hrebi¢ek
l
fractals
in
language
journal
of
quantitative
linguistics
1(1
1994
82—86
doi
10.1080/
09296179408590001
andres
j
on
conjecture
about
the
fractal
structure
of
language
journal
of
quantitative
linguistics
17(2
2010
101-122
doi
10.1080/09296171003643189
andres
j
benesova
m
kuba
l
vrbkova
j
methodological
note
on
the
fractal
analysis
of
texts
jour
nal
of
quantitative
linguistics
19(1
2012
1-31
doi
10.1080/09296174.2011.608604
ausloos
m
generalized
hurst
exponent
and
multifractal
function
of
original
and
translated
texts
mapped
into
frequency
and
length
time
series
phys
rev
e
86.2012:031108
doi
10.1103/
physreve.86.031108
ausloos
m
measuring
complexity
with
multifractals
in
texts
translation
effects
chaos
solitons
and
fractals
45.2012
13491357
eftekhari
a
fractal
geometry
of
literature
first
attempt
to
shakespear's
works
journal
of
quantita
tive
linguistics
13.2006
177-193
doi
10.1080/09296170600850106
luhn
hp
the
automatic
creation
of
literature
abstracts
ibm
journal
of
research
and
development
2.1958
159-165
doi
10.1147/rd.22.0159
ortufio
m
carpena
p
bernaola-galvan
p
munoz
e
somoza
am
keyword
detection
in
natural
lan
guages
and
dna
europhysics
letters
57.2002
759-764
doi
10.1209/epl/i2002-00528-3
carpena
p
bernaola-galvan
p
hackenberg
m
coronado
av
oliver
jl
level
statistics
of
words
finding
keywords
in
literary
texts
and
symbolic
sequences
physical
review
79
2009
035102
doi
10.1103/physreve.79.035102
zhou
h
slater
gw
metric
to
search
for
relevant
words
physica
329
2003
309-327
doi
10
1016/s0378-4371(03)00625-3
mihalcea
r
tarau
p
textrank
bringing
order
into
texts
proceedings
of
conference
on
empirical
methods
in
natural
language
processing
emnlp
2004
404—411
herrera
jp
pury
pa
statistical
keyword
detection
in
literary
corpora
eur
phys
j
63
2008
135
doi
10.1140/epjb/e2008-00206-x
mehri
a
darooneh
ah
the
role
of
entropy
in
word
ranking
physica
390.2011
3157-3163
doi
10
1016/j.physa.2011.04.013
mehri
a
darooneh
ah
keyword
extraction
by
nonextensivity
measure
physical
review
83.2011
056106
doi
10.1103/physreve.83.056106
gouyet
jf
physics
and
fractal
structures
new
york
masson
springer
1996
darwin
c
on
the
origin
of
species
by
means
of
natural
selection
or
the
preservation
of
favoured
races
in
the
struggle
for
life
nature
london
john
murray
5
1859
http://www.gutenberg.org/files/22764/
kord
delsame
k
ranking
the
words
in
text
by
value
method
msc
thesis
in
persian
university
of
zanjan
2012
http://literature.org/authors/darwin-charles/the-origin-of-species/glossary.html
weinberg
s
the
first
three
minutes
cambridge
pegasus
press
1949
hawking
s
brief
history
of
time
bantam
books
1988
plos
one
doi:10.1371/journal.pone.0130617
june
19
2015
18/18
