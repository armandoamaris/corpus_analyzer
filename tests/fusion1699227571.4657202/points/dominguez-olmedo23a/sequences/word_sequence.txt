on
data
manifolds
entailed
by
structural
causal
models
ricardo
dominguez-olmedo
amir-hossein
karimi
georgios
arvanitidis>
bernhard
schélkopf
abstract
the
geometric
structure
of
data
is
an
important
inductive
bias
in
machine
learning
in
this
work
we
characterize
the
data
manifolds
entailed
by
structural
causal
models
the
strengths
of
the
proposed
framework
are
twofold
firstly
the
geo
metric
structure
of
the
data
manifolds
is
causally
informed
and
secondly
it
enables
causal
reason
ing
about
the
data
manifolds
in
an
interventional
and
counterfactual
sense
we
showcase
the
ver
satility
of
the
proposed
framework
by
applying
it
to
the
generation
of
causally-grounded
counterfac
tual
explanations
for
machine
learning
classifiers
measuring
distances
along
the
data
manifold
in
differential
geometric-principled
manner
1
introduction
the
manifold
hypothesis
states
that
most
naturally
occur
ring
datasets
lie
near
non-linear
manifold
embedded
in
the
feature
space
hastie
stuetzle
1989
smola
et
al
2001
belkin
niyogi
2003
the
geometric
structure
of
the
data
manifold
is
powerful
inductive
bias
for
variety
of
ma
chine
learning
tasks
such
as
classification
clustering
den
sity
estimation
representation
learning
and
transfer
learn
ing
across
diverse
set
of
application
domains
including
computer
vision
tosi
et
al
2014
arvanitidis
et
al
2018
2021
robotics
scannell
et
al
2021
beik-mohammadi
etal
2021
2022
human
motion
capture
tosi
et
al
2014
and
protein
sequencing
detlefsen
et
al
2022
generative
models
offer
an
appealing
framework
to
approx
imately
learn
the
data
manifold
from
data
arvanitidis
et
al
2018
under
certain
smoothness
conditions
generative
model’s
entailed
data
manifold
is
amenable
to
the
study
of
differential
geometry
this
allows
to
formally
define
set
of
fundamental
operations
on
the
data
manifold
e.g
max
planck
institute
for
intelligent
systems
tiibingen
ger
many
2eth
ziirich
ziirich
switzerland
>technical
university
of
denmark
lyngby
denmark
correspondence
to
ricardo
dominguez-olmedo
<ricardo.olmedo@tuebingen.mpg.de>
proceedings
of
the
40
international
conference
on
machine
learning
honolulu
hawaii
usa
pmlr
202
2023
copyright
2023
by
the
author(s
interpolations
distances
measures
that
are
meaningfully
informed
by
its
geometric
structure
hauberg
2018
in
this
work
we
characterize
the
data
manifolds
entailed
by
structural
causal
models
scms
pearl
2009
scms
are
generative
models
that
besides
modeling
the
data
distribu
tion
incorporate
additionally
knowledge
about
the
causal
relationships
between
the
variables
of
the
modeled
system
importantly
scms
provide
formal
framework
for
reason
ing
about
the
data
distribution
under
interventions
to
the
variables
of
the
system
including
counterfactual
statements
pertaining
to
what
would
have
or
could
have
been
given
that
something
else
was
actually
observed
analogous
to
the
observational
interventional
and
coun
terfactual
distributions
entailed
by
an
scm
we
first
derive
sufficient
conditions
for
an
scm
to
induce
observational
interventional
and
counterfactual
smooth
manifolds
§3
and
we
show
that
scm
classes
with
broad
causal
identifia
bility
results
namely
additive
noise
models
post-nonlinear
models
and
location-scale
noise
models
satisfy
such
suffi
cient
conditions
having
established
the
foundations
for
differential
geometric
study
of
scms
we
secondly
discuss
riemannian
metrics
that
are
informed
by
the
causal
knowl
edge
embedded
in
an
scm
§4.1
thirdly
we
endow
the
smooth
manifolds
induced
by
an
scms
with
riemannian
metric
thus
characterizing
as
riemannian
manifolds
the
observational
interventional
and
counterfactual
data
man
ifolds
entailed
by
an
scm
§4.2.1
this
characterization
allows
us
to
define
operations
on
the
data
manifold
that
are
informed
by
the
causal
structure
of
the
data
lastly
we
leverage
the
proposed
framework
to
generate
counterfactual
explanations
for
machine
learning
classifiers
we
propose
methods
to
generate
counterfactual
explanations
that
are
both
causally
grounded
and
close
to
the
data
mani
fold
§5
in
contrast
to
previous
approaches
we
measure
distances
along
the
data
manifold
in
differential
geometric
principled
manner
we
demonstrate
the
effectiveness
of
the
proposed
methods
on
two
real-world
datasets
§6
2
background
2.1
structural
causal
models
structural
causal
model
scm
s
py
pearl
2009
over
aset
{x1
x4}
of
random
variables
on
data
manifolds
entailed
by
structural
causal
models
the
endogenous
variables
consists
of
1
asets
{xi
f
xpﬂ(i
ui)}idzl
of
structural
assignments
each
describing
as
function
f
the
causal
relationship
between
variable
x
its
direct
causal
parents
x
(
and
random
variable
u
representing
unobserved
background
factors
of
variation
ii
joint
distribution
py
us
.
uqg
over
the
set
of
exogenous
noise
variables
{u1
.
uq}
we
henceforth
make
two
common
assumptions
assumption
2.1
acyclicity
the
causal
graph
implied
by
the
structural
assignments
s
with
nodes
and
edges
{(v,x
xpuy
uyl
is
acyclic
under
acyclicity
each
realization
of
the
exogenous
vari
ables
entails
unique
realization
f(u
of
the
endoge
nous
variables
x
where
is
the
reduced-form
mapping
obtained
by
recursive
substitution
of
the
structural
assign
ments
in
topological
order
of
the
causal
graph
g
the
scm
then
entails
unique
joint
distribution
px
over
the
endogenous
variables
x
i.e
the
observational
distribution
definition
2.2
entailed
distribution
the
entailed
distri
bution
px
of
an
scm
=
s
py
is
the
pushforward
measure
of
py
through
the
reduced-form
mapping
of
px(x
=z):=
py(u=f"(x
1
where
f~1
is
the
preimage
of
the
reduced-form
mapping
f
we
additionally
assume
that
there
are
no
hidden
confounders
causally
affecting
more
than
one
of
the
observables
x
assumption
2.3
causal
sufficiency
the
exogenous
vari
ables
uy
.
up
are
independent
that
is
their
distribution
factorizes
as
py
py
.
py
interventions
scms
allow
for
modeling
and
evaluating
the
effect
of
external
manipulation
of
the
system
modeled
by
the
scm
pearl
2009
interventions
are
modeled
by
modifying
some
of
the
structural
assignments
resulting
in
modified
scm
m
s
py
its
entailed
distribution
p)j
is
then
an
interventional
distribution
hard
interven
tions
=
do(xz
6
pearl
2009
fix
the
values
of
asubset
{1,...,d}
of
the
endogenous
variables
to
some
given
value
rizi
such
that
s7
=xz
=0
for
i€{l,...,|z|}
and
s}
=
s
vi
t
notably
hard
inter
ventions
sever
the
causal
relationship
between
intervened
upon
variables
and
all
their
causal
ancestors
counterfactuals
scms
offer
principled
framework
to
reason
about
counterfactuals
that
is
what
would
have
hap
pened
under
certain
hypothetical
interventions
all
else
being
equal
pearl
2009
formally
the
counterfactual
distri
bution
p)jq;z
pertaining
to
some
observation
under
some
hypothetical
intervention
is
defined
to
be
the
entailed
dis
tribution
of
the
modified
scm
m’-¥=
=
s%
py
where
py
is
posterior
over
given
the
observed
z
es
sentially
the
posterior
uz
amounts
to
the
background
con
ditions
likely
to
have
resulted
in
the
observation
z
which
are
propagated
through
the
intervened-upon
structural
as
signments
s
under
some
hard
intervention
z
if
is
invertible
then
each
observable
is
mapped
to
exactly
one
counterfactual
°f
since
the
posterior
u|z
collapses
to
single
realization
of
the
exogenous
variables
for
hypo
thetical
intervention
j
we
denote
the
map
from
factuals
to
counterfactuals
as
z°f
cf(z,j
causal
identifiability
identifying
the
true
causal
relation
ships
between
variables
i.e
the
causal
graph
g
from
obser
vational
data
alone
is
in
general
not
possible
without
further
assumptions
such
as
restrictions
on
the
function
class
of
the
structural
assignments
s
classes
of
scms
identifiable
from
observational
data
alone
include
additive
noise
models
of
the
form
s
=
fi(xpu(s
us
peters
et
al
2017
post-nonlinear
models
zhang
hyvirinen
2009
and
location-scale
noise
models
immer
et
al
2022
2.2
riemannian
manifolds
d-dimensional
smooth
manifold
is
topological
space
which
locally
resembles
r
and
has
smooth
structure
riemannian
manifold
do
carmo
1992
is
smooth
mani
fold
equipped
with
riemannian
metric
definition
2.4
riemannian
metric
riemannian
met
ricm
si
is
smooth
function
that
assigns
symmetric
positive
definite
matrix
to
any
point
in
m
intuitively
the
riemannian
metric
defines
an
infinitesimal
notion
of
distance
on
the
manifold
m
thus
endowing
the
manifold
with
particular
metric
structure
the
length
of
smooth
curve
0
1
on
is
then
defined
as
o
vo
mamsma
where
7(t
=
%7(1%
denotes
the
velocity
of
the
curve
natural
notion
of
interpolation
between
two
points
p
on
the
manifold
is
the
shortest
curve
on
that
connects
and
¢
distances
between
points
on
the
manifold
are
then
defined
as
length
of
the
shortest
curve
connecting
them
definition
2.5
riemannian
distance
the
distance
dm(p
q
between
two
points
p,q
on
riemannian
manifold
m
m
is
defined
as
the
infimum
of
the
length
of
all
smooth
curves
0,1
connecting
and
dm(p,q
inf{l(y
7(0
=p
v(1)=q}
3
both
for
non-gaussian
py
and
for
non-linear
fi
on
data
manifolds
entailed
by
structural
causal
models
the
riemannian
volume
measure
the
riemannian
vol
ume
measure
vol
p
=
/det
m(p
provides
measure
of
the
magnitude
of
the
local
distortion
of
space
at
m
curves
which
traverse
regions
of
the
manifold
with
large
volume
measure
tend
to
have
large
lengths
and
shortest
paths
between
points
of
the
manifold
tend
to
avoid
if
possi
ble
such
regions
with
large
volume
measure
the
pullback
metric
let
be
smooth
mapping
between
two
smooth
manifolds
w
m
and
let
be
equipped
with
riemannian
metric
m
then
the
metric
can
be
pulled
back
to
via
the
pullback
metric
w
=
dg(w
m(p(w
dd(w
4
where
sz(u
is
the
jacobian
of
at
w
via
the
pull
back
w
the
manifold
}v
inherits
the
infinitesimal
notion
of
distance
of
the
manifold
m
m
if
is
an
immersion
then
it
holds
that
is
riemannian
metric
if
and
only
if
is
additionally
injective
i.e
diffeomorphism
then
is
an
isometry
and
the
manifolds
w
w
and
m
m
induce
identical
riemannian
distances
dw
p,q
dm(8(p
q
5
conformal
equivalence
two
metrics
m
m
in
are
conformally
equivalent
if
there
exists
some
smooth
function
a
0,00
such
that
m'(p
a(p)m(p
vp
m
the
function
is
commondly
denoted
as
the
conformal
factor
intuitively
and
are
identical
up
to
scale
3
scms
entail
smooth
manifolds
in
this
section
we
present
sufficient
conditions
for
an
scm
to
induce
observational
interventional
and
counterfactual
smooth
manifolds
we
will
then
equip
these
entailed
smooth
manifolds
with
suitable
riemannian
metric
in
order
to
characterize
the
entailed
data
manifolds
of
an
scm
§4
our
starting
point
of
study
is
the
exogenous
space
u
that
is
the
space
of
realizations
of
the
exogenous
variables
u
we
argue
that
the
exogenous
space
is
smooth
manifold
for
typical
modeling
choices
of
the
exogenous
distribution
py
in
particular
under
causal
sufficiency
it
suffices
that
the
support
of
every
marginal
py
is
smooth
manifold
observation
3.1
under
causal
sufficiency
if
the
support
of
every
marginal
py
is
d;-dimensional
smooth
manifold
then
the
exogenous
space
is
d-dimensional
smooth
manifold
where
=y
d
for
scms
with
real-valued
variables
typical
modeling
choices
of
the
marginal
distributions
py
e.g
gaussian
2the
rank
of
the
jacobian
d¢(g
of
at
every
point
is
equal
to
the
dimensionality
of
the
manifold
y
this
ensures
that
the
pullback
metric
induces
positive-definite
inner
product
alternatively
both
and
its
inverse
~
are
differentiable
or
gamma
distributions
satisfy
the
conditions
of
observa
tion
3.1
since
their
support
is
non-empty
open
interval
of
r
which
is
trivially
1-dimensional
smooth
manifold
consequently
we
argue
that
in
practice
the
exogenous
space
generally
admits
differential
geometric
treatment
we
will
now
consider
the
endogenous
space
x
3.1
the
endogenous
space
x
for
acyclic
scms
the
endogenous
space
x
i.e
the
space
of
realizations
of
the
endogenous
variables
x
is
precisely
the
image
of
the
exogenous
space
through
the
reduced
form
mapping
of
the
scm
that
is
x
=
f(u
we
now
present
sufficient
conditions
under
which
the
endogenous
space
x
is
smooth
manifold
lemma
3.2
under
acyclicity
if
for
all
structural
as
signments
it
holds
that
f
is
differentiable
and
its
partial
derivative
dy
fi(xpa(i
ui
is
nonvanishing
inu
then
the
reduced-form
map
is
an
immersion
lemma
3.3
under
acyclicity
if
forall
{1,...,d}
and
all
w™m,u
such
that
uil
52
it
holds
that
fi
%aa(z)«,?lil
fi
%za(z)«,?lii
©
then
the
reduced-form
mapping
is
injective
proposition
3.4
under
the
conditions
of
lemma
3.2
and
lemma
3.3
if
the
exogenous
space
is
smooth
manifold
then
the
endogenous
space
=
f(u
is
smooth
manifold
and
the
map
is
diffeomorphism
corollary
3.5
for
additive
noise
models
post-nonlinear
models
and
location-scale
noise
models
if
the
exogenous
space
is
smooth
manifold
and
the
structural
assignments
are
differentiable
then
the
endogenous
space
=
f(u
is
smooth
manifold
and
is
diffeomorphism
note
that
the
condition
of
lemma
3.3
is
weaker
than
injec
tivity
of
every
f
since
the
functions
f
are
only
required
to
be
injective
with
respect
to
u
corollary
3.5
is
signifi
cant
result
since
additive
noise
models
and
post-nonlinear
models
are
precisely
the
classes
of
scms
with
strong
causal
identifiability
guarantees
§2.1
note
that
the
differentia
bility
condition
in
corollary
3.5
is
generally
required
to
establish
causal
identifiability
peters
et
al
2017
zhang
hyvirinen
2009
and
in
that
sense
amounts
to
relatively
mild
condition
on
the
structural
assignments
of
the
scm
we
have
so
far
presented
sufficient
conditions
for
the
en
dogenous
space
induced
by
an
scm
to
be
smooth
manifold
and
showed
that
particularly
notable
classes
of
scms
satisty
such
conditions
analogous
to
the
interven
tional
and
counterfactual
distributions
entailed
by
an
scm
we
now
present
sufficient
conditions
for
an
scm
to
entail
interventional
and
counterfactual
smooth
manifolds
on
data
manifolds
entailed
by
structural
causal
models
3.2
interventional
smooth
manifolds
an
intervention
to
some
scm
s
py
results
in
modified
scm
m
s
py
similarly
to
§3.1
we
de
fine
the
endogenous
space
x7
under
an
intervention
as
the
image
of
the
modified
exogenous
space
u
=
supp(p}
through
the
reduced-form
mapping
f
of
the
intervened
upon
structural
assignments
s
that
is
x7
=
f7(u°
if
the
modified
scm
satisfies
the
conditions
of
proposi
tion
3.4
the
interventional
space
x
is
smooth
manifold
which
denote
as
an
interventional
manifold
hard
interventions
=
do(xz
6
are
arguably
the
most
common
modeling
choice
for
interventions
we
show
that
for
hard
interventions
j
the
interventional
space
x7
is
smooth
manifold
under
strictly
weaker
conditions
than
those
presented
in
§3.1
for
the
observational
setting
proposition
3.6
let
=
do(xz
0
be
hard
in
tervention
on
the
variables
xz
if
the
structural
assign
ments
f
yj
satisfy
the
conditions
of
lemma
3.2
and
lemma
3.3
and
u
is
smooth
manifold
then
in
terventional
space
=
f>(u
is
d
m)-dimensional
smooth
manifold
embedded
in
r<
where
is
the
number
of
endogenous
variables
and
=
|i|
is
the
number
of
intervened-upon
variables
additionally
the
reduced-form
mapping
{7
u
x7
is
diffeomorphism
corollary
3.7
if
an
scm
satisfies
the
conditions
of
proposition
3.4
then
entails
interventional
smooth
mani
folds
x
under
hard
interventions
=
do(xz
0
consequently
when
considering
hard
interventions
the
in
terventional
space
x7
admits
differential
geometric
study
without
requiring
additional
assumptions
compared
to
the
observational
setting
discussed
in
§3.1
3.3
counterfactual
smooth
manifolds
the
sufficient
conditions
presented
in
§3.1
and
§3.2
im
ply
that
the
reduced-form
mapping
is
invertible
then
given
some
observation
x
abduction
results
in
the
posterior
over
exogenous
variables
uz
collapsing
to
single
real
ization
f~!(x
consequently
under
non-stochastic
interventions
i.e
hard
interventions
the
entailed
coun
terfactual
distributions
collapse
to
single
realizations
of
the
endogenous
variables
zcf
in
order
to
meaningfully
define
notion
of
counterfactual
manifolds
given
that
counterfactual
distributions
collapse
to
single
counterfactuals
we
instead
consider
counterfactuals
under
space
of
interventions
h
we
argue
that
such
counterfactual
manifolds
commonly
arise
in
counterfactual
reasoning
when
considering
the
ef
fects
of
competing
hypothetical
interventions
such
as
with
the
query
what
dietary
intervention
would
most
favorably
improve
the
health
outcomes
of
some
particular
individual
for
simplicity
we
restrict
our
analysis
to
spaces
of
hard
in
terventions
of
the
form
=
{do(xz
0
a}
as
introduced
in
§2.1
for
any
given
scm
let
us
denote
by
cf
the
mapping
between
factual
and
counterfactuals
for
some
hard
intervention
=
do(xz
6
such
that
zf
cf(x,j
we
define
the
space
of
counterfactuals
x7®
for
some
observable
under
some
space
of
interventions
h
as
the
image
of
#h
through
the
counterfactual
mapping
cf
that
is
x7
=
cf(x
h
we
now
present
sufficient
conditions
on
for
x7i
to
be
smooth
manifold
proposition
3.8
let
=
{do(xz
=0
|0
a}
be
space
of
hard
interventions
on
the
variables
x1
and
let
be
the
causal
descendants
of
xz
excluding
xz
under
the
conditions
of
lemma
3.3
injectivity
of
f
if
the
structural
assignments
f
corresponding
to
the
causal
descendants
are
differentiable
and
is
m-dimensional
smooth
manifold
then
the
counterfactual
space
x%
=
cf(x,h
is
an
m-dimensional
smooth
manifold
and
the
mapping
cf(z
x2
diffeomorphism
corollary
3.9
if
an
scm
satisfies
the
conditions
of
proposition
3.4
then
entails
counterfactual
smooth
man
ifolds
x%
under
spaces
of
hard
interventions
=
{do(xz
0
a}
where
is
smooth
manifold
note
that
only
the
causal
descendants
are
required
to
have
differentiable
structural
assignments
and
that
there
are
no
requirements
on
the
exogenous
variables
u
in
particular
the
causal
ancestors
of
the
intervened-upon
variables
need
not
be
real-valued
for
the
space
of
counterfactuals
to
admit
differential
geometric
treatment
such
scenarios
are
com
mon
in
socioeconomic
settings
where
root
variables
often
include
categorical
variables
e.g
gender
or
nationality
analogously
to
the
interventional
setting
when
considering
hard
interventions
the
counterfactual
space
x”*1
admits
differential
geometric
study
without
requiring
additional
assumptions
compared
to
the
observational
setting
§3.1
4
scms
entail
data
manifolds
in
the
previous
section
we
derived
sufficient
conditions
on
an
scm
such
that
it
induces
observational
interventional
and
counterfactual
smooth
manifolds
in
this
section
we
discuss
the
inductive
biases
of
different
riemannian
metrics
informed
by
the
scm
such
metrics
allow
us
to
endow
the
aforementioned
smooth
manifolds
with
metric
structures
that
are
meaningfully
informed
by
the
causal
structure
of
the
data
we
then
characterize
as
riemannian
manifolds
the
observational
interventional
and
counterfactual
data
manifolds
entailed
by
an
scm
4.1
riemannian
metrics
and
their
inductive
biases
insofar
as
geometric
judgments
are
hypotheses
about
the
world
riemann
1868
the
choice
of
riemannian
metric
is
fundamental
modeling
tool
towards
encoding
appropriate
inductive
biases
for
the
system
being
modeled
prior
works
on
data
manifolds
entailed
by
structural
causal
models
in
manifold
learning
typically
considers
locally
euclidean
metrics
which
are
then
regularized
to
have
large
volume
measure
in
regions
of
the
feature
space
far
away
from
the
ob
served
data
hauberg
et
al
2012
tosi
et
al
2014
hauberg
2018
arvanitidis
et
al
2016
2018
2022
such
model
ing
choice
encodes
the
inductive
bias
that
shortest
paths
on
the
data
manifold
should
remain
close
to
the
observed
data
since
curves
crossing
regions
of
the
feature
space
with
low
data
density
will
then
necessarily
have
large
length
4.1.1
locally
euclidean
metrics
the
sufficient
conditions
presented
in
§3
establish
an
isom
etry
between
the
exogenous
space
and
the
endogenous
space
x
consequently
for
any
riemannian
metric
m
resp
y
defined
in
resp
x
there
exists
an
equiva
lent
metric
in
x
resp
i/
defined
by
the
pushforward
resp
pullback
via
the
reduced-form
map
of
the
structural
as
signments
of
the
scm
such
that
x
resp
uf
inherits
the
infinitesimal
notion
of
distance
defined
by
m
resp
x
locally
euclidean
my
=
encodes
the
inductive
bias
that
background
conditions
i.e
the
exogenous
variables
u
should
be
considered
similar
if
they
lead
to
similar
ob
servations
i.e
the
endogenous
variables
x
in
locally
euclidean
sense
loosely
the
resulting
metric
structure
places
more
weight
on
differences
in
outcomes
rather
than
differences
in
causes
such
choice
of
metric
is
addition
ally
well-justified
for
scms
whose
exogenous
variables
merely
encode
the
stochasticity
in
the
relation
between
causal
variables
but
their
numerical
values
are
arbitrary
e.g
the
model
would
be
equally
useful
under
elementwise
reparametrization
of
the
exogenous
variables
importantly
pulling
back
m.y
allows
us
to
define
metric
in
the
exoge
nous
space
that
is
grounded
on
the
observed
space
x
and
that
is
invariant
to
diffeomorphic
reparametrizations
of
/
locally
euclidean
m
=
encodes
the
inductive
bias
that
the
similarity
of
observables
should
be
measured
in
terms
of
the
similarity
of
the
background
conditions
which
gave
rise
to
said
observables
in
locally
euclidean
sense
intuitively
if
the
differences
in
income
and
savings
of
two
individuals
can
be
explained
solely
due
to
their
differ
ence
in
income
which
causally
affects
savings
then
their
dissimilarity
may
be
smaller
than
if
we
were
to
con
sider
two
individuals
for
which
their
differences
cannot
be
explained
in
terms
of
single
common
cause
however
we
emphasize
that
for
m
=
to
be
meaningful
metric
the
exogenous
variables
themselves
must
be
intrinsically
mean
ingful
i.e
not
merely
device
to
encode
the
stochasticity
in
the
relation
between
causal
variables
one
prominient
class
of
models
for
which
exogenous
variables
may
be
in
trinsically
meaningful
are
additive
noise
models
where
they
indicate
the
deviation
of
an
observed
variable
from
its
ex
pected
state
given
its
observed
causal
parents
4.1.2
regularizing
the
riemannian
metric
as
stated
previously
prior
works
in
manifold
learning
regu
larize
the
metric
to
have
large
volume
measure
in
regions
of
the
feature
space
with
low
data
density
scms
model
the
probability
distribution
px
of
the
endogenous
variables
formally
defined
as
the
pushforward
measure
of
py
through
the
reduced-form
mapping
of
the
scm
we
draw
in
spiration
from
arvanitidis
et
al
2022
and
propose
to
scale
the
riemannian
metric
my
by
the
conformal
factor
mx
7)==
a
px(z
b)~%/4
where
px
is
the
density
of
px
and
«
upper
and
lower
bound
ax
the
confor
mal
factor
a\x
scales
the
volume
measure
of
the
manifold
inversely
proportionally
to
the
data
density
px
such
that
volyt
(
px(@
volyxmy
z
the
parameters
and
/3
determine
the
local
curvature
of
the
manifold
as
function
of
the
data
density
px
determining
how
strongly
shortest
paths
are
pulled
towards
regions
of
the
space
with
large
data
density
the
values
of
and
are
consequently
further
modeling
choice
with
which
practitioners
may
encode
appropriate
inductive
biases
4.2
data
manifolds
entailed
by
scms
we
now
characterize
the
data
manifolds
entailed
by
an
scm
by
equipping
the
smooth
manifolds
described
in
§3
with
riemannian
metric
which
is
regularized
by
the
con
formal
factor
motivated
in
§4.1.2
note
that
we
make
no
assumptions
on
and
treat
it
as
modeling
choice
definition
4.1
entailed
data
manifold
an
entailed
data
manifold
of
an
scm
=
s
py
is
riemmanian
mani
fold
x
ax
m
comprised
of
smooth
manifold
=
f(u
the
endogenous
space
equipped
with
riemannian
metric
scaled
by
some
conformal
factor
ax
ax
a-px(z
b)"h4
a,>0
8
where
px
is
the
density
of
the
probability
distribution
px
entailed
by
the
scm
m
analogous
to
an
scm’s
entailed
distribution
defini
tion
2.2
it
is
possible
to
reason
about
an
scm’s
inter
ventional
and
counterfactual
data
manifolds
4.2.1
interventional
data
manifolds
we
define
an
scm’s
interventional
data
manifold
under
some
intervention
as
an
entailed
manifold
of
the
modified
scm
mz
87
py
in
particular
such
interventional
data
manifold
is
comprised
of
the
interventional
smooth
on
data
manifolds
entailed
by
structural
causal
models
manifold
x7
=
f?(u
equipped
with
riemannian
met
ric
m
scaled
by
some
conformal
factor
a%
inversely
pro
portional
to
the
density
of
the
interventional
distribution
p
note
that
an
intervention
alters
the
observational
data
manifold
in
multiple
ways
firstly
by
modifying
the
space
of
possible
realizations
x7
of
the
endogenous
variables
secondly
by
modifying
the
distribution
over
observations
p,j
i.e
the
contents
of
the
interventional
space
which
curves
the
data
manifold
and
thirdly
by
modifying
the
riemannian
metric
m
which
endows
the
manifold
with
an
infinitesimal
notion
of
distance
e.g
when
considering
the
pushforward
metric
through
the
modified
reduced
map
f7
4.2.2
counterfactual
data
manifolds
we
define
an
scm’s
entailed
counterfactual
data
manifold
by
endowing
the
counterfactual
smooth
manifolds
x7l
introduced
in
§3.3
with
riemannian
metric
m”71
note
that
as
discussed
in
§3.3
since
we
assume
the
reduced-form
mapping
to
be
invertible
the
entailed
counterfactual
dis
tributions
of
the
scm
collapse
to
single
counterfactuals
we
argue
that
suitable
alternative
may
be
to
simply
consider
the
conformal
factor
ax
corresponding
to
the
observational
data
density
px
if
the
counterfactual
space
x7i
is
subset
of
the
support
of
px
that
is
the
if
the
hypothetical
counter
factuals
are
somewhat
consistent
with
the
data
distribution
we
argue
that
such
choice
of
conformal
factor
is
particularly
well
justified
for
systems
where
the
observed
distribution
px
is
snapshot
of
some
process
in
which
individual
units
naturally
experience
interventions
such
as
socioeconomic
systems
for
instance
people
regularly
ex
perience
or
decide
on
interventions
such
as
changes
in
their
employment
status
in
such
settings
we
argue
that
the
data
distribution
implicitly
contains
some
counterfactual
information
for
instance
if
the
counterfactual
zcf
of
some
observed
under
an
intervention
has
arbitrarily
small
den
sity
px
(
then
it
might
be
likely
that
the
intervention
on
the
individual
is
not
realistically
feasible
5
implications
for
counterfactual
explanations
machine
learning
classifiers
are
increasingly
being
deployed
in
consequential
decision-making
settings
prior
work
has
argued
that
for
such
systems
to
be
trustworthy
algorithmic
decisions
should
be
accompanied
by
an
explanation
such
that
individuals
are
able
to
understand
and
possibly
contest
such
decisions
wachter
et
al
2017
venkatasubramanian
alfano
2020
counterfactual
explanations
cfes
have
gained
much
popularity
in
recent
years
as
cfes
are
gener
ally
thought
to
be
both
easily
comprehensible
and
compliant
with
some
regulatory
frameworks
wachter
et
al
2017
despite
their
name
counterfactual
explanations
in
ml
are
generally
not
counterfactuals
in
the
causal
sense
of
pearl
2009
consider
the
setting
where
classifier
{0,1}
is
used
to
assign
either
favourable
or
unfavourable
outcomes
to
individuals
x
for
negatively
classified
individual
z
counterfactual
explanations
seek
to
explain
the
classifier’s
decision
by
searching
for
the
closest
individual
2
that
would
have
been
favourably
classified
that
is
argmin
d(z,z
ex
s.t
h(a
=1
the
distance
function
0,00
amounts
to
modeling
choice
encoding
the
set
of
desiderata
as
to
what
amounts
to
an
effective
counterfactual
explanation
z
we
focus
on
two
desiderata
that
have
received
much
attention
in
the
literature
counterfactual
explanations
should
be
realistic
i.e
well
supported
by
the
observed
data
and
consistent
with
the
underlying
causal
structure
of
the
world
verma
et
al
2020
karimi
et
al
2022
realistic
cfes
prior
works
argue
that
for
cfe
2
to
be
realistic
there
must
exist
some
plausible
path
of
change
between
the
negatively
classified
individual
and
the
of
fered
cfe
2
joshi
et
al
2019
poyiadzi
et
al
2020
this
desideratum
is
particularly
notable
if
the
intent
of
the
cfe
is
not
only
to
aid
the
understanding
of
the
classifier’s
decision
but
also
to
inform
individuals
of
what
features
they
can
real
istically
change
in
order
to
obtain
favourable
classification
in
the
future
setting
known
as
algorithmic
recourse
ar
for
instance
it
may
not
be
reasonable
to
recommend
to
an
unsuccessful
life
insurance
applicant
to
play
more
sports
if
said
applicant
is
physically
disabled
poyiadzi
et
al
2020
while
prior
works
aim
to
model
the
data
manifold
in
order
to
search
for
realistic
cfes
joshi
et
al
2019
mahajan
et
al
2019
pawelczyk
et
al
2020
downs
et
al
2020
poyiadzi
et
al
2020
antoran
et
al
2021
none
adopt
principled
differential
geometric
approach
in
contrast
we
propose
to
model
the
data
manifold
as
riemannian
manifold
and
to
consider
riemannian
distances
along
the
manifold
as
the
distance
function
d
such
principled
approach
ensures
that
there
exists
some
minimally
costly
path
of
change
i.e
shortest
curve
connecting
an
individual
and
the
counter
factual
example
2
offered
to
them
causally
grounded
cfes
prior
works
have
proposed
methods
to
generate
counterfactual
examples
within
the
rig
orous
causality
framework
of
pearl
2009
in
particular
karimi
et
al
2020
2021
dominguez-olmedo
et
al
2022
search
for
set
of
hypothetical
hard
interventions
on
the
features
of
the
individual
such
that
the
resulting
coun
terfactual
would
be
favourably
classified
in
contrast
von
kiigelgen
et
al
2023
argue
for
backtracking
counterfac
tuals
non-interventional
notion
of
causal
counterfactuals
where
one
instead
searches
over
different
background
con
tions
i.e
exogenous
variables
that
would
have
given
rise
on
data
manifolds
entailed
by
structural
causal
models
to
favourably
classified
counterfactual
z
however
nei
ther
of
the
two
approaches
consider
the
underlying
data
manifold
when
generating
counterfactual
explanations
we
propose
to
leverage
scms
entailed
data
manifolds
§4
to
generate
counterfactual
examples
that
both
respect
the
underlying
data
manifold
and
are
causally
grounded
we
present
methods
to
generate
observational
counterfactual
examples
i.e
backtracking
and
interventional
counter
factual
examples
i.e
causal
algorithmic
recourse
5.1
backtracking
cfes
on
the
data
manifold
backtracking
involves
reasoning
about
counterfactuals
by
varying
the
exogenous
variables
uz
i.e
the
background
conditions
that
plausibly
gave
rise
to
some
observation
x
without
altering
the
structural
assignments
s
since
is
not
intervened-upon
backtracking
counterfactuals
are
also
know
as
observational
counterfactuals
in
contrast
to
the
interventional
counterfactuals
of
pearl
2009
we
assume
access
to
know
scm
s
py
over
the
features
used
for
the
classification
of
individuals
x
we
additionally
assume
that
the
scm
satisfies
the
sufficient
conditions
of
proposition
3.4
such
that
the
endogenous
space
x
is
smooth
manifold
the
reduced
form
mapping
is
then
invertible
and
searching
for
backtracking
cfes
as
formalized
by
von
kiigelgen
et
al
2023
reduces
to
the
following
optimization
problem
min
d(f7(2),u
st
h(f(u
=1
where
is
negatively
classified
individual
for
which
we
seek
an
explanation
is
the
decision-making
classifier
d:u
xu
0,00
is
distance
function
on
the
exoge
nous
space
u
and
2
=
f(u
is
the
backtracking
cfe
we
propose
to
instead
search
for
backtracking
counterfactual
examples
along
the
data
manifold
induced
by
the
scm
m
since
by
assumption
the
reduced-form
map
is
diffeomor
phism
the
exogenous
space
i/
and
the
endogenous
space
are
isomorphic
for
any
riemannian
metric
in
/x
there
exists
an
equivalent
riemannian
metric
in
/
and
vice
versa
differential
geometric
viewpoint
thus
provides
new
per
spective
on
the
extent
to
which
backtracking
counterfactuals
are
observational
searching
for
counterfactuals
across
the
observed
features
x
and
across
the
exogenous
space
is
equivalent
for
equivalent
choices
of
metric
10
without
loss
of
generally
we
search
for
backtracking
coun
terfactuals
along
the
exogenous
space
u
for
some
choice
of
metric
si+4
we
note
that
as
discussed
in
§4.1.1
the
exogenous
space
&/
may
lack
an
intrinsically
meaningful
metric
structure
and
it
might
be
appropriate
to
consider
the
pullback
of
metric
defined
on
x
we
scale
the
metric
with
conformal
factor
ay
inversely
proportional
to
the
density
of
py
as
motivated
in
§4.1.2
consequently
searching
for
counterfactual
examples
along
the
data
man
ifold
entailed
by
the
scm
is
equivalent
to
solving
the
following
optimization
problem
min
dyom
f
(
u
a1
s.t
h(f(uw
=1
where
)
is
the
riemannian
distance
function
induced
by
the
riemannian
manifold
u
\um
5.2
causal
algorithmic
recourse
on
the
data
manifold
causal
algorithmic
recourse
models
recourse
recommenda
tions
as
hard
interventions
on
some
subset
x7
of
the
fea
tures
of
individuals
x
thus
reasoning
in
causally
principled
manner
about
the
downstream
causal
effects
of
the
recourse
recommendations
offered
to
individuals
let
=
{do(xz
=0
a}
be
the
set
of
hard
interven
tions
actionable
for
some
negatively
classified
individual
x
the
causal
algorithm
recourse
problem
is
formalized
as
the
following
optimization
problem
karimi
et
al
2021
min
d(z,cf(z,7
jeh
12
s.t
h(cf(z,7
=1
where
cf(z
-
x7u%
denotes
the
mapping
between
factuals
and
counterfactuals
under
hard
interventions
on
x7
and
typically
z
cf(z
do(xz
0
||zz
0||
we
propose
to
instead
search
for
recourse
interventions
along
counterfactual
data
manifold
entailed
by
the
scm
m
we
assume
that
satisfies
the
conditions
stated
in
proposition
3.8
such
that
the
space
of
counterfactuals
x'7
is
smooth
manifold
for
some
appropriate
choice
of
met
ric
x%ue
s
we
propose
to
scale
such
metric
by
conformal
factor
ax
inversely
proportional
to
the
den
sity
of
the
observational
distribution
px
as
motivated
in
§4.2.2
we
then
consider
the
pullback
metric
m
of
ax
via
the
counterfactual
mapping
cf(z
consequently
we
formalize
the
search
for
algorithmic
recourse
along
the
counterfactual
data
manifold
entailed
by
the
scm
as
min
dwy
2
cf(z,7
jeeh
13
s.t
h(cf(z,7
=1
where
dpys
is
the
riemannian
distance
function
induced
by
the
riemannian
manifold
x
m
and
m
is
the
pullback
metric
of
ax
via
the
mapping
cf(z
-
6
experiments
we
evaluate
the
methods
proposed
in
§5
against
variety
of
previously
proposed
counterfactual
explanation
methods
on
data
manifolds
entailed
by
structural
causal
models
we
open
source
our
implementation
and
experiments®
we
consider
the
following
prior
art
wachter
et
al
2017
considers
the
objective
function
mins
|||
4+
£(h(xz
§),1
where
is
the
cross
entropy
loss
and
is
gradually
annealed
revise
joshi
et
al
2019
similar
wachter
et
al
2017
but
the
optimization
problem
is
solved
in
the
latent
space
of
vae
trained
to
reconstruct
the
data
face
poyiadzi
et
al
2020
searches
for
the
clos
est
counterfactual
along
weighted
nearest
neighbour
graph
constructed
from
the
observed
data
causal
recourse
karimi
et
al
2021
solves
the
opti
mization
problem
presented
in
equation
11
backtracking
counterfactuals
von
kiigelgen
et
al
2023
solves
the
optimization
problem
presented
in
equation
10
we
consider
euclidean
distances
in
/
for
cfes
we
include
revise
as
representative
method
of
the
several
vae-based
approaches
in
the
literature
for
realistic
cfe
generation
we
include
face
because
it
con
siders
some
approximate
notion
of
distance
along
the
data
manifold
via
shortest
paths
in
nearest
neighbour
graph
for
causal
ar
we
consider
the
standard
approach
of
karimi
et
al
2021
for
our
proposed
approach
we
consider
both
the
setting
where
is
assumed
locally
euclidean
ours-u
and
the
setting
where
x
is
locally
euclidean
ours-x
optimizing
along
the
data
manifold
we
solve
the
opti
mization
problems
in
equation
11
and
equation
13
using
gradient
descent
we
compute
riemannian
distances
by
solving
for
the
geodesic
v
0,1
which
con
nects
the
two
points
of
interest
v*(0
ug
v*(1
uy
such
that
dig
ug
u1
=
£(7
we
compute
the
geodesic
~
by
solving
the
boundary
value
problem
bvp
=90y
¥(0
=uo
v(1
=w
14
where
is
system
of
ordinary
differential
equations
deter
mined
by
the
riemannian
metric
of
the
manifold
do
carmo
1992
we
use
the
versatile
automatic
differentiation
sys
tem
of
jax
bradbury
et
al
2018
to
differentiate
through
the
boundary
conditions
of
the
bvp
which
we
solve
using
fourth
order
collocation
algorithm
with
residual
control
similar
to
kierzenka
shampine
2001
scms
and
datasets
we
consider
two
real-world
data
sets
the
compas
recidivism
dataset
larson
et
al
2016
and
the
adult
demographic
dataset
kohavi
becker
1996
for
which
we
assume
the
causal
graphs
presented
in
nabi
shpitser
2018
we
assume
additive
noise
model
scms
>https://github.com/ricardodominguez/data-manifolds-scms
and
we
regress
the
structural
equations
using
mlps
with
one
hidden
layer
we
model
the
probability
density
of
the
residuals
i.e
u
using
kernel
density
estimation
evaluation
metrics
we
evaluate
the
counterfactuals
gen
erated
by
each
of
the
methods
with
the
following
metrics
lo
{5
distance
between
the
factual
and
counterfactual
or
norm
of
the
recommended
feature
change
||4
ly
resp
ly
riemannian
distance
induced
by
the
data
manifold
entailed
by
the
scm
m
where
the
rie
mannian
metric
is
locally
euclidean
in
/
resp
x
and
scaled
by
conformal
factor
ay
resp
ax
inversely
proportional
to
the
density
of
py
resp
px
riemannian
distance
induced
by
data
manifold
constructed
using
kernel
density
estimation
with
lo
cally
euclidean
metric
in
feature
space
we
include
this
metric
to
test
whether
despite
the
restrictive
func
tional
assumptions
made
on
the
scm
i.e
additive
noise
the
cfes
generated
generalize
well
to
mani
folds
learned
without
such
functional
assumptions
other
experimental
details
for
prediction
we
train
both
logistic
regression
lr
classifiers
as
well
as
neural
network
nn
classifiers
with
two
hidden
layers
we
search
for
counterfactuals
for
the
negatively
classified
individuals
in
the
test
set
when
searching
for
counterfactuals
we
only
allow
changes
to
real-valued
features
the
experimental
results
are
averaged
over
five
random
seeds
6.1
results
for
counterfactual
explanations
we
present
the
results
for
cfe
generation
in
table
1
as
expected
methods
that
do
not
explicitly
consider
the
un
derlying
geometry
of
the
data
manifold
wachter
and
back
tracking
counterfactuals
achieve
lowest
lo
distances
for
most
classifiers
and
datasets
we
additionally
observe
that
face
and
revise
generally
result
in
counterfactuals
that
are
closer
along
the
data
manifold
compared
to
the
method
of
wachter
et
al
2017
in
contrast
backtracking
produces
competitive
results
compared
to
face
and
revise
indicat
ing
that
searching
for
counterfactuals
along
the
exogenous
space
of
the
scm
may
be
an
effective
approach
to
search
for
counterfactuals
along
the
data
manifold
as
expected
our
proposed
methods
result
in
counterfactu
als
which
are
closer
along
the
entailed
data
manifolds
of
the
scm
l
and
ly
since
they
precisely
optimize
for
such
riemannian
distances
we
observe
that
our
proposed
methods
also
fare
favourably
in
terms
of
the
the
data
mani
fold
learned
without
the
scm
l
»4
indicating
that
despite
the
functional
assumptions
on
the
scm
i.e
additive
noise
models
the
generated
cfes
generalize
well
on
data
manifolds
entailed
by
structural
causal
models
table
1
experimental
results
counterfactual
examples
linear
classifier
nn
classifier
adult
compas
adult
compas
method
lay
lo
ly
lx
lm
lo
ly
lx
lm
lo
ly
lx
lm
l2
ly
lx
wachter
7.38
1.65
5.76
5.86|
247
0.80
3.00
2.66|3.83
188
6.59
6.89|290
0.81
275
2.68
backtr
3.12
1.69
547
6.07
411
083
2.85
280|351
192
640
7.00|
253
0.85
2.83
2.81
face
329
1.85
550
5.69|231
085
2.88
271|500
210
7.02
6.78
225
0.85
3.73
2.54
revise
5.64
218
9.02
871|222
092
257
253|387
221
635
646|255
096
2.83
290
ours
lyy
279
1.71
3.21
3.48
2.77
0.84
2.33
233|325
195
4.02
4.58|2.74
0.86
2.51
2.52
ours
ly
2.75
1.70
343
3.48
2.18
0.81
235
2.27
|3.64
1.94
429
436|219
0.83
241
2.51
table
2
experimental
results
algorithmic
recourse
linear
classifier
nn
classifier
adult
compas
adult
compas
method
lym
lo
lu
ly
lm
ly
ly
lx
lm
ly
ly
lx
lm
ly
lu
lx
karimiet
al
2.68
1.49
4.04
4.05]|
1.33
0.75
2.62
2.63|3.47
1.84
563
566|137
079
2.68
2.69
ours
ly
1.29
1.58
1.48
1.48
1.19
0.79
2.25
229|086
192
115
1.15|
120
085
220
2.23
ours
ly
1.09
1.58
1.31
1.32|1.17
0.79
227
227|113
191
1.52
1.52|122
085
223
2.19
6.2
results
for
algorithmic
recourse
we
present
the
results
for
causal
algorithmic
recourse
in
table
2
as
expected
the
approach
of
karimi
et
al
2021
which
precisely
seeks
to
minimize
the
magnitude
of
the
recourse
intervention
generates
recourse
recommendations
with
smaller
magnitude
l2
compared
to
our
proposed
methods
however
our
proposed
methods
achieve
better
results
for
all
distance
measures
that
are
informed
by
the
data
manifold
ly
ly
laq
insofar
minimal
distances
along
the
data
manifold
is
desideratum
for
algorithmic
recourse
our
differential
geometric-principled
approach
bridges
the
gap
between
causal
algorithmic
recourse
and
previously
proposed
manifold-based
non-causal
methods
7
conclusion
and
outlook
in
this
work
we
have
analyzed
scms
from
novel
dif
ferential
geometric
perspective
we
first
derived
sufficient
conditions
for
scms
to
admit
differential
geometric
study
i.e
induce
smooth
manifolds
in
the
observational
in
terventional
and
counterfactual
settings
and
showed
that
these
conditions
are
satisfied
by
well-studied
classes
of
scms
with
broad
idenfibiality
results
namely
additive
noise
models
post-nonlinear
models
and
location-scale
models
drawing
inspiration
from
the
prior
works
in
manifold
learn
ing
we
then
proposed
riemannian
characterization
of
the
data
manifolds
entailed
by
scms
this
characterization
enables
us
to
define
operations
on
the
data
manifold
e.g
distance
computations
that
are
informed
by
the
causal
struc
ture
of
the
data
and
it
enables
to
causally
reason
about
the
data
manifold
in
an
interventional
and
counterfactual
sense
we
then
leveraged
the
proposed
framework
to
generate
coun
terfactual
explanations
for
machine
learning
classifiers
in
contrast
to
previous
manifold-based
methods
for
generating
counterfactual
explanation
we
measure
distances
along
the
data
manifold
in
differential-geometric
principled
manner
leveraging
the
pertinent
entailed
observational
data
mani
folds
lastly
we
novelly
consider
the
problem
of
manifold
based
causal
algorithmic
recourse
for
which
we
instead
leverage
the
entailed
counterfactual
data
manifolds
the
study
of
causal
models
from
differential
geometric
perspective
is
promising
avenue
of
future
research
since
both
causality
and
manifold
learning
allow
the
introduc
tion
of
strong
inductive
biases
for
machine
learning
in
this
work
we
characterize
the
data
manifolds
entailed
by
scms
as
deterministic
manifolds
however
future
work
may
consider
different
causal
models
i.e
causal
graphical
models
and/or
manifold
characterizations
i.e
statistical
manifolds
lastly
in
this
work
we
leveraged
the
observa
tional
and
counterfactual
data
manifolds
entailed
by
scms
to
generate
counterfactual
explanations
future
works
may
instead
consider
tasks
that
require
reasoning
about
the
inter
ventional
data
manifolds
entailed
by
scms
acknowledgments
the
authors
thank
the
international
max
planck
research
school
for
intelligent
systems
imprs-is
for
supporting
ricardo
dominguez-olmedo
amir-hossein
karimi
acknowledges
generous
founding
sup
port
from
nserc
cls
and
google
on
data
manifolds
entailed
by
structural
causal
models
references
antoran
j
bhatt
u
adel
t
weller
a
and
herndndez
lobato
j
m
getting
clue
method
for
explaining
uncertainty
estimates
in
international
conference
on
learning
representations
2021
arvanitidis
g
hansen
l
k
and
hauberg
s
locally
adaptive
normal
distribution
advances
in
neural
infor
mation
processing
systems
29
2016
arvanitidis
g
hansen
l
k
and
hauberg
s
latent
space
oddity
on
the
curvature
of
deep
generative
models
in
6th
international
conference
on
learning
representa
tions
iclr
2018
2018
arvanitidis
g
hauberg
s
and
scholkopf
b
geomet
rically
enriched
latent
spaces
in
international
confer
ence
on
artificial
intelligence
and
statistics
pp
631-639
pmlr
2021
arvanitidis
g
georgiev
b
m
and
schélkopf
b
prior-based
approximate
latent
riemannian
metric
in
international
conference
on
artificial
intelligence
and
statistics
pp
4634-4658
pmlr
2022
beik-mohammadi
h
hauberg
s
arvanitidis
g
neu
mann
g
and
rozo
l
learning
riemannian
manifolds
for
geodesic
motion
skills
in
robotics
science
and
systems
2021
beik-mohammadi
h
hauberg
s
arvanitidis
g
neu
mann
g
and
rozo
l
reactive
motion
genera
tion
on
learned
riemannian
manifolds
arxiv
preprint
arxiv:2203.07761
2022
belkin
m
and
niyogi
p
laplacian
eigenmaps
for
di
mensionality
reduction
and
data
representation
neural
computation
15(6):1373
1396
2003
bradbury
j
frostig
r
hawkins
p
johnson
m
j
leary
c
maclaurin
d
necula
g
paszke
a
vanderplas
j
wanderman-milne
s
and
zhang
q
jax
composable
transformations
of
python+numpy
programs
2018
url
http://github.com/google/jax
crampin
m
and
pirani
f
a
e
applicable
differential
geometry
cambridge
university
press
1994
detlefsen
n
s
hauberg
s
and
boomsma
w
learning
meaningful
representations
of
protein
sequences
nature
communications
13(1):1-12
2022
do
carmo
m
p
riemannian
geometry
birkhduser
1992
dominguez-olmedo
r
karimi
a
h
and
scholkopf
b
on
the
adversarial
robustness
of
causal
algorithmic
recourse
in
international
conference
on
machine
learn
ing
pp
5324-5342
pmlr
2022
10
downs
m
chu
j
l
yacoby
y
doshi-velez
f
and
pan
w
cruds
counterfactual
recourse
using
disentangled
subspaces
in
icml
workshop
on
human
interpretability
in
machine
learning
2020
hastie
t
j
and
stuetzle
w
principal
curves
journal
of
the
american
statistical
association
84(406):502-516
1989
hauberg
s
only
bayes
should
learn
manifold
on
the
estimation
of
differential
geometric
structure
from
data
arxiv
preprint
arxiv:1806.04994
2018
hauberg
s
freifeld
o
and
black
m
geometric
take
on
metric
learning
advances
in
neural
information
pro
cessing
systems
25
2012
immer
a
schultheiss
c
vogt
j
e
scholkopf
b
biihlmann
p
and
marx
a
on
the
identifiability
and
estimation
of
causal
location-scale
noise
models
arxiv
preprint
arxiv:2210.09054
2022
joshi
s
koyejo
o
vijitbenjaronk
w
kim
b
and
ghosh
j
towards
realistic
individual
recourse
and
ac
tionable
explanations
in
black-box
decision
making
sys
tems
safe
machine
learning
workshop
at
iclr
2019
karimi
a.-h
von
kiigelgen
j
schélkopf
b
and
valera
1
algorithmic
recourse
under
imperfect
causal
knowl
edge
probabilistic
approach
advances
in
neural
infor
mation
processing
systems
pp
265-277
2020
karimi
a.-h
scholkopf
b
and
valera
i
algorithmic
re
course
from
counterfactual
explanations
to
interventions
in
proceedings
of
the
2021
acm
conference
on
fairness
accountability
and
transparency
pp
353-362
2021
karimi
a.-h
barthe
g
scholkopf
b
and
valera
1
survey
of
algorithmic
recourse
definitions
formula
tions
solutions
and
prospects
acm
computing
surveys
csur
2022
kierzenka
j
and
shampine
l
f
bvp
solver
based
on
residual
control
and
the
maltab
pse
acm
transactions
on
mathematical
software
toms
27(3):299-316
2001
kohavi
r
and
becker
b
uci
adult
data
set
uci
meachine
learning
repository
6
1996
larson
j
mattu
s
kirchner
l
and
angwin
j
how
we
analyzed
the
compas
recidivism
algorithm
propublica
52016),9(1
2016
mahajan
d
tan
c
and
sharma
a
preserving
causal
constraints
in
counterfactual
explanations
for
machine
learning
classifiers
neurips
2019
workshop
do
the
right
thing
machine
learning
and
causal
inference
for
improved
decision
making
2019
on
data
manifolds
entailed
by
structural
causal
models
nabi
r
and
shpitser
1
fair
inference
on
outcomes
in
proceedings
of
the
aaai
conference
on
artificial
intelli
gence
volume
32
2018
pawelczyk
m
broelemann
k
and
kasneci
g
on
coun
terfactual
explanations
under
predictive
multiplicity
in
conference
on
uncertainty
in
artificial
intelligence
pp
809-818
pmlr
2020
pearl
j
causality
cambridge
university
press
2009
peters
j
janzing
d
and
scholkopf
b
elements
of
causal
inference
foundations
and
learning
algorithms
the
mit
press
2017
poyiadzi
r
sokol
k
santos-rodriguez
r
de
bie
t
and
flach
p
face
feasible
and
actionable
counterfactual
explanations
in
proceedings
of
the
aaai/acm
confer
ence
on
al
ethics
and
society
pp
344-350
2020
riemann
b
ueber
die
hypothesen
welche
der
geome
trie
zu
grunde
liegen
abhandlungen
der
koniglichen
gesellschaft
der
wissenschaften
zu
gottingen
13
1868
scannell
a
ek
c
h
and
richards
a
trajectory
opti
misation
in
learned
multimodal
dynamical
systems
via
latent-ode
collocation
in
2021
ieee
international
con
ference
on
robotics
and
automation
icra
pp
12745
12751
ieee
2021
smola
a
j
mika
s
scholkopf
b
and
williamson
r
c
regularized
principal
manifolds
journal
of
machine
learning
research
1:179-209
2001
http:/www.jmlr.org
tosi
a
hauberg
s
vellido
alcacena
a
and
lawrence
n
d
metrics
for
probabilistic
geometries
in
uncertainty
in
artificial
intelligence
proceedings
of
the
thirtieth
con
ference
2014
july
23-27
2014
quebec
city
quebec
canada
pp
800-808
auai
press
association
for
un
certainty
in
artificial
intelligence
2014
venkatasubramanian
s
and
alfano
m
the
philosophical
basis
of
algorithmic
recourse
in
proceedings
of
the
2020
conference
on
fairness
accountability
and
transparency
pp
284-293
2020
verma
s
dickerson
j
and
hines
k
counterfactual
ex
planations
for
machine
learning
review
neurips
2020
workshop
ml-retrospectives
surveys
meta-analyses
2020
von
kiigelgen
j
mohamed
a
and
beckers
s
backtrack
ing
counterfactuals
2nd
conference
on
causal
learning
and
reasoning
2023
wachter
s
mittelstadt
b
and
russell
c
counterfactual
explanations
without
opening
the
black
box
automated
decisions
and
the
gdpr
harv
jl
tech
31:841
2017
11
zhang
k
and
hyvirinen
a
on
the
identifiability
of
the
post-nonlinear
causal
model
in
proceedings
of
the
twenty-fifth
conference
on
uncertainty
in
artificial
in
telligence
uai1’09
pp
647-655
auai
press
2009
on
data
manifolds
entailed
by
structural
causal
models
a
proofs
a.1
observation
3.1
under
causal
sufficiency
the
exogenous
distribution
py
factorizes
as
py
py
.
py
and
consequently
its
support
is
the
cartesian
product
of
every
marginal
that
is
supp
py
supp
py
.
supp
py
the
cartesian
product
of
n-many
d;-dimensional
smooth
manifolds
is
d-dimensional
smooth
manifold
where
z:l:o
d
a.2
lemma
3.2
under
acyclicity
of
the
causal
graph
it
is
possible
to
construct
the
reduced-form
mapping
&/
by
recursive
substitution
of
the
structural
assignments
f
in
topological
order
of
the
causal
graph
if
all
f
are
differentiable
then
it
follows
that
is
differentiable
since
the
composition
of
differentiable
functions
is
differentiable
therefore
its
jacobian
df
r™
well-defined
in
its
domain
u
where
is
the
number
of
exogenous
variables
since
the
causal
graph
is
acyclic
let
us
assume
without
loss
of
generality
that
the
endogenous
variables
are
ordered
such
that
i<j
x
xpu
then
the
partial
derivatives
dy
f
vanish
for
all
j
which
implies
that
the
jacobian
is
lower-triangular
and
consequently
det
d
f
hid:1
ou
fi
therefore
if
all
partial
derivatives
dy
f
are
non-vanishing
in
u
it
holds
that
det
f(u
vu
u
and
consequently
rank(d
f(u
vu
u
since
is
defined
as
the
image
of
through
f
and
is
differentiable
and
its
jacobian
has
everywhere
rank
d
under
the
assumption
that
i/
is
d-dimensional
smooth
manifold
then
by
definition
&/
x
is
an
immersion
crampin
pirani
1994
pp
243
a.3
lemma
3.3
under
acyclicity
of
the
causal
graph
it
is
possible
to
construct
the
reduced-form
mapping
&/
by
recursive
substitution
of
the
structural
assignments
f
in
topological
order
of
the
causal
graph
without
loss
of
generality
let
us
assume
that
the
endogenous
variables
are
ordered
such
that
x
let
us
denote
by
f
the
recursive
substitution
of
the
structural
assignments
up
to
x
where
f
is
map
from
u
supp
py
.
supp
py
to
xi){:l
note
that
the
reduced-form
mapping
is
by
definition
f
by
the
condition
in
equation
it
trivially
holds
that
x
=
f1(uy
is
injective
note
that
necessarily
xp,1
0
let
us
assume
that
fj,l
is
injective
let
u
u
u
such
that
u
u
if
u}
uj
then
f’j(u
f](u
since
they
must
differ
in
x
by
the
condition
of
equation
6
if
u
u
but
u
uj
then
f](u
f}(u
since
they
must
differ
in
some
for
by
assumption
that
fj,l
is
injective
consequently
fj
is
injective
by
induction
f‘d
is
injective
where
is
the
number
of
exogenous
variables
therefore
the
reduced-form
map
is
injective
a.4
proposition
3.4
under
lemma
3.2
if
/
is
d-dimensional
smooth
manifold
then
the
reduced-form
mapping
x
is
an
immersion
by
lemma
3.3
is
injective
consequently
i/
is
smooth
embedding
such
that
i/
is
diffeomorphic
to
its
image
by
assumption
is
d-dimensional
smooth
manifold
it
follows
that
&
is
d-dimensional
manifold
and
diffeomorphism
a.s5
corollary
3.5
a.5.1
additive
noise
models
for
additive
noise
models
of
the
form
x
f(xpa
ui
it
holds
that
0y
f
for
all
u
supp
py
and
{1,...,d}
and
therefore
the
conditions
of
lemma
3.2
i.e
non-vanishing
partial
derivative
in
{{
are
satisfied
furthermore
for
any
;
it
holds
that
u®
u®
fi(ps
um
fi(2p(sy
u
since
fi(p(r
ul
fi(@pagiy
u®
vum
u®
s.t
u
u
consequently
the
conditions
of
lemma
3.3
are
satisfied
and
therefore
proposition
3.4
holds
12
on
data
manifolds
entailed
by
structural
causal
models
a.5.2
post-nonlinear
models
for
post-nonlinear
models
the
structural
equations
take
the
form
f;(xp,(i
ui
gfl
gf
xpaiy
us
for
invertible
gfl)a
for
differentiable
f
then
dy
fi(xpa(i
ui
0[;19(1
gf)(xpn
which
is
non-vanishing
per
assumption
that
ggl
is
invertible
thus
satisfying
the
assumptions
of
lemma
3.2
furthermore
for
any
;
it
holds
that
u
u®
fi(@pagi
uv
filtpa
u®
since
per
ggl
invertible
it
holds
that
f;(2p(i
u™
fi(zpa
u®
<>
g®
tpaiy
ull
ggz)(zpam
u®
where
the
rhs
is
satisfied
vu®
u(®
s.t
u(}
u
consequently
the
conditions
of
lemma
3.3
are
satisfied
and
therefore
proposition
3.4
holds
a.5.3
location-scale
noise
models
for
location-scale
noise
models
the
structural
equations
take
the
form
f;(xpa(i
ui
gfl)((xpn
g§2)(xpn(,))ui
2
where
g
is
strictly
positive
i.e
maps
to
r
consequently
dy
f
gfz
gf
is
strictly
positive
thus
satisfying
the
assumptions
of
lemma
3.2
furthermore
for
any
;
it
holds
that
xpa(i
which
is
non-vanishing
since
u®
u®
fi(zpa),uv
fi(@p
u®
since
per
assumption
that
g
is
strictly
positive
gfz)(mpﬂ(i))u(l
g®
zpaiy)um
vud
u®
s.t
ul
u®
consequently
the
conditions
of
lemma
3.3
are
satisfied
and
therefore
proposition
3.4
holds
a.6
corollary
3.7
the
corollary
follows
directly
since
for
any
{1,...,d}
if
f
vi
{1,...,d}
satisfies
the
conditions
of
lemma
3.2
and
lemma
3.3
then
f
vi
necessarily
satisfies
the
conditions
of
lemma
3.2
and
lemma
3.3
a.7
proposition
3.6
let
us
consider
the
hard
intervention
=
do(xz
)
where
{1
d}
with
cardinality
=
|z|
and
let
us
denote
the
remaining
indices
by
=
{1,...,d}
corresponding
to
the
non-intervened-upon
variables
since
by
assumption
py
satisfies
the
conditions
of
proposition
3.1
then
it
also
holds
that
p&
py
xx
py
isa
n
m)-dimensional
smooth
manifold
i.e
the
cartesian
product
of
n
m)-many
1-dimensional
smooth
manifolds
consider
the
interventional
structural
assignments
s
where
sj
=
and
87y
=
s7
note
that
since
by
assumption
the
causal
graph
of
is
acyclic
lemmas
3.2
and
3.3
then
the
interventional
causal
graph
gz
must
also
be
acyclic
since
edges
are
only
removed
from
to
obtain
g
consequently
the
interventional
reduced-form
map
f
u
x}
can
be
readily
obtained
by
recursive
substitution
of
the
structural
assignments
s
in
topological
order
of
the
causal
graph
g
since
every
f
is
differentiable
per
assumption
in
lemma
a.2
and
the
composition
of
differentiable
functions
is
differentiable
function
then
it
follows
that
f
is
differentiable
following
the
same
argument
as
proposition
3.4
the
jacobian
f7
r(*=™)*("=m
hag
rank
equal
to
if
dy
f
is
non-vanishing
for
7
which
holds
by
assumption
that
the
scm
meets
the
conditions
of
proposition
3.2
if
the
injetivity
conditions
of
lemma
3.3
on
f
are
satisfied
then
the
reduced-form
map
f7
is
injective
following
the
same
argument
of
appendix
a.3
consequently
f
is
smooth
embedding
and
the
interventional
space
x7
is
n
m
dimensional
manifold
embedded
in
r<
a.8
proposition
3.8
by
assumption
the
reduced-form
mapping
of
the
scm
is
invertible
and
consequently
the
observed
x
corresponds
to
unique
realization
f~'(z
of
the
exogenous
variables
let
{1
.
d}
be
the
indices
of
the
intervened-upon
variables
with
cardinality
=
|z|
and
let
us
denote
the
remaining
indices
by
=
{1,...,d}
z
additionally
let
be
the
set
of
indices
corresponding
to
the
causal
descendants
of
x7
excluding
xz
by
assumption
the
space
of
intervention
values
is
m-dimensional
smooth
manifold
a
consider
the
interventional
structural
equations
s;‘)(xi:q
and
s?(xi:g
=
s7
recursive
substitution
of
such
structural
equations
in
topological
order
of
the
acyclic
interventional
graph
gz
results
in
the
counterfactual
reduced-form
map
fcf
rii+ipi
from
the
intervention
variables
to
the
values
of
the
intervened
upon
variables
x7
and
its
causal
descendants
xp
where
13
on
data
manifolds
entailed
by
structural
causal
models
the
exogenous
variables
are
kept
fixed
to
and
the
intervention
values
are
allowed
to
vary
by
assumption
the
structural
assignments
corresponding
to
the
causal
descendants
xp
are
differentiable
and
thus
the
jacobian
ff
ruzihipdxizi
i5
well-defined
without
loss
of
generality
let
us
assume
that
the
endogenous
variables
are
ordered
such
that
x
in
the
interventional
graph
g7
corresponding
to
hard
intervening
in
xz
then
it
follows
that
the
partial
derivatives
8y
f
vanish
for
all
j
and
consequently
since
det
df
~
dp
f{
=1
everywhere
it
holds
that
rank
d
f°f
m
consequently
ff
is
an
immersion
into
its
image
in
rizi+/p1
let
us
denote
by
d
{1,...,d}
zu
d
the
indices
of
the
endogenous
variables
that
are
neither
intervened
upon
nor
causal
descendants
of
intervened-upon
variables
consider
the
modified
reduced-form
f°f
r<
such
that
fe(0
==
x
ff(9
since
fcf
is
differentiable
and
its
jacobian
has
rank
everywhere
it
trivially
holds
that
f<
also
is
differentiable
and
its
jacobian
has
rank
everywhere
then
by
definition
fcfi
is
an
immersion
into
its
image
in
r%
such
image
is
precisely
defined
as
the
space
of
counterfactual
1
=
frmi(a)a
since
is
assumed
invertible
then
each
fi
is
injective
and
fcf
is
also
injective
following
the
same
argument
as
lemma
3.3
it
then
holds
that
fcf
is
smooth
embedding
and
x*i
is
m-dimensional
smooth
manifold
embedded
in
r%
a.9
corollary
3.9
the
corollary
follows
directly
since
if
an
scm
satisfies
the
conditions
of
proposition
3.4
then
f
vi
{1,...,d}
satisfy
the
conditions
of
lemma
3.2
and
lemma
3.3
the
latter
implying
that
f
vi
is
differentiable
for
any
given
set
of
causal
descendants
of
the
intervened-upon
variables
z
14
