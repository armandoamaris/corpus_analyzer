does
the
geometry
of
word
embeddings
help
document
classification
case
study
on
persistent
homology
based
representations
paul
michel
abstract
we
investigate
the
pertinence
of
meth
ods
from
algebraic
topology
for
text
data
analysis
these
methods
enable
the
de
velopment
of
mathematically-principled
isometric-invariant
mappings
from
set
of
vectors
to
document
embedding
which
is
stable
with
respect
to
the
geometry
of
the
document
in
the
selected
metric
space
in
this
work
we
evaluate
the
utility
of
these
topology-based
document
represen
tations
in
traditional
nlp
tasks
specifi
cally
document
clustering
and
sentiment
classification
we
find
that
the
embed
dings
do
not
benefit
text
analysis
in
fact
performance
is
worse
than
simple
tech
niques
like
#f-idf
indicating
that
the
ge
ometry
of
the
document
does
not
provide
enough
variability
for
classification
on
the
basis
of
topic
or
sentiment
in
the
chosen
datasets
introduction
given
embedding
model
mapping
words
to
di
mensional
vectors
every
document
can
be
repre
sented
as
finite
subset
of
r™
comparing
doc
uments
then
amounts
to
comparing
such
subsets
while
previous
work
shows
that
the
earth
mover’s
distance
kusner
et
al
2015
or
distance
between
the
weighted
average
of
word
vectors
arora
et
al
2017
provides
information
that
is
useful
for
clas
sification
tasks
we
wish
to
go
step
further
and
investigate
whether
useful
information
can
also
be
found
in
the
shape
of
document
in
word
em
bedding
space
persistent
homology
is
tool
from
algebraic
topology
used
to
compute
topological
signatures
called
persistence
diagrams
on
compact
metric
the
indicated
authors
contributed
equally
to
this
work
abhilasha
ravichander
carnegie
mellon
university
~carnegie
mellon
university
pmichell@cs.cmu.edu
aravicha@cs.cmu.edu
235
shruti
rijhwani
carnegie
mellon
university
srijhwan@cs.cmu.edu
spaces
these
have
the
property
of
being
sta
ble
with
respect
to
the
gromov-haussdorff
dis
tance
gromov
et
al
1981
in
other
words
com
pact
metric
spaces
that
are
close
up
to
an
isometry
will
have
similar
embeddings
in
this
work
we
ex
amine
the
utility
of
such
embeddings
in
text
clas
sification
tasks
to
the
best
of
our
knowledge
no
previous
work
has
been
performed
on
using
topo
logical
representations
for
traditional
nlp
tasks
nor
has
any
comparison
been
made
with
state-of
the-art
approaches
we
begin
by
considering
document
as
the
set
of
its
word
vectors
generated
with
pretrained
word
embedding
model
these
form
the
metric
space
on
which
we
build
persistence
diagrams
us
ing
euclidean
distance
as
the
distance
measure
the
diagrams
are
representation
of
the
docu
ment’s
geometry
in
the
metric
space
we
then
per
form
clustering
on
the
twenty
newsgroups
dataset
with
the
features
extracted
from
the
persistence
diagram
we
also
evaluate
the
method
on
senti
ment
classification
tasks
using
the
cornell
sen
tence
polarity
csp
pang
and
lee
2005
and
imdb
movie
review
datasets
maas
et
al
2011
as
suggested
by
zhu
2013
we
posit
that
the
information
about
the
intrinsic
geometry
of
doc
uments
found
in
the
persistence
diagrams
might
yield
information
that
our
classifier
can
leverage
either
on
its
own
or
in
combination
with
other
rep
resentations
the
primary
objective
of
our
work
is
to
empirically
evaluate
these
representations
in
the
case
of
sentiment
and
topic
classification
and
assess
their
usefulness
for
real-world
tasks
method
2.1
word
embeddings
as
first
step
we
compute
word
vectors
for
each
document
in
our
corpus
using
word2vec
mikolov
et
al
2013
model
trained
on
the
google
proceedings
of
the
2nd
workshop
on
representation
learning
for
nlp
pages
235-240
vancouver
canada
august
3
2017
©2017
association
for
computational
linguistics
news
dataset
in
addition
to
being
widely
used
word
embedding
technique
word2vec
has
been
known
to
exhibit
interesting
linear
properties
with
respect
to
analogies
mikolov
et
al
2013
which
hints
at
rich
semantic
structure
2.2
gromov-haussdorff
distance
given
dictionary
of
word
vectors
of
dimension
n
we
can
represent
any
document
as
finite
subset
of
r™
the
haussdorff
distance
gives
us
way
to
evaluate
the
distance
between
two
such
sets
more
precisely
the
haussdorff
distance
dr
between
two
finite
subsets
a
of
r™
is
defined
as
dp
a
b
max(sup
d(a
b),supd(b
a
aca
beb
where
d(z,y
infycy
||z
y|2
is
the
distance
of
point
from
set
y
however
this
distance
is
sensitive
to
transla
tions
and
other
isometric
transformations
hence
more
natural
metric
is
the
gromov-haussdorff
distance
gromov
et
al
1981
simply
defined
as
dn(a.b
inf
dy(a.f(b
where
e
is
the
set
of
all
isometries
of
r™
figure
provides
an
example
of
practical
gromov-haussdorff
gh
distance
computation
between
two
sets
of
three
points
each
both
sets
are
embedded
in
r
middle
panel
using
isome
tries
i.e
the
distance
between
points
in
each
set
is
conserved
the
haussdorff
distance
between
the
two
embedded
sets
corresponds
to
the
length
of
the
black
segment
the
gh
distance
is
the
min
imum
haussdorff
distance
under
all
possible
iso
metric
embeddings
we
want
to
compare
documents
based
on
their
intrinsic
geometric
properties
intuitively
the
gh
distance
measures
how
far
two
sets
are
from
being
isometric
this
allows
us
to
define
the
geometry
of
document
more
precisely
definition
document
geometry
we
say
that
two
documents
a
have
the
same
geometry
if
dau(a
b
0
ie
if
they
are
the
same
up
to
an
isometry
mathematically
speaking
this
amounts
to
defin
ing
the
geometry
of
document
as
its
equivalence
class
under
the
equivalence
relation
induced
by
the
gh
distance
on
the
set
of
all
documents
https://code.google.com/archive/p/word2vec/
2f
r™
r™
is
isometric
if
it
is
distance
preserving
ievz,y
r
||
f(z)—f(y)|l2
||lz—y||2
rotations
trans
lations
and
reflections
are
examples
of
linear
isometries
236
o.iﬁ.\
figure
1
gromov-haussdorff
distance
between
two
sets
red
green
the
black
bar
represents
the
actual
distance
given
that
the
isometric
embed
ding
is
optimal
comparison
to
the
earth
mover
distance
kusner
et
al
2015
proposed
new
method
for
computing
distance
between
documents
based
on
an
instance
of
the
earth
mover
distance
rub
ner
et
al
1998
called
word
mover
distance
wmd
while
wmd
quantifies
the
total
cost
of
matching
all
words
of
one
document
to
another
the
gh
distance
is
the
cost
up
to
an
isometry
of
the
worst-case
matching
2.3
persistence
diagrams
efficiently
computing
the
gh
distance
is
still
an
open
problem
despite
lot
of
recent
work
in
this
area
mémoli
and
sapiro
2005
bronstein
et
al
2006
mémoli
2007
agarwal
et
al
2015
fortunately
carriere
et
al
2015
provides
us
with
way
to
derive
signature
which
is
stable
with
respect
to
the
gh
distance
more
specifi
cally
given
finite
point
cloud
r
the
per
sistence
diagram
of
the
vietori-rips
filtration
on
a
dg(a
can
be
computed
this
approach
is
inspired
by
persistent
homology
subfield
of
al
gebraic
topology
the
rigorous
definition
of
these
notions
is
not
the
crux
of
this
paper
and
we
will
only
present
them
informally
the
curious
reader
is
invited
to
refer
to
zhu
2013
for
short
introduction
more
details
are
in
delfinado
and
edelsbrunner
1995
edelsbrunner
et
al
2002
robins
1999
persistence
diagram
is
scatter
plot
of
2-d
points
representing
the
appearance
and
disappear
ance
of
geometric
features®
under
varying
reso
lutions
this
can
be
imagined
as
replacing
each
point
by
sphere
of
increasing
radius
we
use
the
procedure
described
in
carriere
et
al
3such
as
connected
components
holes
or
empty
hulls
word2vec
word
vectors
persistence
dataset
per
document
diagrams
clustering
or
downstream
task
document
classification
embeddings
figure
2
method
pipeline
2015
to
derive
fixed-sized
vectors
from
persis
tence
diagrams
these
vectors
have
the
follow
ing
property
if
and
are
two
finite
sub
sets
of
r™
dg(a
and
dg(b
are
their
persis
tence
diagrams
max(|dg(a)|,|dg(b)|
and
vi
vg
r™2
then
2n(n
1)dgu(a
b
va—vall2
in
other
words
the
resulting
signatures
v4
and
vg
are
stable
with
respect
to
the
gh
distance
the
size
of
the
vectors
are
dependent
on
the
underlying
sets
and
b
however
as
is
argued
in
carriere
et
al
2015
we
can
truncate
the
vectors
to
di
mension
fixed
across
our
dataset
while
preserving
the
stability
property
albeit
losing
some
of
the
representative
ability
of
the
signatures
experiments
3.1
experiments
the
pipeline
for
our
experiments
is
shown
in
fig
ure
2
in
order
to
build
persistence
diagram
we
convert
each
document
to
the
set
of
its
word
vectors
we
then
use
dionysus
morozov
2008
2016
c++
library
for
computing
persistence
di
agrams
and
form
the
signatures
described
in
2.3
we
will
subsequently
refer
to
these
diagrams
as
persistent
homology
ph
embeddings
once
we
have
the
embeddings
for
each
document
they
can
be
used
as
input
to
standard
clustering
or
classifi
cation
algorithms
as
baseline
document
representation
we
use
the
average
of
the
word
vectors
for
that
document
subsequently
called
aw2v
embeddings
for
clustering
we
experiment
with
k-means
and
gaussian
mixture
models
gmm
on
sub
set
of
the
twenty
newsgroups
dataset
the
sub
set
was
selected
to
ensure
that
most
documents
are
from
related
topics
making
clustering
non
trivial
and
the
documents
are
of
reasonable
length
to
compute
the
representation
alt.atheism
sci.space
and
talk.religion.misc
categories
237
for
classification
we
perform
both
sentence
level
and
document-level
binary
sentiment
classi
fication
using
logistic
regression
on
the
csp
and
imdb
corpora
respectively
results
4.1
hyper-parameters
our
method
depends
on
very
few
hyper
parameters
our
main
choices
are
listed
below
choice
of
distance
we
experimented
with
both
euclidean
distance
and
cosine
similarity
angular
distance
after
preliminary
experiments
we
de
termined
that
both
performed
equally
and
hence
we
only
report
results
with
the
euclidean
distance
persistence
diagram
computation
the
hyper
parameters
of
the
diagram
computation
are
mono
tonic
and
mostly
control
the
degree
of
approxima
tion
we
set
them
to
the
highest
values
that
al
lowed
our
experiment
to
run
in
reasonable
time®
4.2
document
clustering
we
perform
clustering
experiments
with
the
base
line
document
features
aw2v
tf-idf
and
our
ph
signatures
figure
shows
the
b-cubed
preci
sion
recall
and
f1-score
of
each
method
metrics
as
defined
in
amigo
et
al
2009
to
further
as
sess
the
utility
of
ph
embeddings
we
concatenate
them
with
aw2v
to
obtain
third
representation
aw2v+ph
with
gmm
and
aww2v+ph
the
fl-score
of
clustering
is
0.499
in
terms
of
f1
and
precision
we
see
that
tf-idf
representations
perform
better
than
ph
for
reasons
that
we
will
discuss
in
later
sections
in
terms
of
recall
ph
as
well
as
aw2v
perform
fairly
well
importantly
we
see
that
all
the
metrics
for
ph
are
significantly
above
the
ran
dom
baseline
indicating
that
some
valuable
infor
mation
is
contained
in
them
4.3
sentiment
classification
4.3.1
sentence-level
sentiment
analysis
we
evaluate
our
method
on
the
csp
dataset®
the
results
are
presented
in
table
1
for
comparison
we
provide
results
for
one
of
the
state
of
the
art
models
cnn-based
sentence
classifier
kim
3selected
such
that
the
computation
of
the
diagram
of
the
longest
file
in
the
training
data
took
less
than
10
minutes
for
lack
of
canonical
split
we
use
random
10%
of
the
dataset
as
test
set
p1-kmeans
ph-gmm
0
tfldf-kmeans
084
tfldf-gmm
pu+aw2v-gmm
random
aw2v-gmm
f-score
precision
recall
figure
3
results
for
clustering
on
subclasses
of
the
twenty
newsgroups
dataset
model
accuracy
cnn
non-static
81.5%
ph
logreg
53.19%
aw2v
logreg
77.13%
aw2v
ph
logreg
77.13%
table
1
performance
on
the
csp
dataset
model
accuracy
paragraph
vector
92.58%
ph
logreg
53.16%
aw2v
logreg
82.94%
aw2v
ph
logreg
83.08%
table
2
performance
on
the
imdb
dataset
2014
we
observe
that
by
themselves
ph
em
beddings
are
not
useful
at
predicting
the
sentiment
of
each
sentence
aw2v
gives
reasonable
perfor
mance
in
this
task
but
combining
the
two
repre
sentations
does
not
impact
the
accuracy
at
all
4.3.2
document-level
sentiment
analysis
we
perform
document-level
binary
sentiment
clas
sification
on
the
imdb
movie
reviews
dataset
maas
et
al
2011
we
use
sentence
vectors
in
this
experiment
each
of
which
is
the
average
of
the
word
vectors
in
that
sentence
the
results
are
presented
in
table
2
we
compare
our
results
with
the
paragraph-vector
approach
le
and
mikolov
2014
we
observe
that
ph
embeddings
perform
poorly
on
this
dataset
similar
to
the
csp
dataset
aw2v
embeddings
give
acceptable
results
the
combined
representation
performs
slightly
better
but
not
by
margin
of
significance
discussion
and
analysis
as
seen
in
figure
3
the
ph
representation
does
not
outperform
#f-idf
or
aw2vv
and
in
fact
often
doesn’t
perform
much
better
than
chance
one
possible
reason
is
linked
to
the
nature
of
our
datasets
the
computation
of
the
persistence
diagram
is
very
sensitive
to
the
size
of
the
docu
ments
the
geometry
of
small
documents
where
the
number
of
words
is
negligible
with
respect
to
the
dimensionality
of
the
word
vectors
is
not
very
rich
the
resulting
topological
signatures
are
very
sparse
which
is
problem
for
csp
as
well
as
documents
in
imdb
and
twenty
newsgroups
that
contain
only
one
line
on
the
opposite
side
of
the
spectrum
persistence
diagrams
are
intractable
to
compute
without
down-sampling
for
very
long
documents
which
in
turn
negatively
impacts
the
representation
of
smaller
documents
we
performed
an
additional
experiment
on
subset
of
the
imdb
corpus
that
only
contained
documents
of
reasonable
length
but
obtained
sim
ilar
results
this
indicates
that
the
poor
perfor
mance
of
ph
representations
even
when
com
bined
with
other
features
aw2v
cannot
be
ex
plained
only
by
limitations
of
the
data
these
observations
lead
to
the
conclusion
that
for
these
datasets
the
intrinsic
geometry
of
doc
uments
in
the
word2vec
semantic
space
does
not
help
text
classification
tasks
related
work
learning
distributed
representations
of
sentences
or
documents
for
downstream
classification
and
information
retrieval
tasks
has
received
recent
at
tention
owing
to
their
utility
in
several
applica
tions
be
it
representations
trained
on
the
sen
238
tence/paragraph
level
le
and
mikolov
2014
kiros
et
al
2015
or
purely
word
vector
based
methods
arora
et
al
2017
document
classification
and
clustering
willett
1988
hotho
et
al
2005
steinbach
et
al
2000
huang
2008
xu
and
gong
2004
kuang
et
al
2015
miller
et
al
2016
and
sentiment
classifi
cation
nakagawa
et
al
2010
kim
2014
wang
and
manning
2012
are
relatively
well
studied
topological
data
analysis
has
been
used
for
var
ious
tasks
such
as
3d
shapes
classification
chazal
et
al
2009
or
protein
structure
analysis
xia
and
wei
2014
however
such
techniques
have
not
been
used
in
nlp
primarily
because
the
theory
is
inaccessible
and
suitable
applications
are
scarce
zhu
2013
offers
an
introduction
to
using
per
sistent
homology
in
nlp
by
creating
represen
tations
of
nursery-rhymes
and
novels
as
well
as
highlights
structural
differences
between
child
and
adolescent
writing
however
these
techniques
have
not
been
applied
to
core
nlp
tasks
conclusion
based
on
our
experiments
using
persistence
di
agrams
for
text
representation
does
not
seem
to
positively
contribute
to
document
clustering
and
sentiment
classification
tasks
there
are
certainly
merits
to
the
method
specifically
its
strong
math
ematical
foundation
and
its
domain-independent
unsupervised
nature
theoretically
algebraic
topology
has
the
ability
to
capture
structural
con
text
and
this
could
potentially
benefit
syntax
based
nlp
tasks
such
as
parsing
we
plan
to
in
vestigate
this
connection
in
the
future
acknowledgments
this
work
was
supported
in
part
by
the
defense
advanced
research
projects
agency
darpa
information
innovation
office
i20
under
the
low
resource
languages
for
emergent
incidents
lorelei
program
issued
by
darpa/i20
un
der
contract
no
hr0011-15-c-0114
the
views
expressed
are
those
of
the
authors
and
do
not
re
flect
the
official
policy
or
position
of
the
depart
ment
of
defense
or
the
u.s
government
we
are
grateful
to
matt
gormley
hyun
ah
song
shivani
poddar
and
hai
pham
for
their
sug
gestions
on
the
writing
of
this
paper
as
well
as
to
steve
oudot
for
pointing
us
to
helpful
references
we
would
also
like
to
thank
the
anonymous
acl
reviewers
for
their
valuable
suggestions
239
references
pankaj
agarwal
kyle
fox
abhinandan
nath
anas
tasios
sidiropoulos
and
yusu
wang
2015
com
puting
the
gromov-hausdorff
distance
for
metric
trees
in
international
symposium
on
algorithms
and
computation
springer
pages
529-540
enrique
amigo
julio
gonzalo
javier
artiles
and
felisa
verdejo
2009
comparison
of
extrinsic
clustering
evaluation
metrics
based
on
formal
con
straints
information
retrieval
12(4):461-486
sanjeev
arora
yingyu
liang
and
tengyu
ma
2017
simple
but
tough-to-beat
baseline
for
sentence
em
beddings
in
international
conference
on
learning
representations
to
appear
alexander
bronstein
michael
bronstein
and
ron
kimmel
2006
efficient
computation
of
isometry-invariant
distances
between
surfaces
siam
journal
on
scientific
computing
28(5):1812
1836
mathieu
carrieére
steve
oudot
and
maks
ovs
janikov
2015
stable
topological
signatures
for
points
on
3d
shapes
in
computer
graphics
forum
wiley
online
library
volume
34
pages
1-12
frédéric
chazal
david
cohen-steiner
leonidas
guibas
facundo
mémoli
and
steve
oudot
2009
gromov-hausdortf
stable
signatures
for
shapes
using
persistence
in
computer
graphics
forum
wiley
online
library
volume
28
pages
1393-1403
cecil
jose
delfinado
and
herbert
edelsbrunner
1995
an
incremental
algorithm
for
betti
numbers
of
simplicial
complexes
on
the
3-sphere
computer
aided
geometric
design
12(7):771-784
herbert
edelsbrunner
david
letscher
and
afra
zomorodian
2002
topological
persistence
and
simplification
discrete
and
computational
geome
try
28(4):511-533
mikhael
gromov
jacques
lafontaine
and
pierre
pansu
1981
structures
métriques
pour
les
variétés
riemanniennes
andreas
hotho
andreas
niirnberger
and
gerhard
paaf}
2005
brief
survey
of
text
mining
in
ldv
forum
anna
huang
2008
similarity
measures
for
text
doc
ument
clustering
in
proceedings
of
the
sixth
new
zealand
computer
science
research
student
confer
ence
nzcsrsc2008
christchurch
new
zealand
pages
49-56
yoon
kim
2014
convolutional
neural
networks
for
sentence
classification
in
in
emnlp
ryan
kiros
yukun
zhu
ruslan
salakhutdinov
richard
zemel
raquel
urtasun
antonio
torralba
and
sanja
fidler
2015
skip-thought
vectors
in
advances
in
neural
information
processing
systems
pages
3294-3302
da
kuang
jaegul
choo
and
haesun
park
2015
non
negative
matrix
factorization
for
interactive
topic
modeling
and
document
clustering
in
partitional
clustering
algorithms
springer
pages
215-243
matt
kusner
yu
sun
nicholas
kolkin
kilian
weinberger
et
al
2015
from
word
embeddings
to
document
distances
in
jcml
volume
15
pages
957-966
quoc
v
le
and
tomas
mikolov
2014
distributed
rep
resentations
of
sentences
and
documents
in
pro
ceedings
of
the
31th
international
conference
on
machine
learning
icml
2014
beijing
china
21
26
june
2014
pages
1188—1196
andrew
l
maas
raymond
e
daly
peter
t
pham
dan
huang
andrew
y
ng
and
christopher
potts
2011
learning
word
vectors
for
sentiment
analysis
in
proceedings
of
the
49th
annual
meeting
of
the
association
for
computational
linguistics
human
language
technologies
association
for
computa
tional
linguistics
portland
oregon
usa
pages
142-150
http://www.aclweb.org/anthology/p11
1015
facundo
mémoli
2007
on
the
use
of
gromov
hausdorff
distances
for
shape
comparison
facundo
mémoli
and
guillermo
sapiro
2005
the
oretical
and
computational
framework
for
isometry
invariant
recognition
of
point
cloud
data
founda
tions
of
computational
mathematics
5(3):313-347
tomas
mikolov
ilya
sutskever
kai
chen
greg
cor
rado
and
jeff
dean
2013
distributed
representa
tions
of
words
and
phrases
and
their
compositional
ity
in
advances
in
neural
information
processing
systems
pages
3111-3119
timothy
miller
dmitriy
dligach
and
guergana
savova
2016
unsupervised
document
classifica
tion
with
informed
topic
models
acl
dmitriy
morozov
2008-2016
dyonisus
c++
li
brary
for
computing
persistent
homology
http
//mrzv.org/software/dionysus/
tetsuji
nakagawa
kentaro
inui
and
sadao
kuro
hashi
2010
dependency
tree-based
sentiment
classification
using
crfs
with
hidden
variables
in
human
language
technologies
the
2010
annual
conference
of
the
north
american
chapter
of
the
association
for
computational
linguis
tics
association
for
computational
linguistics
stroudsburg
pa
usa
hlt
10
pages
786-794
http://dl.acm.org/citation.cfm?id=1857999.1858119
bo
pang
and
lillian
lee
2005
seeing
stars
exploit
ing
class
relationships
for
sentiment
categorization
with
respect
to
rating
scales
in
proceedings
of
the
acl
vanessa
robins
1999
towards
computing
homology
from
finite
approximations
in
topology
proceed
ings
volume
24
pages
503-532
240
yossi
rubner
carlo
tomasi
and
leonidas
guibas
1998
metric
for
distributions
with
applications
to
image
databases
in
computer
vision
1998
sixth
international
conference
on
ieee
pages
59—66
michael
steinbach
george
karypis
vipin
kumar
et
al
2000
comparison
of
document
clustering
techniques
in
kdd
workshop
on
text
mining
sida
wang
and
christopher
d
manning
2012
baselines
and
bigrams
simple
good
sentiment
and
topic
classification
in
proceedings
of
the
50th
annual
meeting
of
the
association
for
com
putational
linguistics
short
papers
volume
2
association
for
computational
linguistics
stroudsburg
pa
usa
acl
12
pages
90-94
http://dl.acm.org/citation.cfm?id=2390665.2390688
peter
willett
1988
recent
trends
in
hierarchic
docu
ment
clustering
critical
review
information
pro
cessing
management
24(5):577-597
kelin
xia
and
guo-wei
wei
2014
persistent
ho
mology
analysis
of
protein
structure
flexibility
and
folding
international
journal
for
numerical
meth
ods
in
biomedical
engineering
30(8):814-844
wei
xu
and
yihong
gong
2004
document
clustering
by
concept
factorization
in
pro
ceedings
of
the
27th
annual
international
acm
sigir
conference
on
research
and
devel
opment
in
information
retrieval
acm
new
york
ny
usa
sigir
04
pages
202-209
https://doi.org/10.1145/1008992.1009029
xiaojin
zhu
2013
persistent
homology
an
introduc
tion
and
new
text
representation
for
natural
lan
guage
processing
in
proceedings
of
the
23rd
inter
national
joint
conference
on
artificial
intelligence
