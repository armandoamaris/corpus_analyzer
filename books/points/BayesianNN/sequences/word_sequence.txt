2007.06823v3
cs.lg
jan
2022
arxiv
hands-on
bayesian
neural
networks
tutorial
for
deep
learning
users
laurent
valentin
jospin
university
of
western
australia
australia
hamid
laga
murdoch
university
australia
farid
boussaid
university
of
western
australia
australia
wray
buntine
monash
university
australia
mohammed
bennamoun
university
of
western
australia
australia
abstract—modern
deep
learning
methods
constitute
incredibly
powerful
tools
to
tackle
myriad
of
challenging
problems
however
since
deep
learning
methods
operate
as
black
boxes
the
uncertainty
associated
with
their
predictions
is
often
challenging
to
quantify
bayesian
statistics
offer
formalism
to
understand
and
quantify
the
uncertainty
associated
with
deep
neural
network
predictions
this
tutorial
provides
deep
learning
practitioners
with
an
overview
of
the
relevant
literature
and
complete
toolset
to
design
implement
train
use
and
evaluate
bayesian
neural
networks
i.e
stochastic
artificial
neural
networks
trained
using
bayesian
methods
index
terms—bayesian
methods
bayesian
deep
learning
bayesian
neural
networks
approximate
bayesian
methods
1
introduction
deep
learning
has
led
to
revolution
in
machine
learning
providing
solutions
to
tackle
problems
that
were
traditionally
difficult
to
solve
however
deep
learning
models
are
prone
to
overfitting
which
adversely
affects
their
generalization
capabilities
1
they
also
tend
to
be
overconfident
about
their
predictions
when
they
provide
confidence
interval
this
is
problematic
for
applications
where
silent
failures
can
lead
to
dramatic
outcomes
e.g
autonomous
driving
2
medical
diagnosis
3
or
finance
4
consequently
many
approaches
have
been
proposed
to
mitigate
this
risk
5
among
them
the
bayesian
paradigm
provides
rigorous
framework
to
analyze
and
train
uncertainty-aware
neural
networks
and
more
generally
to
support
the
development
of
learning
algorithms
the
bayesian
paradigm
in
statistics
contrasts
with
the
frequentist
paradigm
with
major
area
of
distinction
in
hypothesis
testing
6
it
is
based
on
two
simple
ideas
the
first
is
that
probability
is
measure
of
belief
in
the
occurrence
of
events
rather
than
the
limit
in
the
frequency
of
occurrence
when
the
number
of
samples
goes
toward
infinity
as
assumed
in
the
frequentist
paradigm
the
second
idea
is
that
prior
beliefs
influence
posterior
beliefs
bayes
theorem
which
states
that
p(d|h)p(h
p(d,h
phd)=
f
=+
hid
p(d
fh
d,h")dh
summarizes
this
interpretation
formula
1
is
still
true
in
the
frequentist
interpretation
where
and
are
considered
as
sets
of
outcomes
the
bayesian
interpretation
considers
to
corresponding
author
laurent
valentin
email
lau
rent.jospin
@research.uwa.edu.au
jospin
point
estimate
neural
network
bayesian
neural
network
architecture
functional
model
supervision
§1v-d
}—l
loss
§l1
stochastic
model
§1v
likelihood
§iv-a
p(y|x,0
prior
§iv-c
p(6
regularization
penalization
in
loss
§|v-c3
inference
§v-a
bopot
mcmc
§v-a
~
variational
inference
mc-dropout
§v-e1
stochastic
variational
inference
§v-b
training
gradient
descent
algorithms
e.g
sgd
adam
.
backpropagation
bayes-by-backprop
§v-c
fig
1
ilustration
of
the
correspondence
between
the
concepts
used
in
deep
learning
for
point-estimate
neural
networks
and
their
counterparts
in
bayesian
neural
networks
bnns
be
hypothesis
about
which
one
holds
some
prior
belief
and
to
be
some
data
that
will
update
one’s
belief
about
h
the
probability
distribution
p(d|h
is
called
the
likelihood
it
encodes
the
aleatoric
uncertainty
in
the
model
i.e
the
uncer
tainty
due
to
the
noise
in
the
process
p(h
is
the
prior
and
y
p(d,h")dh
the
evidence
p(h|d
is
called
the
pmterlor
it
encodes
the
epistemic
uncertainty
i.e
the
uncertainty
due
to
the
lack
of
data
p(d|h)p(h
p(d
h
is
the
joint
probability
of
and
h
using
bayes
formula
to
train
predictor
can
be
understood
as
learning
from
the
data
d
in
other
words
the
bayesian
paradigm
not
only
offers
solid
approach
for
the
quantification
of
uncertainty
in
deep
learning
models
but
also
provides
mathematical
framework
to
understand
many
regularization
techniques
and
learning
strategies
that
are
already
used
in
classic
deep
learning
7
section
iv-c3
bayesian
neural
networks
bnns
8
9
10
are
stochastic
neural
networks
trained
using
bayesian
approach
there
is
rich
literature
about
bnns
and
the
related
field
of
bayesian
stochastic
model
inference
training
summary
ariational
posterior
prior
mcmc
variational
inference
if
needed
95(0
»(6
p(ylz
6
gibbs
sampling
metropolis
hasting
hmc
nuts
.
functional
model
svi
bayes
by
backprop
probabilistic
backpropagation
.
po(x
sgld
recast
.
dl-specific
training
data
{(&1,y1
tn
y,)}
mc-dropout
deep
ensembles
kfac
swag
.
b
fig
2
workflow
to
design
a
train
b
and
use
bnn
for
predictions
c
deep
learning
which
is
referred
to
by
wang
and
yeung
11
as
the
conjoint
use
of
deep
learning
for
perception
and
traditional
bayesian
models
for
inference
however
navigating
through
this
literature
is
challenging
without
some
prior
background
in
bayesian
statistics
this
brings
an
additional
layer
of
com
plexity
for
deep
learning
practitioners
interested
in
building
and
using
bnns
this
paper
conceived
as
tutorial
presents
unified
workflow
to
design
implement
train
and
evaluate
bnn
figure
2
it
also
provides
an
overview
of
the
relevant
litera
ture
where
large
number
of
approaches
have
been
developed
to
efficiently
train
and
use
bnns
good
knowledge
of
those
different
methods
is
prerequisite
for
an
efficient
use
of
bnns
in
big
data
applications
of
deep
learning
in
this
tutorial
we
assume
that
the
reader
is
already
familiar
with
the
concepts
of
traditional
deep
learning
such
as
artificial
neural
networks
training
algorithms
supervision
strategies
and
loss
functions
13
this
paper
focuses
on
exploring
the
correspondences
between
traditional
deep
learning
approaches
and
bayesian
methods
figure
1
it
is
intended
to
motivate
and
help
researchers
and
students
to
use
bnns
in
measuring
uncertainty
for
problems
in
their
respective
fields
of
study
and
research
helping
them
relate
their
existing
knowledge
in
deep
learning
to
the
relevant
bayesian
methods
the
remaining
parts
of
this
paper
are
organized
as
follows
section
ii
introduces
the
concept
of
bnn
section
iii
presents
the
motivations
for
bnns
as
well
as
their
appli
cations
section
iv
explains
how
to
design
the
stochastic
model
associated
with
bnn
section
explores
the
most
important
algorithms
used
for
bayesian
inference
and
how
they
were
adapted
for
deep
learning
section
vi
reviews
bnn
simplification
methods
section
vii
presents
the
methods
used
to
evaluate
the
performance
of
bnn
finally
section
viii
concludes
the
paper
the
supplementary
material
contains
gallery
of
practical
examples
illustrating
the
theoretical
concepts
presented
in
sections
1l
iv
and
of
the
main
paper
each
example
source
code
is
also
available
online
on
github
note
that
some
other
authors
use
different
definition
of
bayesian
deep
learning
which
is
closer
to
the
idea
of
bnn
12
to
provide
implementation
examples
of
the
most
important
algorithms
to
work
with
bnns
ii
what
is
bayesian
neural
network
bnn
is
defined
slightly
differently
across
the
literature
but
commonly
agreed
definition
is
that
bnn
is
stochastic
artificial
neural
network
trained
using
bayesian
inference
the
goal
of
artificial
neural
networks
anns
is
to
rep
resent
an
arbitrary
function
®(x
traditional
anns
such
as
feedforward
networks
and
recurrent
networks
are
built
using
one
input
layer
i
succession
of
hidden
layers
l;;t
=1,...,n
1
and
one
output
layer
[
here
is
the
total
number
of
layers
in
the
simplest
architecture
of
feedforward
networks
each
layer
is
represented
as
linear
transformation
followed
by
nonlinear
operation
s
also
known
as
an
activation
function
ly
==z
l
y=
here
w
b
are
the
parameters
of
the
network
where
are
the
weights
of
the
network
connections
and
the
biases
given
ann
architecture
represents
set
of
functions
isomorphic
to
the
set
of
possible
parameters
6
deep
learning
is
the
process
of
regressing
the
parameters
from
the
training
data
d
where
is
composed
of
series
of
input
and
their
corresponding
labels
y
the
standard
approach
is
to
approximate
minimal
cost
point
estimate
of
the
network
parameters
0
ic
single
value
for
each
parameter
fig
ure
3a
using
the
backpropagation
algorithm
with
all
other
possible
parametrizations
of
the
network
discarded
the
cost
function
is
often
defined
as
the
log
likelihood
of
the
training
set
sometimes
with
regularization
term
included
from
statistician’s
point
of
view
this
is
maximum
likelihood
esti
mation
mle
or
maximum
posteriori
map
estimation
when
regularization
is
used
the
point
estimate
approach
which
is
the
traditional
ap
proach
in
deep
learning
is
relatively
easy
to
deploy
with
modern
algorithms
and
software
packages
but
tends
to
lack
explainability
14
the
final
model
might
also
generalize
wili_y
b
vi€
ln
@
n
fig
3
a
point
estimate
neural
network
b
stochastic
neural
network
with
probability
distribution
for
the
activations
and
c
stochastic
neural
network
with
probability
distribution
over
the
weights
in
unforeseen
and
overconfident
ways
on
out-of-training
distribution
data
points
15
16
this
property
in
addition
to
the
inability
of
anns
to
say
i
don’t
know
is
problematic
for
many
critical
applications
of
all
the
techniques
that
exist
to
mitigate
this
17
stochastic
neural
networks
have
proven
to
be
one
of
the
most
generic
and
flexible
stochastic
neural
networks
are
type
of
ann
built
by
introducing
stochastic
components
into
the
network
this
is
performed
by
giving
the
network
either
stochastic
activation
figure
3b
or
stochastic
weights
figure
3c
to
simulate
multiple
possible
models
with
their
associated
probability
distribution
p(6
thus
bnns
can
be
considered
special
case
of
ensemble
learning
18
the
main
motivation
behind
ensemble
learning
comes
from
the
observation
that
aggregating
the
predictions
of
large
set
of
average-performing
but
independent
predictors
can
lead
to
better
predictions
than
single
well-performing
expert
predictor
19
20
stochastic
neural
networks
might
improve
their
performance
over
their
point
estimate
counterparts
in
similar
fashion
but
this
is
not
their
main
aim
rather
the
main
goal
of
using
stochastic
neural
network
architecture
is
to
obtain
better
idea
of
the
uncertainty
associated
with
the
underlying
processes
this
is
accomplished
by
comparing
the
predictions
of
multiple
sampled
model
parametrizations
6
if
the
different
models
agree
then
the
uncertainty
is
low
if
they
disagree
then
the
uncertainty
is
high
this
process
can
be
summarized
as
follows
6~
p(8
y=>o9(x
where
represents
random
noise
to
account
for
the
fact
that
the
function
is
only
an
approximation
bnn
can
then
be
defined
as
any
stochastic
artificial
neural
network
trained
using
bayesian
inference
21
to
design
bnn
the
first
step
is
the
choice
of
deep
neural
network
architecture
i.e
functional
model
then
one
has
to
choose
stochastic
model
i.e
prior
distribution
over
the
possible
model
parametrization
p(@
and
prior
confidence
in
the
predictive
power
of
the
model
p(y|x,8
figure
2a
the
model
parametrization
can
be
considered
to
be
the
hypothesis
and
the
training
set
is
the
data
d
the
choice
of
bnn’s
stochastic
model
is
somehow
equivalent
to
the
choice
of
loss
function
when
training
point
estimate
neural
network
see
section
iv-c3
in
the
rest
of
this
paper
we
will
denote
the
model
parameters
by
6
the
training
set
3
by
d
the
training
inputs
by
d
and
the
training
labels
by
d
by
applying
bayes
theorem
and
enforcing
independence
between
the
model
parameters
and
the
input
the
bayesian
posterior
can
be
written
as
p(dy|da,0)p(0
p(6|d
p(dy|dz
0)p(6
4
the
bayesian
posterior
for
complex
models
such
as
artifi
cial
neural
networks
is
high
dimensional
and
highly
non
convex
probability
distribution
22
this
complexity
makes
computing
and
sampling
it
using
standard
methods
an
in
tractable
problem
especially
because
computing
the
evidence
jop(dy|
do
0")p(6")d6
is
difficult
to
address
this
problem
two
broad
approaches
have
been
introduced
1
markov
chain
monte
carlo
and
2
variational
inference
these
are
presented
in
more
details
in
section
v
when
using
bnn
for
prediction
the
probability
distribu
tion
p(y|x
d
12
called
the
marginal
and
which
quantifies
the
model’s
uncertainty
on
its
prediction
is
of
particular
interest
given
p(0|d
p(y|z
d
can
be
computed
as
wole.d
o@d
in
practice
p(y|x
d
is
sampled
indirectly
using
equation
3
the
final
prediction
can
be
summarized
by
statistics
computed
using
monte
carlo
approach
figure
2c
large
set
of
weights
6
is
sampled
from
the
posterior
and
used
to
compute
series
of
possible
outputs
y
as
shown
in
algorithm
1
which
corresponds
to
samples
from
the
marginal
algorithm
inference
procedure
for
bnn
p(dy|
dz
0)p(6
define
p(68|d
f
p
fep(dy‘dm,el)p(el)dol
for
=0
to
do
draw
6
p(6|d
y
o
x
end
for
return
{y;li
0,n)}
={8;]i
0,n)}
in
algorithm
1
is
set
of
samples
from
p(y|x
d
and
collection
of
samples
from
p(6|d
usually
aggregates
are
computed
on
those
samples
to
summarize
the
uncertainty
of
the
bnn
and
obtain
an
estimator
for
the
output
y
this
estimator
is
denoted
by
when
performing
regression
the
procedure
that
is
usually
used
to
summarize
the
predictions
of
bnn
is
model
aver
aging
23
9=
d
(
©
this
approach
is
so
common
in
ensemble
learning
that
it
is
sometimes
called
ensembling
to
quantify
uncertainty
the
covariance
matrix
can
be
computed
as
follows
@o,(z
9
to
(
pt
d
ylz.d
ta7
|e]-1
oo
when
performing
classification
the
average
model
prediction
will
give
the
relative
probability
of
each
class
which
can
be
considered
measure
of
uncertainty
p
(
®
the
final
prediction
is
taken
as
the
most
likely
class
argmaxp
p
this
definition
considers
bnns
as
discriminative
models
i.e
models
that
aim
to
reconstruct
target
variable
given
observations
«
this
excludes
generative
models
although
there
are
examples
of
generative
anns
based
on
the
bayesian
formalism
e.g
variational
autoencoders
24
those
are
out
of
the
scope
of
this
tutorial
iii
advantages
of
bayesian
methods
for
deep
learning
one
of
the
major
critiques
of
bayesian
methods
is
that
they
rely
on
prior
knowledge
this
is
especially
true
in
deep
learn
ing
as
deriving
any
insight
about
plausible
parametrization
for
given
model
before
training
is
very
challenging
thus
why
use
bayesian
methods
for
deep
learning
discriminative
mod
els
implicitly
represent
the
conditional
probability
p(y|z
)
and
bayes
formula
is
an
appropriate
tool
to
invert
conditional
probabilities
even
if
one
has
little
insight
about
p(@
priori
while
there
are
strong
theoretical
principles
and
schema
upon
which
this
bayes
formula
can
be
based
25
this
section
focuses
on
some
practical
benefits
of
using
bnns
first
bayesian
methods
provide
natural
approach
to
quantify
uncertainty
in
deep
learning
since
bnns
have
better
calibration
than
classical
neural
networks
26
27
28
i.e
their
uncertainty
is
more
consistent
with
the
observed
errors
they
are
less
often
overconfident
or
underconfident
second
bnn
allows
distinguishing
between
the
epis
temic
uncertainty
p(@|d
and
the
aleatoric
uncertainty
p(y|z,0
29
this
makes
bnns
very
data-efficient
since
they
can
learn
from
small
dataset
without
overfitting
30
at
prediction
time
out-of-training
distribution
points
will
have
high
epistemic
uncertainty
instead
of
blindly
giving
wrong
prediction
third
the
no-free-lunch
theorem
for
machine
learning
31
can
be
interpreted
as
stating
that
any
supervised
learning
algorithm
includes
some
implicit
prior
bayesian
methods
when
used
correctly
will
at
least
make
the
prior
explicit
integrating
prior
knowledge
into
anns
which
work
as
black
boxes
is
difficult
but
not
impossible
in
bayesian
deep
learning
priors
are
often
considered
as
soft
constraints
anal
ogous
to
regularization
or
data
transformations
such
as
data
augmentation
in
traditional
deep
learning
see
section
iv-c
most
regularization
methods
used
for
point
estimate
neural
networks
can
be
understood
from
bayesian
perspective
as
setting
prior
see
section
iv-c3
finally
the
bayesian
paradigm
enables
the
analysis
of
learning
methods
number
of
those
methods
initially
not
presented
as
bayesian
can
be
implicitly
understood
as
being
approximate
bayesian
e.g
regularization
section
iv-c3
algorithm
active
learning
loop
with
bnn
while
and
threshold
and
c
maxc
y|tmas
do
draw
{6
p(8|d)|i
0,n)}
for
do
syle.p
o1
yo.co
po:(®
9
po,(x
9)7
if
3y
eyz,,,..0
then
tmaz
end
if
end
for
dz
dy
u{@max}
dy
dy
{oracle(@max
}
u=u\{zmx}
c=c+1
end
while
algorithm
online
learning
loop
with
bnn
define
p(6
p(0)o
while
true
do
dy
i|dai
0)p(8
p\lpy,i|ve,i
v)p0
)
define
p(0|d
f——f
po
dy
d
0)p(8
007
define
p(0);+1
p(0|d
end
while
or
ensembling
section
v-e2b
in
fact
most
of
the
bnns
used
in
practice
rely
on
methods
that
are
approximately
or
implicitly
bayesian
section
v-e
since
the
exact
algorithms
are
computationally
too
expensive
the
bayesian
paradigm
also
provides
systematic
framework
to
design
new
learning
and
regularization
strategies
even
for
point
estimate
models
bnns
have
been
used
in
many
fields
to
quantify
uncertainty
e.g
in
computer
vision
32
network
traffic
monitoring
33
aviation
34
civil
engineering
35
36
hydrology
37
astronomy
38
electronics
39
and
medicine
40
bnns
are
useful
in
1
active
learning
41
42
where
an
oracle
e.g
human
annotator
crowd
an
expensive
algorithm
can
label
new
points
from
an
unlabeled
dataset
u
the
model
needs
to
determine
which
points
should
be
submitted
to
the
oracle
to
maximize
its
performance
while
minimizing
the
calls
to
the
oracle
bnns
are
also
useful
in
2
online
learning
43
where
the
model
is
retrained
multiple
times
as
new
data
become
available
for
active
learning
data
points
in
the
training
set
with
high
epistemic
uncertainty
are
scheduled
to
be
labeled
with
higher
priority
see
algorithm
2
in
contrast
in
online
learning
previous
posteriors
can
be
recycled
as
priors
when
new
data
become
available
to
avoid
the
so-called
problem
of
catastrophic
forgetting
44
see
algorithm
3
iv
setting
the
stochastic
model
for
bayesian
neural
network
designing
bnn
requires
choosing
functional
model
and
stochastic
model
this
tutorial
will
not
cover
the
design
of
the
functional
model
as
almost
any
model
used
for
point
estimate
networks
can
be
used
as
functional
model
for
bnn
furthermore
rich
literature
on
the
subject
exists
already
see
for
example
45
instead
this
section
will
//
fig
4
the
different
symbols
pgm
a
observed
variables
are
in
colored
circles
b
unobserved
variables
are
in
white
circles
c
deterministic
variables
are
in
dashed
circles
and
d
parameters
are
in
rectangles
plates
represented
as
rectangle
around
subgraph
indicate
multiple
independent
instances
of
the
subgraph
for
batch
of
variables
e
focus
on
how
to
design
the
stochastic
model
section
iv-a
introduces
probabilistic
graphical
models
pgms
tool
used
to
represent
the
relationships
between
the
model’s
stochastic
variables
section
iv-b
details
how
to
derive
the
posterior
for
bnn
from
its
pgm
section
iv-c
discusses
how
to
choose
the
probability
laws
used
as
priors
finally
section
iv-d
presents
how
the
choice
of
pgm
can
affect
the
degree
of
supervision
or
incorporate
other
forms
of
prior
knowledge
into
the
model
a
probabilistic
graphical
models
probabilistic
graphical
models
pgms
use
graphs
to
repre
sent
the
interdependence
of
multivariate
stochastic
variables
and
subsequently
decompose
their
probability
distributions
pgms
cover
large
variety
of
models
the
type
of
pgms
this
tutorial
focuses
on
are
bayesian
belief
networks
bbn
which
are
pgms
whose
graphs
are
acyclic
and
directed
we
refer
the
reader
to
46
for
more
details
on
how
to
represent
learning
algorithms
using
general
pgms
in
pgm
variables
v
are
the
nodes
in
the
graph
different
symbols
are
used
to
distinguish
the
nature
of
the
considered
variables
figure
4
directed
link
which
is
the
only
type
of
link
allowed
in
bbn
means
that
the
probability
distribution
of
the
target
variable
is
defined
conditioned
on
the
source
variable
the
fact
that
the
bbn
is
acyclic
allows
the
computation
of
the
joint
probability
distribution
of
all
the
variables
v
in
the
graph
p,y
,
plwilparents(v
10
i=1
the
type
of
distribution
used
to
define
the
conditional
prob
abilities
p(v;|parents(v
depends
on
the
context
once
the
conditional
probabilities
are
defined
the
bbn
describes
data
generation
process
parents
are
sampled
before
their
children
this
is
always
possible
since
the
graph
is
acyclic
all
the
variables
together
represent
sample
from
the
joint
probability
distribution
p(v1y,...,v
models
usually
learn
from
multiple
examples
sampled
from
the
same
distribution
to
highlight
this
fact
the
plate
notation
figure
4e
has
been
introduced
plate
indicates
that
the
variables
v
.
v
in
the
subgraph
encapsulated
by
the
plate
are
copied
along
given
batch
dimension
plate
implies
independence
between
all
the
duplicated
nodes
this
fact
can
an
\;r
at
aajala
vi
a
p®
fig
5
bbns
with
a
coefficients
as
stochastic
variables
and
b
activations
as
stochastic
variables
be
exploited
to
compute
the
joint
probability
of
batch
{(v1
.
vp)p:b=1,...,|b|}
as
p(b
an
in
pgm
the
observed
variables
depicted
in
figure
4a
using
colored
circles
are
treated
as
the
data
the
unobserved
also
called
latent
variables
represented
by
white
circle
in
fig
ure
4b
are
treated
as
the
hypothesis
from
the
joint
probability
derived
from
the
pgm
defining
the
posterior
for
the
latent
variables
given
the
observed
variables
is
straightforward
using
bayes
formula
p('ulmem|'uobs
p('vost
ulmem)»
12
the
joint
distribution
p(vobs
viaent
is
then
used
by
the
differ
ent
inference
algorithms
see
section
v
b
defining
the
stochastic
model
of
bnn
from
pgm
consider
the
two
models
presented
in
figure
5
with
both
the
bnn
and
the
corresponding
bbn
depicted
the
bnn
with
stochastic
weights
figure
5a
if
meant
to
perform
regression
could
represent
the
following
data
generation
process
0~
p(6
n1
d
p(ylz,0
n(po(z
x
the
choice
of
using
normal
laws
p
x
with
mean
and
covariance
x
is
arbitrary
but
is
common
in
practice
because
of
its
good
mathematical
properties
for
classification
the
model
samples
the
prediction
from
categorical
law
cat(p
i.e
~p(6
n(p
%
p(yle
8
cat(®g
then
one
can
use
the
fact
that
multiple
data
points
from
the
training
set
are
independent
as
indicated
by
the
plate
notation
in
figure
5
to
write
the
probability
of
the
training
set
as
p(dy|dz,0)=
pylz.0
x,y)ed
13
14
15
in
the
case
of
stochastic
activations
figure
5b
the
data
generation
process
might
become
ly==
~plilli—1
siv(wili—1
b;,%
vi€
1,n
y=1
16
the
formulation
of
the
joint
probability
is
slightly
more
complex
as
we
have
to
account
for
the
chain
of
dependencies
spanned
by
the
bbn
over
the
multiple
latent
variables
y
_1
11
h
p(hhm
an
lo.ln)ed
\i=1
p<dy<l[1,n—l]‘dm
it
is
sometimes
possible
and
often
desirable
to
define
p(l;|l;—1
such
that
the
bnns
described
in
figure
5a
and
in
figure
5b
can
be
considered
equivalent
for
instance
sampling
as
w~
n(pw
zw
b~
n(py
)
l=s(wl_,+b
18
is
equivalent
to
sampling
as
l~
sv
pwlo1+pp
tol1)tew
i
®1_1)+34
19
where
denotes
kronecker
product
the
basic
bayesian
regression
architecture
shown
in
fig
ure
5a
is
more
common
in
practice
the
alternative
architecture
shown
in
figure
5b
is
sometimes
used
as
it
allows
compressing
the
number
of
variational
parameters
when
using
variational
inference
47
see
also
section
v
c
setting
the
priors
setting
the
prior
of
deep
neural
network
is
often
not
an
intuitive
task
the
main
problem
is
that
it
is
not
truly
explicit
how
models
with
very
large
number
of
parameters
and
nontrivial
architecture
such
as
an
ann
will
generalize
for
given
parametrization
48
in
this
section
we
first
present
the
common
practice
discuss
the
issues
related
to
the
statistical
unidentifiability
of
annss
and
then
show
the
link
between
the
prior
for
bnns
and
regularization
for
the
point
estimate
algorithms
finally
we
present
method
to
build
the
prior
from
high
level
knowledge
1
good
default
prior
for
basic
architectures
such
as
bayesian
regression
figure
sa
standard
procedure
is
to
use
normal
prior
with
zero
mean
and
diagonal
covariance
oi
on
the
coefficients
of
the
network
p(6
n(0,01
20
this
approach
is
equivalent
to
weighted
{»
regularization
with
weights
1/0
when
training
point
estimate
network
as
will
be
demonstrated
in
section
iv-c3
the
documentation
of
the
probabilistic
programming
language
stan
49
provides
examples
on
how
to
choose
knowing
the
expected
scale
of
the
considered
parameters
50
although
such
an
approach
is
often
used
in
practice
there
is
no
theoretical
argument
that
makes
it
better
than
any
other
formulation
51
the
normal
law
is
preferred
due
to
its
mathematical
properties
and
the
simple
formulation
of
its
log
which
is
used
in
most
of
the
learning
algorithms
2
addressing
unidentifiability
in
bayesian
neural
net
works
one
of
the
main
problems
with
bayesian
deep
learning
is
that
deep
neural
networks
are
overparametrized
models
i.e
they
have
many
equivalent
parametrizations
52
this
is
an
example
of
statistical
unidentifiability
which
can
lead
to
complex
multimodal
posteriors
that
are
hard
to
sample
and
approximate
when
training
bnn
22
there
are
two
solutions
to
deal
with
this
issue
1
changing
the
functional
model
parametrization
or
2
constraining
the
support
of
the
prior
to
remove
unidentifiability
the
two
most
common
classes
of
nonuniqueness
in
anns
are
weight-space
symmetry
and
scaling
symmetry
53
both
are
not
concern
for
point
estimate
neural
networks
but
might
be
for
bnns
weight-space
symmetry
implies
that
one
can
build
an
equivalent
parametrization
of
an
ann
with
at
least
one
hidden
layer
this
is
achieved
by
permuting
two
rows
in
w
b
the
weights
and
their
corresponding
bias
b
of
one
of
the
hidden
layers
as
well
as
the
corresponding
columns
in
the
following
layer’s
weight
matrix
w
this
means
that
as
the
number
of
hidden
layers
and
the
number
of
units
in
the
hidden
layers
grow
the
number
of
equivalent
representations
which
would
roughly
correspond
to
the
modes
in
the
posterior
distribution
grows
factorially
mitigation
strategy
is
to
enforce
the
bias
vector
in
each
layer
to
be
sorted
in
an
ascending
or
descending
order
however
the
practical
effects
of
doing
so
may
be
to
degrade
optimization
weight
space
symmetry
may
implicitly
support
the
exploration
of
the
parameter
space
during
the
early
stages
of
the
optimization
scaling
symmetry
is
an
unidentifiability
problem
arising
when
using
nonlinearities
with
the
property
s(az
as(z
which
is
the
case
of
relu
and
leaky-relu
two
popular
nonlinearities
in
modern
machine
learning
in
this
case
assign
ing
the
weights
w
w1
to
two
consecutive
layers
and
/41
becomes
strictly
equivalent
to
assigning
aw
1/a)w
4
this
can
reduce
the
convergence
speed
for
point
estimate
neural
networks
problem
that
is
addressed
in
practice
with
various
activation
normalization
techniques
54
bnns
are
slightly
more
complex
as
the
scaling
symmetry
influences
the
posterior
shape
making
it
harder
to
approximate
givens
transformations
also
called
givens
rotations
have
been
pro
posed
as
mean
to
constrain
the
norm
of
the
hidden
layers
53
and
address
the
scaling
symmetry
issue
in
practice
using
gaussian
prior
already
reduces
the
scaling
symmetry
problem
as
it
favors
weights
with
the
same
frobenius
norm
on
each
layer
soft
version
of
the
activation
normalization
can
also
be
implemented
by
using
consistency
condition
see
section
iv-c4
the
additional
complexity
associated
with
sampling
the
network
parameters
in
constrained
space
to
perfectly
remove
the
scaling
symmetry
is
computationally
prohibitive
we
provide
in
the
practical
example
iii
of
the
supplementary
material
additional
discussion
on
this
issue
using
the
paperfold
practical
example
3
the
link
between
regularization
and
priors
the
usual
learning
procedure
for
point
estimate
neural
network
is
to
find
the
set
of
parameters
that
minimize
loss
function
built
using
the
training
set
d
arg
min
lossp
b
6
2n
©
o®
®|®
c
a
noisy
labels
b
semi-supervised
learning
c
data
augmentation
d
meta-learning
e
self-supervised
learning
fig
6
different
examples
of
pgms
to
adapt
the
learning
strategy
for
given
bnn
with
stochastic
weights
assuming
that
the
loss
is
defined
as
minus
the
log-likelihood
function
up
to
an
additive
constant
the
problem
can
be
rewritten
as
6=
argmax
p(dy|dg
6
22
which
would
be
the
first
half
of
the
model
according
to
the
bayesian
paradigm
now
assume
that
we
also
have
prior
for
6
and
we
want
to
find
the
most
likely
point
estimate
from
the
posterior
the
problem
can
be
reformulated
as
argmax
p(dy|
dy
0)p(6
23
next
one
would
go
back
to
log-likelihood
formulation
argmin
lossp
p
(
+reg(8
24
which
is
easier
to
optimize
equation
24
is
how
regulariza
tion
is
usually
applied
in
machine
learning
and
in
many
other
fields
another
argument
less
formal
is
that
regularization
acts
as
soft
constraint
on
the
search
space
in
manner
similar
to
what
prior
does
for
posterior
4
prior
with
consistency
condition
regularization
can
also
be
implemented
with
consistency
condition
c(0,x
which
is
function
used
to
measure
how
well
the
model
respects
some
hypothesis
given
parametrization
and
an
input
for
example
c
can
be
set
to
favor
sparse
or
regular
predictions
to
encourage
monotonicity
of
predictions
with
respect
to
some
input
variables
e.g
the
probability
of
getting
the
flu
increases
with
age
or
to
favor
decision
boundaries
in
low
density
regions
when
using
semi-supervised
learning
see
section
iv-d1
can
be
seen
as
the
relative
log
likelihood
of
prediction
given
the
input
and
parameter
set
8
thus
it
can
be
included
in
the
prior
to
this
end
c
should
be
averaged
over
all
possible
inputs
c(9):/c(0,z)p(m)dz4
in
practice
as
p(x
is
unknown
c(0
is
approximated
from
the
features
in
the
lraining
set
c(0,x
ze\d
25
26
we
can
now
write
function
proportional
to
the
prior
with
the
consistency
condition
included
—
c(8,z
meld
p(6|dz
p(0
exp
@7
where
p(8
is
the
prior
without
the
consistency
condition
d
degree
of
supervision
and
alternative
forms
of
prior
knowledge
the
architecture
presented
in
section
iv-b
focuses
mainly
on
the
use
of
bnns
in
supervised
learning
setting
how
ever
in
real
world
applications
obtaining
ground-truth
labels
can
be
expensive
thus
new
learning
strategies
should
be
adopted
55
we
will
now
present
how
to
adapt
bnns
for
different
degrees
of
supervision
while
doing
so
we
will
also
demonstrate
how
pgms
in
general
and
bbns
in
particular
are
useful
in
designing
or
interpreting
learning
strategies
in
particular
the
formulation
of
the
bayesian
posterior
which
is
derived
from
the
different
pgms
presented
in
figure
6
can
also
be
used
for
point
estimate
neural
network
to
obtain
suitable
loss
function
to
search
for
an
map
estimator
for
the
parameters
section
iv-c3
we
also
provide
practical
example
in
the
supplementary
material
practical
example
it
to
illustrate
how
such
strategies
can
be
implemented
for
an
actual
bnn
1
noisy
labels
and
semi-supervised
learning
the
inputs
d
in
the
training
sets
can
be
uncertain
either
because
the
labels
d
are
corrupted
by
noise
56
or
because
labels
are
missing
for
number
of
points
in
the
case
of
noisy
labels
one
should
extend
the
bbn
to
add
new
variable
for
the
noisy
labels
conditioned
on
figure
6a
it
is
common
as
the
noise
level
itself
is
often
unknown
to
add
variable
to
characterize
the
noise
frenay
er
al
57
proposed
taxonomy
of
the
different
approaches
used
to
integrate
in
pgm
figure
7
they
distinguish
three
cases
noise
completely
at
random
ncar
noise
at
random
nar
and
noise
not
at
random
nnar
models
in
the
ncar
model
the
noise
is
independent
of
any
other
variable
i.e
it
is
homoscedastic
in
the
nar
model
is
dependent
on
the
true
label
but
remains
independent
of
the
features
nnac
models
also
account
for
the
influence
of
the
features
x
e.g
if
the
level
of
noise
in
an
image
increases
then
the
probability
that
the
image
has
been
mislabeled
also
increases
both
nar
and
nnac
models
represent
heteroscedastic
i.e
the
antonym
of
homoscedastic
noise
these
noise-aware
pgms
are
slightly
more
complex
than
purely
supervised
bnn
as
presented
in
section
iv-b
however
they
can
be
treated
in
similar
fashion
by
deriv
ing
the
formula
for
the
posterior
from
the
pgm
equation
12
and
applying
the
chosen
inference
algorithm
for
the
nnar
model
the
most
generic
stochastic
model
of
the
three
described
above
since
the
ncar
and
nar
models
are
special
cases
of
the
nnar
model
the
posterior
becomes
p(y,0,0|d
o<
p(dgly,o)p(o|de,y)p(y|dx
0)p(8
28
during
the
prediction
phase
and
can
simply
be
discarded
for
each
tuple
y,o,0
sampled
from
the
posterior
in
the
case
of
partially
labeled
data
figure
6b
also
known
as
semi-supervised
learning
the
dataset
is
split
into
labeled
and
unlabeled
examples
in
theory
this
pgm
can
be
considered
equivalent
to
the
one
used
in
the
supervised
learning
case
depicted
in
figure
5a
but
in
this
case
the
unobserved
data
would
bring
no
information
the
additional
information
of
unlabeled
data
comes
from
the
prior
and
only
the
prior
similar
to
traditional
machine
learning
the
most
common
approaches
to
implement
semi-supervised
learning
in
bayesian
learning
are
either
to
use
some
type
of
data-driven
regularization
58
or
to
rely
on
pseudo
labels
59
data-driven
regularization
implies
modifying
the
prior
assumptions
and
thus
the
stochastic
model
to
be
able
to
extract
meaningful
information
from
the
unlabeled
dataset
u
there
are
two
common
ways
to
approach
this
process
the
first
one
is
to
condition
the
prior
distribution
of
the
model
parameters
on
the
unlabeled
examples
to
favor
certain
properties
of
the
model
such
as
decision
boundary
in
low
density
region
using
distribution
p(@|u
instead
of
p
this
implies
formulating
the
stochastic
model
as
p(8|d
p(ly|lx
0)p(0]u
29
where
p(@|u
is
prior
with
consistency
condition
as
defined
in
equation
27
the
consistency
condition
usually
expresses
the
fact
that
points
that
are
close
to
each
other
should
lead
to
the
same
prediction
e.g
graph
laplacian
norm
regularization
60
the
second
way
is
to
assume
some
kind
of
dependency
across
the
observed
and
unobserved
labels
in
the
dataset
this
type
of
semi-supervised
bayesian
learning
relies
either
on
an
undirected
pgm
61
to
build
the
prior
or
at
least
does
not
assume
independence
between
different
training
pairs
z,y
62
to
keep
things
simple
we
represent
this
fact
by
dropping
the
plate
around
in
figure
6b
the
posterior
is
written
in
the
usual
way
equation
4
the
main
difference
is
that
p(dy|d,,0
is
chosen
to
enforce
some
kind
of
consistency
across
the
dataset
for
example
one
can
assume
that
two
close
points
are
likely
to
have
similar
labels
with
level
of
uncertainty
that
increases
with
the
distance
op
op
®
fig
7
bbns
corresponding
to
a
the
noise
completely
at
random
ncar
b
noise
at
random
nar
and
c
noise
not
at
random
nnar
models
from
57
both
approaches
have
similar
effect
and
the
choice
of
one
over
the
other
will
depend
on
the
mathematical
formulation
favored
to
build
the
model
the
semi-supervised
learning
strategy
can
also
be
reformu
lated
as
having
weak
predictor
capable
of
generating
some
pseudo
labels
g
sometimes
with
some
confidence
level
many
of
the
algorithms
used
for
semi-supervised
learning
use
an
initial
version
of
the
model
trained
with
the
labeled
examples
63
to
generate
the
pseudo
labels
and
train
the
final
model
with
g
this
is
problematic
for
bnns
when
the
prediction
uncertainty
is
accounted
for
reducing
the
uncertainty
associ
ated
with
the
unlabeled
data
becomes
impossible
at
least
not
without
an
additional
hypothesis
in
the
prior
even
if
it
is
less
current
in
practice
using
simpler
model
64
to
obtain
the
pseudo
labels
can
help
mitigate
that
problem
2
data
augmentation
data
augmentation
in
machine
learning
is
strategy
that
is
used
to
significantly
increase
the
diversity
of
the
data
available
to
train
deep
models
without
actually
collecting
new
data
it
relies
on
transformations
that
act
on
the
input
but
have
no
or
very
low
probability
to
change
the
label
or
at
least
do
so
in
predictable
way
to
generate
an
augmented
dataset
a(d
examples
of
such
transformations
include
applying
rotations
flipping
or
adding
noise
in
the
case
of
images
data
augmentation
is
now
at
the
forefront
of
state
of-the-art
techniques
in
computer
vision
59
and
increasingly
in
natural
language
processing
65
the
augmented
dataset
a(d
could
contain
an
infinite
set
of
possible
variants
of
the
initial
dataset
d
e.g
when
using
continuous
transforms
such
as
rotations
or
additional
noise
to
achieve
this
in
practice
a(d
is
sampled
on
the
fly
during
training
rather
than
generating
in
advance
all
possible
aug
mentations
in
the
training
set
this
process
is
straightforward
when
training
point
estimate
neural
networks
but
there
are
some
subtleties
when
applying
it
in
bayesian
statistics
the
main
concern
is
that
the
posterior
of
interest
is
p(¢|d
aug
where
aug
represents
some
knowledge
about
augmentation
not
p(¢|a(d
d
since
a(d
is
not
observed
from
bayesian
perspective
the
additional
information
is
brought
by
the
knowledge
of
the
augmentation
process
rather
than
by
some
additional
data
stated
otherwise
the
data
augmentation
is
part
of
the
stochastic
model
figure
6c
the
idea
is
that
if
one
is
given
data
d
then
one
could
also
have
been
given
data
d
where
each
element
in
is
replaced
by
an
augmentation
then
d
is
different
perspective
of
the
data
d
to
model
this
we
have
the
augmentation
distribution
p(a’|z
aug
that
augments
the
observed
data
using
the
aug
mentation
model
aug
to
generate
probabilistically
2/
which
represents
data
in
the
vicinity
of
figure
6
2
can
then
be
marginalized
to
simplify
the
stochastic
model
the
posterior
is
given
by
62,y
ug
(
plyle
o)n(w'ia
aughia
p(6
g0
this
is
probabilistic
counterpart
to
vicinal
risk
66
the
integral
in
equation
30
can
be
approximated
using
monte
carlo
integration
by
sampling
small
set
of
augmen
tations
a
according
to
p(z’|z
aug
and
averaging
az
z'€ax
p(ylz
0
aug
p(yla
)
3d
when
training
using
monte-carlo-based
estimate
of
the
loss
a
can
contain
as
few
as
single
element
as
long
as
it
is
resampled
for
each
optimization
iteration
this
greatly
simplifies
the
evaluation
of
equation
31
an
extension
of
this
approach
works
in
the
context
of
semi
supervised
learning
the
prior
can
be
designed
to
encourage
consistency
of
predictions
under
augmentation
67
59
using
unlabeled
data
to
build
the
samples
for
the
consistency
con
dition
as
defined
in
equation
27
note
that
this
does
not
add
labeling
to
the
unlabeled
examples
but
only
adds
term
to
encourage
consistency
between
the
labels
for
an
unlabeled
data
point
and
its
augmentation
3
meta-learning
transfer
learning
and
self-supervised
learning
meta-learning
68
in
the
broadest
sense
is
the
use
of
machine
learning
algorithms
to
assist
in
the
training
and
optimization
of
other
machine
learning
models
the
meta
knowledge
acquired
by
meta-learning
can
be
distinguished
from
standard
knowledge
in
the
sense
that
it
is
applicable
to
set
of
related
tasks
rather
than
single
task
transfer
learning
designates
methods
that
reuse
some
in
termediate
knowledge
acquired
on
given
problem
to
address
different
problem
in
deep
learning
it
is
used
mostly
for
domain
adaptation
when
labeled
data
are
abundant
in
do
main
that
is
in
some
way
similar
to
the
domain
of
interest
but
scarce
in
the
domain
of
interest
69
alternatively
pre-trained
models
70
could
be
used
to
study
large
architectures
whose
complete
training
would
be
very
computationally
expensive
self-supervised
learning
is
learning
strategy
where
the
data
themselves
provide
the
labels
71
since
the
labels
directly
obtainable
from
the
data
do
not
match
the
task
of
interest
the
problem
is
approached
by
learning
pretext
or
proxy
task
in
addition
to
the
task
of
interest
the
use
of
self
supervision
is
now
generally
regarded
as
an
essential
step
in
some
areas
for
instance
in
natural
language
processing
most
state-of-the-art
methods
use
these
pre-trained
models
70
in
addition
modern
deep
learning-based
3d
object
reconstruc
tion
72
and
disparity
estimation
in
stereo
vision
73
rely
on
self-supervised
learning
to
overcome
the
time-consuming
manual
annotation
of
training
data
common
approach
for
meta-learning
in
bayesian
statistics
is
to
recast
the
problem
as
hierarchical
bayes
74
with
the
prior
p(6,]€
for
each
task
conditioned
on
new
global
vari
able
figure
6d
can
represent
continuous
metaparameters
or
discrete
information
about
the
structure
of
the
bnn
i.e
to
learn
probable
functional
models
or
the
underlying
subgraph
of
the
pgm
i.e
to
learn
probable
stochastic
models
multiple
levels
can
be
added
to
organize
the
tasks
in
more
complex
hierarchy
if
needed
here
we
present
only
the
case
with
one
level
since
the
generalization
is
straightforward
with
this
broad
bayesian
understanding
of
meta-learning
both
transfer
learning
and
self-supervised
learning
are
special
cases
of
meta
learning
the
general
posterior
becomes
p(0,€|d
hp
dy|
d
ol)pwa
(
tet
32
in
practice
the
problem
is
often
approached
with
empirical
bayes
section
v-d
and
only
point
estimate
is
considered
for
the
global
variable
ideally
the
map
estimate
obtained
by
marginalizing
p(6,&|d
and
selecting
the
most
likely
point
but
this
is
not
always
the
case
in
transfer
learning
the
usual
approach
would
be
to
set
0
with
6
being
the
coefficients
of
the
main
task
the
new
prior
can
then
be
obtained
from
for
example
p(6l§
n((7(£),0),01
where
is
selection
of
the
parameters
to
transfer
and
is
parameter
to
tune
manually
unselected
parameters
are
assigned
new
prior
with
mean
of
by
convention
if
bnn
has
been
trained
for
the
main
task
then
can
be
estimated
from
the
previous
posterior
with
an
increment
to
account
for
the
additional
uncertainty
caused
by
the
domain
shift
self-supervised
learning
can
be
implemented
in
two
steps
the
first
step
learns
the
pretext
task
while
the
second
one
performs
transfer
learning
this
can
be
considered
overly
complex
but
might
be
required
if
the
pretext
task
has
high
computational
complexity
e.g
bert
models
in
natural
language
processing
70
recent
contributions
75
have
shown
that
jointly
learning
the
pretext
task
and
the
final
task
figure
6e
can
improve
the
results
obtained
in
self-supervised
learning
this
approach
which
is
closer
to
hierarchical
bayes
also
allows
setting
the
prior
single
time
while
still
retaining
the
benefits
of
self-supervised
learning
33
v
bayesian
inference
algorithms
priori
bnn
does
not
require
learning
phase
as
one
just
needs
to
sample
the
posterior
and
do
model
averaging
see
algorithm
1
however
sampling
the
posterior
is
not
easy
in
the
general
case
while
the
conditional
probability
p(d|h
of
the
data
and
the
probability
p(h
of
the
model
are
given
by
the
stochastic
model
the
integral
for
the
evidence
term
[
p(d|h')p(h')dh
might
be
excessively
difficult
to
compute
for
nontrivial
models
even
if
the
evidence
has
been
computed
directly
sampling
the
posterior
is
prohibitively
difficult
due
to
the
high
dimensionality
of
the
sampling
space
instead
of
using
traditional
methods
e.g
inversion
sampling
or
rejection
sampling
to
sample
the
posterior
dedicated
al
gorithms
are
used
the
most
popular
ones
are
markov
chain
monte
carlo
mcmc
methods
76
family
of
algorithms
that
exactly
sample
the
posterior
or
variational
inference
77
method
for
learning
an
approximation
of
the
posterior
see
figure
2
this
section
reviews
these
methods
first
in
subsection
v-a
and
v-b
we
introduce
mcmc
and
variational
inference
as
they
are
used
in
traditional
bayesian
statistics
then
in
subsec
tion
v-e
we
review
different
simplifications
or
approximations
that
have
been
proposed
for
deep
learning
we
also
provide
practical
example
in
the
supplementary
material
practical
example
iii
which
compares
different
learning
strategies
a
markov
chain
monte
carlo
mcmc
the
idea
behind
mcmc
methods
is
to
construct
markov
chain
sequence
of
random
samples
s
which
probabilis
tically
depend
only
on
the
previous
sample
s;_1
such
that
the
s
are
distributed
following
desired
distribution
unlike
standard
sampling
methods
such
as
rejection
or
inversion
sam
pling
most
mcmc
algorithms
require
an
initial
burn-in
time
before
the
markov
chain
converges
to
the
desired
distribution
moreover
the
successive
s;’s
might
be
autocorrelated
this
means
that
large
set
of
samples
has
to
be
generated
and
subsampled
to
obtain
approximately
independent
samples
from
the
underlying
distribution
the
final
collection
of
sam
ples
has
to
be
stored
after
training
which
is
expensive
for
most
deep
learning
models
despite
their
inherent
drawbacks
mcmc
methods
can
be
considered
among
the
best
available
and
the
most
pop
ular
solutions
for
sampling
from
exact
posterior
distribu
tions
in
bayesian
statistics
78
however
not
all
mcmc
algorithms
are
relevant
for
bayesian
deep
learning
gibbs
sampling
79
for
example
is
very
popular
in
general
statistics
and
unsupervised
machine
learning
but
is
very
ill-suited
for
bnns
the
most
relevant
mcmc
method
for
bnns
is
the
metropolis-hastings
algorithm
80
the
property
that
makes
the
metropolis-hasting
algorithm
popular
is
that
it
does
not
re
quire
knowledge
about
the
exact
probability
distribution
p(z
to
sample
from
instead
function
f(z
that
is
proportional
to
that
distribution
is
sufficient
this
is
the
case
of
bayesian
posterior
distribution
which
is
usually
quite
easy
to
compute
except
for
the
evidence
term
the
metropolis-hasting
algorithm
see
algorithm
4
starts
with
random
initial
guess
6
and
then
samples
new
candidate
point
¢
around
the
previous
6
using
proposal
distribution
q(#'|6
if
¢
is
more
likely
than
according
to
the
target
distribution
it
is
accepted
if
it
is
less
likely
it
is
accepted
with
certain
probability
or
rejected
otherwise
algorithm
metropolis-hasting
algorithm
draw
6g
initial
probability
distribution
while
=0
to
do
draw
6
q(6/]6
min
1
q@'le
1(0
q(6,16
f(0
)
draw
bernoulli(p
if
k&
then
0,11
=0
n=n+1
end
if
end
while
the
acceptance
probability
can
be
simplified
if
is
chosen
to
be
symmetric
i.e
q(0'10
q(0,]0
the
formula
for
the
acceptance
rate
then
becomes
f(9/
f(0n
in
this
situation
the
algorithm
is
simply
called
the
metropolis
method
common
choices
for
can
be
normal
distribution
q(0'10
n'(8,,,02
or
uniform
distribution
q(8'(6
u
€,0
€
centered
around
the
previous
sample
min
14
34
to
deal
with
non-symmetric
proposal
distributions
e.g
to
accommodate
constraint
in
the
model
such
as
bounded
domain
one
has
to
take
into
account
the
correction
term
imposed
by
the
full
metropolis-hasting
algorithm
the
spread
of
q(6'|6
has
to
be
tweaked
if
it
is
too
large
the
rejection
rate
will
be
too
high
if
it
is
too
small
the
samples
will
be
more
autocorrelated
there
is
no
general
method
to
tweak
those
parameters
however
clever
strategy
to
obtain
the
new
proposed
sample
8
can
reduce
their
impact
this
is
why
the
hamiltonian
monte-carlo
method
has
been
proposed
the
hamiltonian
monte
carlo
algorithm
hmc
81
is
another
example
of
metropolis-hasting
algorithms
for
contin
uous
distributions
it
is
designed
with
clever
scheme
to
draw
new
proposal
@
to
ensure
that
as
few
samples
as
possible
are
rejected
and
there
is
as
few
correlation
as
possible
between
samples
in
addition
the
hmc’s
burn-in
time
is
extremely
short
compared
to
the
standard
metropolis-hasting
algorithm
most
software
packages
for
bayesian
statistics
implement
the
no-u-turn
sampler
nuts
for
short
82
which
is
an
improvement
over
the
classic
hmc
algorithm
allowing
the
hyperparameters
of
the
algorithm
to
be
automatically
tweaked
instead
of
manually
setting
them
b
variational
inference
mcmc
algorithms
are
the
best
tools
for
sampling
from
the
exact
posterior
however
their
lack
of
scalability
has
made
them
less
popular
for
bnns
given
the
size
of
the
models
under
consideration
variational
inference
77
which
scales
better
than
mcmc
algorithms
gained
considerable
popularity
variational
inference
is
not
an
exact
method
rather
than
allowing
sampling
from
the
exact
posterior
the
idea
is
to
have
distribution
g4
h
)
called
the
variational
distribution
parametrized
by
set
of
parameters
¢
the
values
of
the
parameters
are
then
learned
such
that
the
variational
dis
tribution
g4
h
is
as
close
as
possible
to
the
exact
posterior
p(h|d
the
measure
of
closeness
that
is
commonly
used
is
the
kullback-leibler
divergence
kl-divergence
83
it
mea
sures
the
differences
between
probability
distributions
based
on
shannon’s
information
theory
84
the
kl-divergence
represents
the
average
number
of
additional
bits
required
to
encode
sample
from
using
code
optimized
for
g
for
bayesian
inference
it
is
computed
as
dis(aollp
i
1og
%
39
there
is
an
apparent
problem
here
which
is
to
compute
drr.(gs]|p
one
needs
to
compute
p(h|d
anyway
to
overcome
this
different
easily
derived
formula
called
the
evidence
lower
bound
or
elbo
serves
as
loss
asteryion
zueey
art
1os(p(d
distalip
ju
qs(h
36
since
log(p(d
only
depends
on
the
prior
minimizing
d1
qe||p
is
equivalent
to
maximizing
the
elbo
the
most
popular
method
to
optimize
the
elbo
is
stochas
tic
variational
inference
svi
85
which
is
in
fact
the
stochastic
gradient
descent
method
applied
to
variational
infer
ence
this
allows
the
algorithm
to
scale
to
the
large
datasets
loss
average
loss
10°
elbo/|d|[log
scale
2000
4000
6000
10000
epoch
8000
fig
8
typical
training
curve
for
bayes-by-backprop
that
are
encountered
in
modern
machine
learning
since
the
elbo
can
be
computed
on
single
mini-batch
at
each
iteration
convergence
when
learning
the
posterior
with
svi
will
be
slow
compared
to
the
usual
gradient
descent
moreover
most
implementations
use
small
number
of
samples
to
evaluate
the
elbo
often
just
one
before
taking
gradient
step
in
other
words
the
elbo
estimate
will
be
noisy
at
each
iteration
in
traditional
machine
learning
and
statistics
gy(h
is
mostly
constructed
from
distributions
in
the
exponential
fam
ily
e.g
multivariate
normal
86
gamma
and
dirichlet
dis
tributions
the
elbo
can
then
be
dramatically
simplified
into
components
87
leading
to
generalization
of
the
well
known
expectation-maximization
algorithm
to
account
for
correlations
between
the
large
number
of
parameters
certain
approximations
are
made
for
instance
block
diagonal
88
or
low
rank
plus
diagonal
89
covariance
matrices
can
be
used
to
reduce
the
number
of
variational
parameters
from
o(n
to
o(n
where
is
the
number
of
model
parameters
6
appendix
gives
more
details
on
how
these
simplifications
are
implemented
in
practice
c
bayes
by
backpropagation
variational
inference
offers
good
mathematical
tool
for
bayesian
inference
but
it
needs
to
be
adapted
to
deep
learning
the
main
problem
is
that
stochasticity
stops
backpropagation
from
functioning
at
the
internal
nodes
of
network
46
different
solutions
have
been
proposed
to
mitigate
this
prob
lem
including
probabilistic
backpropagation
90
or
bayes
by-backprop
91
the
latter
may
appear
more
familiar
to
deep
learning
practitioners
we
will
thus
focus
on
bayes-by
backprop
in
this
tutorial
bayes-by-backprop
is
indeed
practi
cal
implementation
of
svi
combined
with
reparametrization
trick
92
to
ensure
backpropagation
works
as
usual
the
idea
is
to
use
random
variable
¢
as
nonvariational
source
of
noise
is
not
sampled
directly
but
obtained
via
deterministic
transformation
e
¢
such
that
t(c
follows
q,(0
is
sampled
and
thus
changes
at
each
iteration
but
can
still
be
considered
constant
with
regard
to
other
variables
all
other
transformations
being
non
stochastic
backpropagation
works
as
usual
for
the
variational
parameters
¢
meaning
the
training
loop
can
be
implemented
analogous
to
the
training
loop
of
non-stochastic
neural
network
see
algorithm
5
the
general
formula
for
the
elbo
becomes
p(t(e
~d
5(t(e
=
|det(v.t(e
de
37
jastte
0108
pue
ld
pe(vate
e
this
is
tedious
to
work
with
instead
to
estimate
the
gradient
of
the
elbo
blundell
ef
al
91
proposed
to
use
the
fact
that
if
0)d0
q(e)de
then
for
differentiable
function
6
¢
we
have
55
20@10
0100
a(e
go
38
proof
is
provided
in
91
we
also
provide
in
appendix
an
alternative
proof
to
give
more
details
on
when
we
can
assume
75(0)d0
q(e)de
sufficient
condition
is
for
t(s
to
be
invertible
with
respect
to
and
the
distributions
¢
and
q4(0
to
not
be
degenerated
for
the
case
where
the
weights
are
treated
as
stochastic
variables
and
thus
the
hypothesis
h
the
training
loop
can
be
implemented
as
described
in
algorithm
5
01(0,9
de
algorithm
bayes-by-backprop
algorithm
¢
for
=0
to
do
draw
q
=t(e
¢
1(0
=log(q4(6
log(p(dy|
dz
0)p(6
ay
backprop
f
b=0¢—al,f
end
for
the
objective
function
corresponds
to
an
estimate
of
the
elbo
from
single
sample
this
means
that
the
gradient
estimate
will
be
noisy
the
convergence
graph
will
also
be
much
more
noisy
than
in
the
case
of
classic
backpropagation
figure
8
to
obtain
better
estimate
of
the
convergence
one
can
average
the
loss
over
multiple
epochs
since
algorithm
is
very
similar
to
the
classical
training
loop
for
point
estimate
deep
learning
most
techniques
used
for
optimization
in
deep
learning
are
straightforward
to
use
for
bayes-by-backprop
for
example
it
is
perfectly
fine
to
use
the
adam
optimizer
93
instead
of
the
stochastic
gradient
descent
note
also
that
if
bayes-by-backprop
is
presented
for
bnns
with
stochastic
weights
adapting
it
for
bnns
with
stochastic
activations
is
straightforward
in
that
case
the
activations
represent
the
hypothesis
and
the
weights
are
part
of
the
variational
parameters
¢
d
learning
the
prior
learning
the
prior
and
the
posterior
afterwards
is
possible
this
is
meaningful
if
most
aspects
of
the
prior
can
be
set
using
prior
knowledge
and
only
limited
set
of
free
parameters
of
the
prior
are
learned
before
obtaining
the
posterior
in
standard
bayesian
statistics
this
is
known
as
empirical
bayes
this
is
usually
valid
approximation
when
the
dimensions
of
the
prior
parameters
being
learned
are
significantly
smaller
than
the
dimensions
of
the
model
parameters
given
parametrized
prior
distribution
pe
h
maximizing
the
likelihood
of
the
data
is
good
method
to
learn
the
parameters
&
argmax
p(d|€
39
arg
max/
pe(d|h
pe(h')dh
in
general
directly
finding
is
an
intractable
problem
how
ever
when
using
variational
inference
the
elbo
is
the
log
likelihood
of
the
data
minus
the
kl-divergence
of
6
and
prior
eq
36
log(p(d|€
elbo
dk
s||
p
40
this
property
means
that
maximizing
the
elbo
now
function
of
both
and
¢
is
equivalent
to
maximizing
lower
bound
on
the
log
likelihood
of
the
data
this
lower
bound
becomes
tighter
when
g4
is
from
general
family
of
probability
distributions
with
more
flexibility
to
fit
the
exact
posterior
p(6|d
the
bayes-by-backprop
algorithm
presented
in
section
v-c
needs
only
to
be
slightly
modified
to
include
the
additional
parameters
in
the
training
loop
see
algorithm
6
algorithm
bayes-by-backprop
with
parametric
prior
£=&
py
for
=0
to
do
draw
q(e
=1t(c
p
18
&
log(qs(0
log(pe(dy|da
0)pe
0
agf
backprop
f
ay
backprop
f
€=€—aclacf
=0¢—asdsf
end
for
e
inference
algorithms
adapted
for
deep
learning
we
presented
thus
far
the
fundamental
theory
to
design
and
train
bnns
however
the
aforementioned
methods
are
still
not
easily
applicable
to
most
large
scale
architectures
currently
used
in
deep
learning
recent
research
has
also
shown
that
being
only
approximately
bayesian
is
sufficient
to
achieve
correctly
calibrated
model
with
uncertainty
estimates
27
this
section
presents
how
inference
algorithms
were
adapted
for
deep
learning
resulting
in
more
efficient
methods
specific
inference
methods
can
still
be
classified
as
mcmc
algorithms
i.e
they
generate
sequence
of
samples
from
the
posterior
or
as
form
of
variational
inference
i.e
they
learn
the
parameters
of
an
intermediate
distribution
to
approximate
the
posterior
all
methods
are
summarized
in
figure
9
1
bayes
via
dropout
dropout
has
initially
been
proposed
as
regularization
method
94
it
works
by
applying
multi
plicative
noise
to
the
target
layer
the
most
commonly
used
type
of
noise
is
bernoulli
noise
but
other
types
such
as
the
gaussian
noise
for
gaussian
dropout
94
might
be
used
instead
dropout
is
usually
turned
off
at
evaluation
time
but
leaving
it
on
results
in
distribution
for
the
output
predictions
95
96
it
turns
out
that
this
procedure
called
monte
carlo
dropout
is
in
fact
variational
inference
with
variational
distribution
defined
for
each
weight
matrix
as
zi
bernoulli(p
w
m
diag(z
with
z
being
the
random
activation
coefficients
and
the
matrix
of
weights
before
dropout
is
applied
p
is
the
activation
probability
for
layer
and
can
be
learned
or
set
manually
when
used
to
train
bnn
dropout
should
not
be
seen
as
regularization
method
as
it
is
part
of
the
variational
posterior
not
the
prior
this
means
that
it
should
be
coupled
with
dif
ferent
type
of
regularization
97
e.g
>
weight
penalization
the
equivalence
between
the
objective
function
lgropout
used
for
training
with
dropout
and
¢
weight
regularization
which
is
defined
as
lopon
f(0.9
a3
62
and
the
elbo
assuming
normal
prior
on
the
weights
and
the
distribution
presented
in
equation
41
as
variational
posterior
has
been
demonstrated
in
95
the
argument
is
similar
to
the
one
presented
in
section
iv-c3
mc-dropout
is
very
convenient
technique
to
perform
bayesian
deep
learning
it
is
straightforward
to
implement
and
requires
little
additional
knowledge
or
modeling
effort
compared
to
traditional
methods
it
often
leads
to
faster
train
ing
phase
compared
to
other
variational
inference
approaches
if
model
has
been
trained
with
dropout
layers
which
are
quite
widespread
in
today’s
deep
learning
architectures
and
an
additional
form
of
regularization
acting
as
prior
it
can
be
used
as
bnn
without
any
need
to
be
retrained
on
the
other
hand
mc-dropout
might
lack
some
expres
siveness
and
may
not
fully
capture
the
uncertainty
associated
with
the
model
predictions
98
it
also
lacks
flexibility
com
pared
to
other
bayesian
methods
for
online
or
active
learning
2
bayes
via
stochastic
gradient
descent
stochastic
gra
dient
descent
sgd
and
related
algorithms
are
at
the
core
of
modern
machine
learning
the
initial
goal
of
sgd
is
to
provide
an
algorithm
that
converges
to
an
optimal
point
estimate
solution
while
having
only
noisy
estimates
of
the
gradient
of
the
objective
function
this
is
especially
useful
when
the
training
data
has
to
be
split
into
mini-batches
the
parameter
update
rule
at
time
can
be
written
as
41
42
af
l
gv1og(p(dl.y‘dl.z¢9i
vng(pwl
43
where
d
is
mini-batch
subsampled
at
time
from
the
complete
dataset
d
¢
is
the
learning
rate
at
time
¢
is
the
size
of
the
whole
dataset
and
the
size
of
the
mini-batch
sgd
or
related
optimization
algorithms
such
as
adam
93
can
be
reinterpreted
as
markov
chain
algorithm
99
usually
the
hyperparameters
of
the
algorithm
are
tweaked
to
ensure
that
the
chain
converges
to
dirac
distribution
whose
position
gives
the
final
point
estimate
this
is
done
by
reducing
¢
toward
zero
while
ensuring
that
zin
00
however
if
the
learning
rate
is
reduced
toward
strictly
positive
value
the
underlying
markov
chain
will
converge
benefits
limitations
use
cases
mcmc
v.a
classic
methods
hmc
nuts)(§v-a
sgld
and
derivates
§v-e2a
warm
restarts
§v-e2a
directly
samples
the
posterior
state
of
the
art
samplers
limit
autocorrelation
between
samples
provide
well
behaved
markov
chain
with
minibatches
help
mcmc
method
explore
different
modes
of
the
posterior
requires
to
store
very
large
number
of
samples
small
and
average
models
do
not
scale
well
to
large
models
small
and
critical
models
focus
on
single
mode
of
the
posterior
models
with
larger
datasets
combined
with
mcmc
sampler
requires
new
burn-in
sequence
for
each
restart
pauiquiod
aq
ue)y
variational
inference
v.b
the
variational
distribution
is
easy
to
sample
large
scale
is
an
approximation
pp
models
bayes
by
backprop
§v-c
monte
carlo-dropout
§v-e1
laplace
approximation
§v-e2b
deep
ensembles
§v-e2b
fit
any
parametric
distribution
as
posterior
can
transform
model
using
dropout
into
bnn
by
analyzing
standard
sgd
get
bnn
from
map
help
focusing
on
different
modes
of
the
posterior
large
scale
noisy
gradient
descent
models
dropout
based
lack
expressive
power
models
unimodals
large
scale
models
focus
on
single
mode
of
the
posterior
pauiquiod
aq
ue)y
multimodals
models
and
combined
with
other
vi
methods
cannot
detect
local
uncertainty
if
used
alone
fig
9
summary
of
the
different
inference
approaches
used
to
train
bnn
with
their
benefits
limitations
and
use
cases
to
stationary
distribution
if
bayesian
prior
is
accounted
for
in
the
objective
function
then
this
stationary
distribution
can
be
an
approximation
of
the
corresponding
posterior
a
mcmc
algorithms
based
on
the
sgd
dynamic
to
approximately
sample
the
posterior
using
the
sgd
algorithm
specific
mcmc
method
called
stochastic
gradient
langevin
dynamic
sgld
100
has
been
developed
see
algorithm
7
coupling
sgd
with
langevin
dynamic
leads
to
slightly
modified
update
step
gviog(p(dl
0
vlog(p(&
n
~n(0
)
44
welling
et
al
100
showed
that
this
method
leads
to
markov
chain
that
samples
the
posterior
if
€
goes
toward
zero
how
ever
in
that
case
the
successive
samples
become
increasingly
autocorrelated
to
address
this
problem
the
authors
proposed
to
stop
reducing
¢
at
some
point
thus
making
the
samples
only
an
approximation
of
the
posterior
nevertheless
sgld
offers
better
theoretical
guarantees
compared
to
other
mcmc
algorithm
stochastic
gradient
langevin
dynamic
sgld
draw
6o
initial
probability
distribution
for
=0
to
do
select
mini-batch
dy
y
dy
d
1(00
10g(p(di.y|dr
01
log(p(6
ag
backpropg(f
draw
n
n(0
€
€t
0111
=0
§a9f+7h
end
for
methods
when
the
dataset
is
split
into
mini-batches
this
makes
the
algorithm
useful
in
bayesian
deep
learning
to
favor
the
exploration
of
the
posterior
one
can
use
warm
restart
of
the
algorithm
101
i.e
restarting
the
algorithm
at
new
random
position
8
and
with
large
learning
rate
€
this
offers
multiple
benefits
the
main
one
is
to
avoid
the
mode
col
lapse
problem
102
in
the
case
of
bnn
the
true
bayesian
posterior
is
usually
complex
multimodal
distribution
as
multiple
and
sometimes
not
equivalent
parametrizations
of
the
network
can
fit
the
training
set
favoring
exploration
over
precise
reconstruction
can
help
to
achieve
better
picture
of
those
different
modes
then
as
parameters
sampled
from
the
same
mode
are
likely
to
make
the
model
generalize
in
similar
manner
using
warm
restarts
enables
much
better
estimate
of
the
epistemic
uncertainty
when
processing
unseen
data
even
if
this
approach
provides
only
very
rough
approximation
of
the
exact
posterior
similar
to
other
mcmc
methods
this
approach
still
suffers
from
huge
memory
footprint
this
is
why
number
of
authors
have
proposed
methods
that
are
more
similar
to
traditional
variational
inference
than
to
an
mcmc
algorithm
b
variational
inference
based
on
sgd
dynamic
instead
of
an
mcmc
algorithm
sgd
dynamic
can
be
used
as
variational
inference
method
to
learn
distribution
by
using
laplace
approximation
laplace
approximation
fits
gaussian
posterior
by
using
the
maximum
posteriori
estimate
as
the
mean
and
the
inverse
of
the
hessian
of
the
loss
assuming
the
loss
is
the
log
likelihood
as
covariance
matrix
p(0|d
6
h™y
computing
usually
intractable
for
large
neural
network
architectures
thus
approximations
are
used
most
of
the
time
45
mcmc
methods
variational
inference
deep
ensembles
with
sgd
fig
10
different
techniques
for
sampling
the
posterior
mcmc
algorithms
sample
the
true
posterior
but
successive
samples
might
be
correlated
variational
inference
uses
parametric
distribution
that
can
suffer
from
mode
collapse
while
deep
ensembles
focus
on
the
modes
of
the
distribution
algorithm
deep
ensembles
for
=0
to
do
draw
6o
initial
probability
distribution
=€
for
=0to
do
1(65
=1og(p(dy|
dz
6:,5
+log(p(6:,5
ag
backprop
f
ao
f
end
for
end
for
by
analysing
the
variance
of
the
gradient
descent
algorithm
88
89
103
however
if
those
methods
are
able
to
capture
the
fine
shape
of
one
mode
of
the
posterior
they
cannot
fit
multiple
modes
lakshminarayanan
et
al
102
proposed
using
warm
restarts
to
obtain
different
point
estimate
networks
instead
of
fitting
parametric
distribution
this
method
called
deep
ensembles
see
figure
10
and
algorithm
8
has
been
used
in
the
past
to
perform
model
averaging
the
main
contribution
of
102
was
to
show
that
it
enables
well-calibrated
error
estimates
while
lakshminarayanan
er
al
102
claim
that
their
method
is
non-bayesian
it
has
been
shown
that
their
approach
can
still
be
understood
from
bayesian
point
of
view
12
104
when
regularization
is
used
the
different
point
estimates
should
correspond
to
modes
of
bayesian
posterior
this
can
be
interpreted
as
approximating
the
posterior
with
distribution
parametrized
as
multiple
dirac
deltas
i.e
a5(0
a,,(0
0.c
46
with
the
«pg
being
positive
constants
such
that
their
sum
is
equal
to
one
this
approach
can
be
seen
as
form
of
variational
inference
note
however
that
for
variational
distribution
containing
dirac
deltas
computing
the
elbo
in
sense
that
is
meaningful
for
traditional
optimization
is
impossible
vi
simplifying
bayesian
neural
networks
after
training
bnn
one
has
to
use
monte
carlo
at
eval
uation
time
to
estimate
uncertainty
this
is
major
drawback
of
bnns
for
mcmc-based
methods
storing
large
set
of
parametrizations
is
also
not
practical
this
section
presents
mitigation
strategies
reported
in
the
literature
a
bayesian
inference
on
the
n-)last
layer(s
only
the
architecture
of
deep
neural
networks
makes
it
quite
redundant
to
account
for
uncertainty
for
large
number
of
successive
layers
instead
recent
research
aims
to
use
only
few
stochastic
layers
usually
positioned
at
the
end
of
the
networks
105
106
see
figure
11
with
only
few
stochastic
layers
training
and
evaluation
can
be
drastically
sped
up
while
still
obtaining
meaningful
results
from
bayesian
perspective
this
approach
can
be
seen
as
learning
point
estimate
transformation
followed
by
shallow
bnn
training
bnn
with
some
non-stochastic
layers
is
similar
to
learning
the
parameters
for
the
prior
presented
in
section
v-d
the
weights
of
the
non-bayesian
layers
should
be
considered
as
both
prior
and
variational-posterior
parameters
b
bayesian
teachers
using
bnn
as
teacher
is
an
idea
derived
from
an
approach
used
in
bayesian
modeling
107
the
approach
is
to
train
non-stochastic
ann
to
predict
the
marginal
probability
p(y|z
d
using
bnn
as
teacher
108
this
is
related
to
the
idea
of
knowledge
distillation
109
110
where
possibly
several
pre-trained
knowledge
sources
can
be
used
to
train
more
functional
system
to
do
so
the
kl-divergence
between
parametric
distri
bution
¢
y|x
where
are
the
coefficients
of
the
student
network
and
p(y|z
d
is
minimized
argmin
p(y
|z
d)||qw
y|z
47
as
this
is
intractable
korattikara
ez
al
108
proposed
monte
carlo
approximation
48
arg
min
min
=0
by
108
000l
6,c0
here
can
be
estimated
using
training
dataset
d
that
contains
only
the
features
during
training
the
probability
p(y|z
0
of
the
labels
is
given
by
the
teacher
bnn
thus
d
can
be
much
larger
than
d
this
helps
the
student
network
retain
the
calibration
and
uncertainty
from
the
teacher
menon
et
al
110
observed
that
for
classification
prob
lems
simply
using
the
class
probabilities
output
by
bnn
teacher
rather
than
one-hot
labels
helps
the
student
to
retain
calibration
and
uncertainty
from
the
teacher
an
an
anai
an
fig
11
pgm
and
bnn
corresponding
to
last-layer
model
bayesian
teacher
can
also
be
used
to
compress
large
set
of
samples
generated
using
mcmc
111
instead
of
storing
o
generative
model
e.g
gan
in
111
is
trained
against
the
mcmc
samples
to
generate
the
coefficients
6
at
evaluation
time
this
approach
is
similar
to
variational
inference
with
representing
parametric
distribution
but
the
proposed
algorithm
allows
training
much
more
complex
model
than
the
distributions
usually
considered
for
variational
inference
vii
performance
metrics
of
bayesian
neural
networks
one
big
challenge
with
bnns
is
how
to
evaluate
their
performance
they
do
not
directly
output
point
estimate
pre
diction
but
conditional
probability
distribution
p(y|x
d
from
which
an
optimal
estimate
can
later
be
extracted
this
means
that
both
the
predictive
performance
i.e
the
ability
of
the
model
to
give
correct
answers
and
the
calibration
i.e
that
the
network
is
neither
overconfident
nor
underconfident
about
its
prediction
have
to
be
assessed
the
predictive
performance
sometimes
called
sharpness
in
statistics
of
network
can
be
assessed
by
treating
the
estimator
as
the
prediction
this
procedure
often
depends
on
the
type
of
data
the
network
is
meant
to
treat
many
different
metrics
e.g
mean
square
error
mse
¢
distances
and
cross
entropy
are
used
in
practice
covering
these
metrics
is
out
of
the
scope
of
this
tutorial
instead
we
refer
the
reader
to
112
for
more
details
the
standard
method
to
assess
the
model
calibration
is
calibration
curve
also
called
reliability
diagram
32
113
it
is
defined
as
function
0,1
0
1
that
represents
the
observed
probability
p
or
empirical
frequency
as
function
of
the
predicted
probability
p
see
figure
12
if
p
then
the
model
is
overconfident
otherwise
it
is
underconfident
well-calibrated
model
should
have
p
using
this
approach
requires
to
first
choose
set
of
events
with
different
predicted
probabilities
and
then
to
measure
the
empirical
frequency
of
each
event
using
test
set
7
for
binary
classifier
the
set
of
test
events
can
be
chosen
as
the
set
of
all
sets
of
datapoints
with
predicted
probabilities
of
acceptance
in
interval
p
&
4
for
chosen
4
or
alternatively
0
p
or
1—p
1
for
small
datasets
the
empirical
frequency
is
given
by
zggty
17
h[ﬁ—&,[w+5
17
ger
ls—o.5+6(h
for
multiclass
classifiers
the
calibration
curve
can
be
inde
pendently
checked
for
each
class
against
all
the
other
classes
in
this
case
the
problem
is
reduced
to
binary
classifier
regression
problems
are
slightly
more
complex
since
the
network
does
not
output
confidence
level
as
in
classifier
but
distribution
of
possible
outputs
the
solution
is
to
use
an
intermediate
statistic
with
known
probability
distribution
assuming
independence
between
the
for
sufficiently
large
set
of
different
randomly
selected
inputs
x
one
can
assume
that
the
normalized
sum
of
squared
residuals
nssr
follows
chi-square
law
nssr
§
9)"25
§
9
xdim(y
p=
49
50
observed
probability
observed
probability
predicted
probability
a
b
predicted
probability
fig
12
examples
of
calibration
curves
for
underconfident
a
and
overconfident
b
models
this
allows
attributing
to
each
data
point
in
the
test
set
7
predicted
probability
that
is
the
probability
of
observing
variance-normalized
distance
between
the
prediction
and
the
true
value
equal
to
or
lower
than
the
measured
nssr
formally
the
predicted
probability
is
computed
as
xim(y
nssr
v(y
;
t
5d
where
x]%im(y
is
the
chi-square
cumulative
distribution
with
dim(y
degrees
of
freedom
the
observed
probability
can
be
computed
as
|7
pi
tj0,00
bj
pi
52
=1
we
present
in
the
supplementary
material
practical
computa
tion
of
such
calibration
curve
for
the
sparse
measure
practical
example
practical
example
ii
giving
the
whole
calibration
curve
for
given
stochastic
model
allows
observing
where
the
model
is
likely
to
be
overconfident
or
underconfident
it
also
allows
to
certain
extent
to
recalibrate
the
model
113
however
providing
summary
measure
to
ease
comparison
or
interpretation
might
also
be
necessary
the
area
under
the
curve
auc
is
standard
metric
of
the
form
auc
pdp
53
an
auc
of
0.5
indicates
that
the
model
is
on
average
well
calibrated
the
distance
from
the
actual
calibration
curve
to
the
ideal
calibration
curve
is
also
good
indicator
for
the
calibration
of
model
o-ra
d(p,p
54
when
d(p,p
0
then
the
model
is
perfectly
calibrated
other
measures
have
also
been
proposed
examples
include
the
expected
calibration
error
and
some
discretized
variants
of
the
distance
from
the
actual
calibration
curve
to
the
ideal
calibration
curve
16
viii
conclusion
this
tutorial
covers
the
design
training
and
evaluation
of
bnns
while
their
underlying
principle
is
simple
i.e
just
training
an
ann
with
some
probability
distribution
attached
to
its
weights
designing
efficient
algorithms
remains
very
challenging
nonetheless
the
potential
applications
of
bnns
are
huge
in
particular
bnns
constitute
promising
paradigm
allowing
the
application
of
deep
learning
in
areas
where
system
is
not
allowed
to
fail
to
generalize
without
emitting
warning
finally
bayesian
methods
can
help
design
new
learning
and
regularization
strategies
thus
their
relevance
extends
to
traditional
point
estimate
models
online
resources
for
the
tutorial
https://github.com/french-paragon/bayesianneuralnetwork
tutorial-metarepos
supplementary
material
as
well
as
additional
practical
examples
for
the
covered
material
with
the
corresponding
source
code
implementation
have
been
provided
ix
acknowledgments
this
material
is
partially
based
on
research
sponsored
by
the
australian
research
council
https://www.arc.gov.au/
grants
dp150100294
and
dp150104251
and
air
force
research
laboratory
and
darpa
https://afrl.dodlive.mil/tag/darpa/
un
der
agreement
number
fa8750-19-2-0501
references
c
szegedy
w
zaremba
i
sutskever
j
bruna
d
erhan
1
goodfellow
and
r
fergus
intriguing
properties
of
neural
networks
arxiv
preprint
arxiv:1312.6199
2013
2
q
rao
and
j
frtunikj
deep
learning
for
self-driving
cars
chances
and
challenges
in
proceedings
of
the
ist
inter
national
workshop
on
software
engineering
for
al
in
au
tonomous
systems
ser
sefais
°18
2018
pp
35-38
3
j
ker
l
wang
j
rao
and
t
lim
deep
learning
appli
cations
in
medical
image
analysis
ieee
access
vol
6
pp
9375-9389
2018
4
r
c
cavalcante
r
c
brasileiro
v
l
souza
j
p
nobrega
and
a
l
oliveira
computational
intelligence
and
financial
markets
survey
and
future
directions
expert
systems
with
applications
vol
55
pp
194-211
2016
5
h
m
d
kabir
a
khosravi
m
a
hosen
and
s
nahavandi
neural
network-based
uncertainty
quantification
survey
of
methodologies
and
applications
ieee
access
vol
6
pp
36218-36234
2018
6
a
etz
q
e
gronau
f
dablander
p
a
edelsbrunner
and
b
baribault
how
to
become
bayesian
in
eight
easy
steps
an
annotated
reading
list
psychonomic
bulletin
review
vol
25
pp
219-234
2018
7
n
g
polson
v
sokolov
et
al
deep
learning
bayesian
perspective
bayesian
analysis
vol
12
no
4
pp
1275-1304
2017
8
j
lampinen
and
a
vehtari
bayesian
approach
for
neural
networks—review
and
case
studies
neural
networks
vol
14
no
3
pp
257
274
2001
9
d
m
titterington
bayesian
methods
for
neural
networks
and
related
models
statist
sci
vol
19
no
1
pp
128-139
02
2004
e
goan
and
c
fookes
bayesian
neural
networks
an
intro
duction
and
survey
cham
springer
international
publishing
2020
pp
45-87
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
h
wang
and
d.-y
yeung
a
survey
on
bayesian
deep
learning
acm
comput
surv
vol
53
no
5
sep
2020
a
g
wilson
and
p
izmailov
bayesian
deep
learning
and
probabilistic
perspective
of
generalization
corr
vol
abs/2002.08791
2020
online
available
http://arxiv.org/
abs/2002.08791
1
goodfellow
y
bengio
and
a
courville
deep
learning
mit
press
2016
http://www.deeplearningbook.org
s
c.-h
yang
w
k
vong
r
b
sojitra
t
folke
and
p
shafto
mitigating
belief
projection
in
explainable
artificial
intelligence
via
bayesian
teaching
scientific
reports
vol
11
no
1
p
9863
may
2021
c
guo
g
pleiss
y
sun
and
k
q
weinberger
on
calibration
of
modern
neural
network
in
proceedings
of
the
34th
international
conference
on
machine
learning
volume
70
ser
icml'17
2017
pp
1321-1330
j
nixon
m
w
dusenberry
l
zhang
g
jerfel
and
d
tran
measuring
calibration
in
deep
learning
in
the
ieee
con
ference
on
computer
vision
and
pattern
recognition
cvpr
workshops
june
2019
d
hendrycks
and
k
gimpel
a
baseline
for
detecting
mis
classified
and
out-of-distribution
examples
in
neural
networks
in
5th
international
conference
on
learning
representations
iclr
2017
conference
track
proceedings
2017
z.-h
zhou
ensemble
methods
foundations
and
algorithms
ist
ed
chapman
and
hall/crc
2012
f
galton
vox
populi
nature
vol
75
no
1949
pp
450
451
mar
1907
l
breiman
bagging
predictors
machine
learning
vol
24
no
2
pp
123-140
aug
1996
d
j
c
mackay
a
practical
bayesian
framework
for
back
propagation
networks
neural
computation
vol
4
no
3
pp
448-472
1992
p
izmailov
s
vikram
m
d
hoffman
and
a
g
wilson
what
are
bayesian
neural
network
posteriors
really
like
corr
vol
abs/2104.14421
2021
online
available
http://arxiv.org/abs/2104.14421
y
gal
and
z
ghahramani
bayesian
convolutional
neural
networks
with
bernoulli
approximate
variational
inference
in
4th
international
conference
on
learning
representations
iclr
workshop
track
2016
d
p
kingma
and
m
welling
stochastic
gradient
vb
and
the
variational
auto-encoder
in
second
international
conference
on
learning
representations
iclr
vol
19
2014
c
robert
the
bayesian
choice
from
decision-theoretic
foun
dations
to
computational
implementation
springer
science
business
media
2007
j
mitros
and
b
m
namee
on
the
validity
of
bayesian
neural
networks
for
uncertainty
estimation
in
aics
2019
a
kristiadi
m
hein
and
p
hennig
being
bayesian
even
just
bit
fixes
overconfidence
in
relu
networks
corr
vol
abs/2002.10118
2020
online
available
http://arxiv.org/abs/2002.10118
y
ovadia
e
fertig
j
ren
z
nado
d
sculley
s
nowozin
j
dillon
b
lakshminarayanan
and
j
snoek
can
you
trust
your
model's
uncertainty
evaluating
predictive
uncer
tainty
under
dataset
shift
in
advances
in
neural
information
processing
systems
32
curran
associates
inc
2019
pp
13991-14002
a
d
kiureghian
and
o
ditlevsen
aleatory
or
epistemic
does
it
matter
structural
safety
vol
31
no
2
pp
105-112
2009
risk
acceptance
and
risk
communication
s
depeweg
j.-m
hernandez-lobato
f
doshi-velez
and
s
udluft
decomposition
of
uncertainty
in
bayesian
deep
learning
for
efficient
and
risk-sensitive
learning
in
pro
ceedings
of
the
35th
international
conference
on
machine
learning
ser
proceedings
of
machine
learning
research
vol
80
2018
pp
1184-1193
d
h
wolpert
the
lack
of
priori
distinctions
between
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
learning
algorithms
neural
computation
vol
8
no
7
pp
1341-1390
1996
a
kendall
and
y
gal
what
uncertainties
do
we
need
in
bayesian
deep
learning
for
computer
vision
in
proceedings
of
the
31st
international
conference
on
neural
information
processing
systems
ser
nips’17
2017
p
5580-5590
t
auld
a
w
moore
and
s
f
gull
bayesian
neural
networks
for
internet
traffic
classification
ieee
transactions
on
neural
networks
vol
18
no
1
pp
223-239
2007
x
zhang
and
s
mahadevan
bayesian
neural
networks
for
flight
trajectory
prediction
and
safety
assessment
decision
support
systems
vol
131
p
113246
2020
s
arangio
and
f
bontempi
structural
health
monitoring
of
cable—stayed
bridge
with
bayesian
neural
networks
structure
and
infrastructure
engineering
vol
11
no
4
pp
575-587
2015
s
m
bateni
d.-s
jeng
and
b
w
melville
bayesian
neural
networks
for
prediction
of
equilibrium
and
time-dependent
scour
depth
around
bridge
piers
advances
in
engineering
software
vol
38
no
2
pp
102-111
2007
x
zhang
f
liang
r
srinivasan
and
m
van
liew
estimat
ing
uncertainty
of
streamflow
simulation
using
bayesian
neural
networks
water
resources
research
vol
45
no
2
2009
a
d
cobb
m
d
himes
f
soboczenski
s
zorzan
m
d
o’beirne
a
g
baydin
y
gal
s
d
domagal-goldman
g
n
amey
and
d
a
and
an
ensemble
of
bayesian
neural
networks
for
exoplanetary
atmospheric
retrieval
the
astronomical
journal
vol
158
no
1
p
33
jun
2019
f
aminian
and
m
aminian
fault
diagnosis
of
analog
circuits
using
bayesian
neural
networks
with
wavelet
transform
as
preprocessor
journal
of
electronic
testing
vol
17
no
1
pp
29-36
feb
2001
w
beker
a
wotos
s
szymku
and
b
a
grzybowski
minimal-uncertainty
prediction
of
general
drug-likeness
based
on
bayesian
neural
networks
nature
machine
intel
ligence
vol
2
no
8
pp
457-465
aug
2020
y
gal
r
islam
and
z
ghahramani
deep
bayesian
active
learning
with
image
data
in
proceedings
of
the
34th
inter
national
conference
on
machine
learning
volume
70
ser
icml’17
2017
p
1183-1192
t
tran
t.-t
do
l
reid
and
g
carneiro
bayesian
generative
active
deep
learning
corr
vol
abs/1904.11643
2019
online
available
http://arxiv.org/abs/1904.11643
m
opper
and
o
winther
a
bayesian
approach
to
on-line
learning
on-line
learning
in
neural
networks
pp
363-378
1998
h
ritter
a
botev
and
d
barber
online
structured
laplace
approximations
for
overcoming
catastrophic
forgetting
in
proceedings
of
the
32nd
international
conference
on
neu
ral
information
processing
systems
ser
nips’18
2018
pp
3742-3752
s
pouyanfar
s
sadiq
y
yan
h
tian
y
tao
m
p
reyes
m.-l
shyu
s.-c
chen
and
s
s
iyengar
a
survey
on
deep
learning
algorithms
techniques
and
applications
acm
comput
surv
vol
51
no
5
sep
2018
w
l
buntine
operations
for
learning
with
graphical
mod
els
journal
of
artificial
intelligence
research
vol
2
pp
159-225
dec
1994
y
wen
p
vicol
j
ba
d
tran
and
r
grosse
flipout
efficient
pseudo-independent
weight
perturbations
on
mini
batches
in
international
conference
on
learning
represen
tations
2018
c
zhang
s
bengio
m
hardt
b
recht
and
o
vinyals
un
derstanding
deep
learning
requires
rethinking
generalization
in
5th
international
conference
on
learning
representations
iclr
2017
b
carpenter
a
gelman
m
d
hoffman
d
lee
b
goodrich
m
betancourt
m
brubaker
j
guo
p
li
and
a
riddell
stan
probabilistic
programming
language
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
journal
of
statistical
software
vol
76
no
1
2017
a
gelman
and
other
stan
developers
prior
choice
recom
mendations
2020
retrieved
from
https://github.com/stan
dev/stan/wiki/prior-choice-recommendations
last
seen
13.07.2020
d
silvestro
and
t
andermann
prior
choice
affects
ability
of
bayesian
neural
networks
to
identify
unknowns
corr
vol
abs/2005.04987
2020
online
available
http://arxiv.org/abs/2005.04987
k
p
murphy
machine
learning
probabilistic
perspective
the
mit
press
2012
a
a
pourzanjani
r
m
jiang
b
mitchell
p
j
atzberger
and
l
r
petzold
bayesian
inference
over
the
stiefel
manifold
via
the
givens
representation
corr
vol
abs/1710.09443
2017
online
available
http://arxiv.org/
abs/1710.09443
j
l
ba
j
r
kiros
and
g
e
hinton
layer
normalization
corr
vol
arxiv:1607.06450
2016
in
nips
2016
deep
learning
symposium
g.-j
qi
and
j
luo
small
data
challenges
in
big
data
era
survey
of
recent
progress
on
unsupervised
and
semi-supervised
methods
corr
vol
abs/1903.11260
2019
online
available
http://arxiv.org/abs/1903.11260
n
natarajan
l
s
dhillon
p
k
ravikumar
and
a
tewari
learning
with
noisy
labels
in
advances
in
neural
informa
tion
processing
systems
26
curran
associates
inc
2013
pp
1196-1204
b
frenay
and
m
verleysen
classification
in
the
presence
of
label
noise
survey
ieee
transactions
on
neural
networks
and
learning
systems
vol
25
no
5
pp
845-869
2014
a
c
tommi
and
t
jaakkola
on
information
regularization
in
in
proceedings
of
the
19th
uai
2003
k
sohn
d
berthelot
c.-l
li
z
zhang
n
carlini
e
d
cubuk
a
kurakin
h
zhang
and
c
raffel
fixmatch
simplifying
semi-supervised
learning
with
consistency
and
confidence
corr
vol
abs/2001.07685
2020
online
available
https://arxiv.org/abs/2001.07685
m
belkin
p
niyogi
and
v
sindhwani
manifold
regular
ization
geometric
framework
for
learning
from
labeled
and
unlabeled
examples
j
mach
learn
res
vol
7
pp
2399
2434
dec
2006
s
yu
b
krishnapuram
r
rosales
and
r
b
rao
bayesian
co-training
journal
of
machine
learning
research
vol
12
no
80
pp
2649-2680
2011
r
kunwar
u
pal
and
m
blumenstein
semi-supervised
online
bayesian
network
learner
for
handwritten
characters
recognition
in
2014
22nd
international
conference
on
pat
tern
recognition
2014
pp
3104-3109
d.-h
lee
pseudo-label
the
simple
and
efficient
semi
supervised
learning
method
for
deep
neural
networks
in
workshop
on
challenges
in
representation
learning
icml
vol
3
2013
z
li
b
ko
and
h.-j
choi
naive
semi-supervised
deep
learning
using
pseudo-label
peer-to-peer
networking
and
applications
vol
12
no
5
pp
1358-1368
2019
m
s
bari
m
t
mohiuddin
and
s
joty
multimix
robust
data
augmentation
strategy
for
cross-lingual
nlp
in
icml
2020
o
chapelle
j
weston
l
bottou
and
v
vapnik
vicinal
risk
minimization
in
advances
in
neural
information
processing
systems
13
mit
press
2001
pp
416-422
q
xie
z
dai
e
h
hovy
m
luong
and
q
v
le
unsupervised
data
augmentation
corr
vol
abs/1904.12848
2019
online
available
http://arxiv.org/abs/1904.12848
t
hospedales
a
antoniou
p
micaelli
and
a
storkey
meta-learning
in
neural
networks
survey
corr
vol
abs/2004.05439
2020
online
available
http://arxiv.org/
abs/2004.05439
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
s
j
pan
and
q
yang
a
survey
on
transfer
learning
ieee
transactions
on
knowledge
and
data
engineering
vol
22
no
10
pp
1345-1359
2010
x
qiu
t
sun
y
xu
y
shao
n
dai
and
x
huang
pre
trained
models
for
natural
language
processing
survey
corr
vol
abs/2003.08271
2020
l
jing
and
y
tian
self-supervised
visual
feature
learning
with
deep
neural
networks
survey
ieee
transactions
on
pattern
analysis
and
machine
intelligence
pp
1-1
2020
x.-f
han
h
laga
and
m
bennamoun
image-based
3d
object
reconstruction
state-of-the-art
and
trends
in
the
deep
learning
era
ieee
transactions
on
pattern
analysis
and
machine
intelligence
vol
43
no
5
pp
1578-1604
2021
h
laga
l
v
jospin
f
boussaid
and
m
bennamoun
a
survey
on
deep
learning
techniques
for
stereo-based
depth
esti
mation
ieee
transactions
on
pattern
analysis
and
machine
intelligence
2020
e
grant
c
finn
s
levine
t
darrell
and
t
l
grif
fiths
recasting
gradient-based
meta-learning
as
hierarchical
bayes
in
6th
international
conference
on
learning
repre
sentations
iclr
2018
vancouver
bc
canada
april
30
may
3
2018
conference
track
proceedings
2018
l
beyer
x
zhai
a
oliver
and
a
kolesnikov
s4l
self-supervised
semi-supervised
learning
in
2019
ieee/cvf
international
conference
on
computer
vision
iccv
2019
pp
1476-1485
w
k
hastings
monte
carlo
sampling
methods
using
markov
chains
and
their
applications
biometrika
vol
57
no
1
pp
97-109
04
1970
d
m
blei
a
kucukelbir
and
j
d
mcauliffe
variational
inference
review
for
statisticians
journal
of
the
american
statistical
association
vol
112
no
518
pp
859-877
2017
r
bardenet
a
doucet
and
c
holmes
on
markov
chain
monte
carlo
methods
for
tall
data
j
mach
learn
res
vol
18
no
1
pp
1515-1557
jan
2017
e
l
george
g
casella
and
e
1
george
explaining
the
gibbs
sampler
the
american
statistician
1992
s
chib
and
e
greenberg
understanding
the
metropolis
hastings
algorithm
the
american
statistician
vol
49
no
4
pp
327-335
1995
r
m
neal
er
al
mcmc
using
hamiltonian
dynamics
handbook
of
markov
chain
monte
carlo
vol
2
no
11
p
2
2011
m
d
hoffman
and
a
gelman
the
no-u-turn
sampler
adaptively
setting
path
lengths
in
hamiltonian
monte
carlo
journal
of
machine
learning
research
vol
15
no
1
pp
1593-1623
2014
s
kullback
and
r
a
leibler
on
information
and
suffi
ciency
the
annals
of
mathematical
statistics
vol
22
no
1
pp
79
86
1951
c
e
shannon
a
mathematical
theory
of
communication
the
bell
system
technical
journal
vol
27
no
3
pp
379
423
1948
m
d
hoffman
d
m
blei
c
wang
and
j
paisley
stochas
tic
variational
inference
j
mach
learn
res
vol
14
no
1
pp
1303-1347
may
2013
a
graves
practical
variational
inference
for
neural
net
works
in
advances
in
neural
information
processing
systems
24
curran
associates
inc
2011
pp
2348-2356
z
ghahramani
and
m
j
beal
propagation
algorithms
for
variational
bayesian
learning
in
advances
in
neural
infor
mation
processing
systems
13
mit
press
2001
pp
507
513
h
ritter
a
botev
and
d
barber
a
scalable
laplace
approx
imation
for
neural
networks
in
international
conference
on
learning
representations
2018
w
j
maddox
p
izmailov
t
garipov
d
p
vetrov
and
a
g
wilson
a
simple
baseline
for
bayesian
uncertainty
in
deep
learning
in
advances
in
neural
information
processing
90
91
92
93
94
95
96
97
98
99
00
01
02
03
04
05
06
systems
32
curran
associates
inc
2019
pp
13
153-13
164
j
m
herndndez-lobato
and
r
p
adams
probabilistic
backpropagation
for
scalable
learning
of
bayesian
neural
net
works
in
proceedings
of
the
32nd
international
conference
on
international
conference
on
machine
learning
volume
37
ser
icml'15
2015
p
1861-1869
c
blundell
j
cornebise
k
kavukcuoglu
and
d
wierstra
weight
uncertainty
in
neural
network
in
proceedings
of
the
32nd
international
conference
on
machine
learning
ser
proceedings
of
machine
learning
research
vol
37
2015
pp
1613-1622
d
p
kingma
m
welling
et
al
an
introduction
to
vari
ational
autoencoders
foundations
and
trends®
in
machine
learning
vol
12
no
4
pp
307-392
2019
d
kingma
and
j
ba
adam
method
for
stochastic
optimization
international
conference
on
learning
repre
sentations
12
2014
n
srivastava
g
hinton
a
krizhevsky
i
sutskever
and
r
salakhutdinov
dropout
simple
way
to
prevent
neu
ral
networks
from
overfitting
journal
of
machine
learning
research
vol
15
no
56
pp
1929-1958
2014
y
gal
and
z
ghahramani
dropout
as
bayesian
approxi
mation
representing
model
uncertainty
in
deep
learning
in
proceedings
of
the
33rd
international
conference
on
machine
learning
volume
48
ser
icml’16
2016
p
1050-1059
y
li
and
y
gal
dropout
inference
in
bayesian
neural
networks
with
alpha-divergences
in
proceedings
of
the
34th
international
conference
on
machine
learning
volume
70
ser
icml’17
2017
pp
2052-2061
j
hron
a
matthews
and
z
ghahramani
variational
bayesian
dropout
pitfalls
and
fixes
in
proceedings
of
the
35th
international
conference
on
machine
learning
ser
pro
ceedings
of
machine
learning
research
vol
80
2018
pp
2019-2028
a
chan
a
alaa
z
qian
and
m
van
der
schaar
unla
belled
data
improves
bayesian
uncertainty
calibration
under
covariate
shift
in
proceedings
of
the
37th
international
conference
on
machine
learning
ser
proceedings
of
machine
learning
research
vol
119
virtual
pmlr
13-18
jul
2020
pp
1392-1402
s
mandt
m
d
hoffman
and
d
m
blei
stochastic
gradient
descent
as
approximate
bayesian
inference
the
journal
of
machine
learning
research
vol
18
no
1
pp
4873-4907
2017
m
welling
and
y
w
teh
bayesian
learning
via
stochastic
gradient
langevin
dynamics
in
proceedings
of
the
28th
international
conference
on
machine
learning
ser
icml
11
2011
pp
681-688
n
seedat
and
c
kanan
towards
calibrated
and
scalable
uncertainty
representations
for
neural
networks
corr
vol
abs/1911.00104
2019
online
available
http://arxiv.org/
abs/1911.00104
b
lakshminarayanan
a
pritzel
and
c
blundell
simple
and
scalable
predictive
uncertainty
estimation
using
deep
ensem
bles
in
advances
in
neural
information
processing
systems
30
curran
associates
inc
2017
pp
6402-6413
m
khan
d
nielsen
v
tangkaratt
w
lin
y
gal
and
a
srivastava
fast
and
scalable
bayesian
deep
learning
by
weight-perturbation
in
adam
in
proceedings
of
the
35th
in
ternational
conference
on
machine
learning
ser
proceedings
of
machine
learning
research
vol
80
2018
pp
2611-2620
t
pearce
f
leibfried
a
brintrup
m
zaki
and
a
neely
uncertainty
in
neural
networks
approximately
bayesian
ensembling
in
aistats
2020
2020
j
zeng
a
lesnikowski
and
j
m
alvarez
the
relevance
of
bayesian
layer
positioning
to
model
uncertainty
in
deep
bayesian
active
learning
corr
vol
abs/1811.12535
2018
online
available
http://arxiv.org/abs/1811.12535
n
brosse
c
riquelme
a
martin
s
gelly
and
eric
classification
estimation
available
moulines
on
last-layer
algorithms
for
decoupling
representation
from
uncertainty
corr
vol
abs/2001.08049
2020
online
http://arxiv.org/abs/2001.08049
07
e
snelson
and
z
ghahramani
compact
approximations
to
bayesian
predictive
distributions
in
proceedings
of
the
22nd
international
conference
on
machine
learning
ser
icml
05
2005
p
840-847
08
a
korattikara
v
rathod
k
murphy
and
m
welling
bayesian
dark
knowledge
in
proceedings
of
the
28th
inter
national
conference
on
neural
information
processing
sys
tems
volume
2
ser
nips’15
2015
pp
3438-3446
09
g
hinton
o
vinyals
and
j
dean
distilling
the
knowledge
in
neural
network
arxiv
preprint
arxiv:1503.02531
2015
in
nips
2014
deep
learning
workshop
10
a
k
menon
a
s
rawat
s
j
reddi
s
kim
and
s
kumar
why
distillation
helps
statistical
perspective
corr
vol
abs/2005.10419
2020
online
available
https://arxiv.org/abs/2005.10419
11
k.-c
wang
p
vicol
j
lucas
l
gu
r
grosse
and
r
zemel
adversarial
distillation
of
bayesian
neural
network
posteriors
in
proceedings
of
the
35th
international
con
ference
on
machine
learning
ser
proceedings
of
machine
learning
research
vol
80
2018
pp
5190-5199
12
k
janocha
and
w
m
czarnecki
on
loss
functions
for
deep
neural
networks
in
classification
schedae
informaticae
vol
1/2016
2017
online
available
http://dx.doi.org/10.4467/
20838476s1.16.004.6185
13
v
kuleshov
n
fenner
and
s
ermon
accurate
uncertainties
for
deep
learning
using
calibrated
regression
in
proceedings
of
the
35th
international
conference
on
machine
learning
ser
proceedings
of
machine
learning
research
vol
80
2018
pp
2796-2804
appendix
implementing
parameter-efficient
normal
distributions
for
variational
inference
given
random
vector
in
which
each
independent
and
identically
distributed
component
follows
standard
normal
distribution
\(0
1
one
can
obtain
sample
from
normal
distribution
p
using
the
following
formulas
0=
ve
+p
where
/x
is
matrix
such
that
\/f\/f-r
3
when
varia
tional
inference
algorithm
needs
to
learn
the
covariance
matrix
of
0
it
is
often
more
convenient
to
learn
v/x
assuming
has
entries
\/i\/i-r
is
the
cholesky
decomposition
of
3
as
such
\/f
is
lower
triangular
matrix
and
o(nz
variational
parameters
are
required
to
learn
the
exact
covariance
matrix
this
becomes
exceedingly
computationally
expensive
rather
quickly
when
becomes
large
straightforward
simplification
is
to
only
consider
diago
nal
approximation
of
x
this
can
be
enforced
by
learning
only
the
diagonal
coefficients
of
v/
meaning
only
o(n
varia
tional
parameters
are
required
figure
13a
this
approach
can
be
extended
to
learn
more
correlation
coefficients
by
learning
block
diagonal
88
covariance
matrix
which
can
be
done
by
learning
the
corresponding
lower
triangular
entries
in
v/x
figure
13b
if
the
maximal
size
of
the
nonzero
blocks
is
fixed
to
be
w
o(w
n
variational
parameters
are
required
the
major
drawback
of
this
model
is
that
the
index
of
two
given
parameters
determines
whether
their
covariance
can
be
learned
55
b
©
fig
13
nonzero
entries
in
/x
when
learning
diagonal
a
block
diagonal
b
or
diagonal
plus
low
rank
c
approximation
of
@
by
the
variational
distribution
this
is
not
always
ideal
as
it
is
hard
to
predict
which
parameters
will
be
the
most
correlated
and
need
to
be
positioned
close
to
one
another
an
alternative
is
to
learn
diagonal
plus
low
rank
approximation
of
89
this
is
done
by
sampling
vector
with
instead
of
n
components
v'3
is
then
defined
as
vve=[d
l
56
where
is
diagonal
matrix
of
size
and
is
lower
triangular
matrix
of
size
nxr
figure
13c
this
means
that
the
model
has
more
flexibility
to
learn
the
correlation
between
all
the
components
of
while
only
requiring
o(n
r
variational
parameters
appendix
proof
of
equation
38
let
us
assume
that
we
have
probability
space
2
f,p
where
is
set
of
outcomes
is
c-algebra
of
representing
possible
events
and
is
measure
defined
on
and
which
assigns
value
of
to
€
representing
the
probability
of
an
event
in
addition
assume
that
we
have
probability
distribution
4(6
for
given
random
variable
6
probability
distribution
g
for
given
random
variable
and
functional
relation
e
¢
such
that
t
@
is
distributed
according
to
@
and
t(e
¢
is
bijection
with
respect
to
e
thus
we
have
p07
t(e
=p(e(e
ve€e(f
57
with
e(f)={{e(w):wee}:ecf}
t(e
¢
{t(e,$
e}
he
uf*e}'/\s(u)ge
0~
h(e
$
ueernoe)cum,e
since
t(e
is
bijection
with
respect
to
e
we
have
e~
y(e
0
t(e
¢
this
implies
q4(0)d6
q(e)de
ve
e(f
58
joct(e
jeee
which
in
turn
implies
q4(0)d0
q(e)de
59
for
non-degenerated
probability
distributions
g,4(6
and
g¢(e
now
given
differentiable
function
f
)
we
have
1(@0)as(@)0
161
0)al)iz
©0
joct(e
jeee
which
implies
equation
38
20
arxiv:2007.06823v3
cs.lg
jan
2022
hands-on
bayesian
neural
networks
supplementary
material
i
practical
example
bayesian
mnist
over
the
years
mnist
1
has
become
the
most
renown
toy
dataset
in
deep
learning
coding
handwritten
digit
classifier
based
on
this
dataset
is
now
the
hello
world
of
deep
neural
network
programming
the
first
practical
example
we
introduce
for
this
tutorial
is
thus
just
plain
old
classifier
for
mnist
implemented
as
bnn
the
code
is
available
on
github
https://github.com/french-paragon/bayesianmnist
the
setup
is
as
follows
the
purpose
is
to
show
bayesian
neural
network
bnn
hello
world
project
the
problem
is
to
train
bnn
to
perform
hand-written
digit
recognition
the
dataset
we
are
going
to
use
is
mnist
however
to
evaluate
how
the
proposed
bnn
reacts
to
unseen
data
we
will
remove
one
of
the
classes
we
used
the
digit
for
this
experiment
from
the
training
set
so
that
the
network
never
sees
it
during
training
the
stochastic
model
is
the
plain
bayesian
regression
presented
in
section
iv-b
of
the
main
paper
we
use
normal
distribution
as
prior
for
the
network
parameters
6
with
standard
deviation
of
for
the
weights
and
25
for
the
bias
as
we
expect
the
scale
of
the
bias
to
have
slightly
more
variability
than
the
weights
this
is
because
in
relu
or
leaky-relu
network
the
weights
influence
the
local
change
in
slope
of
layer
function
while
the
bias
indicates
the
position
of
those
inflection
points
the
functional
model
is
standard
convolutional
neural
network
with
two
convolutional
layers
followed
by
two
fully
connected
layers
see
figure
1
a
training
we
used
variational
inference
to
train
this
bnn
we
use
gaussian
distribution
with
diagonal
covariance
as
varia
tional
posterior
see
section
v-c
of
the
main
paper
since
we
expect
the
posterior
to
be
multi-modal
we
train
an
ensemble
of
bnns
instead
of
single
one
see
section
v-e2b
of
the
main
paper
note
that
the
top
performing
method
on
the
mnist
32@8x8
32@4xa
1x512
1@213?«?32“24
16@12x12
k™
928x2
%
x128
o=l
%
relu
convolution
max-pool
a_
dense
relu
convolution
max-pool
fig
1
the
neural
network
architecture
used
for
the
bayesian
mnist
practical
example
leaderboard
is
actually
an
ensemble-based
method
2
the
authors
of
2
designate
their
model
as
multi-column
neural
network
but
the
idea
is
equivalent
to
an
ensemble
b
results
we
tested
the
final
bnn
against
1
the
test
set
restricted
to
the
classes
the
network
has
been
trained
on
2
the
test
set
restricted
to
the
class
the
network
has
not
been
trained
on
and
3
pure
white
noise
for
each
case
we
report
the
average
probabilities
predicted
by
the
bnn
for
all
input
images
in
the
considered
class
fig
2
the
average
standard
deviation
of
the
network
on
single
sample
and
the
standard
deviation
of
the
prediction
for
all
samples
while
not
as
informative
and
rigorous
as
collection
of
calibration
curves
these
measures
do
indicate
whether
the
bnn
uncertainty
matches
the
natural
variability
of
the
dataset
the
reported
results
show
that
for
class
seen
during
training
the
network
gives
the
correct
prediction
and
is
confident
about
its
results
the
variability
per
sample
and
the
variability
across
samples
are
coherent
with
one
another
and
quite
small
when
presented
with
digit
from
class
unseen
during
training
the
network
attempts
to
match
it
to
digits
that
share
similarities
however
the
predicted
low
probabilities
along
with
the
high
per-sample
and
across-sample
standard
deviation
show
that
the
network
is
aware
that
it
does
not
know
what
these
unseen
digits
are
as
for
the
white
noise
the
average
output
is
constant
meaning
the
networks
is
clear
about
the
fact
this
is
not
character
ii
practical
example
sparse
measure
the
second
practical
example
we
introduce
is
entitled
sparse
measure
below
we
describe
in
detail
the
definition
of
the
stochastic
model
for
this
example
we
also
provide
python
implementation
of
this
example
in
order
to
show
how
the
different
hypotheses
we
present
in
this
appendix
translate
to
actual
code
https://github.com/french-paragon/
sparse_measure_bnn
the
purpose
is
to
present
small
model
illustrating
how
different
training
strategies
can
be
integrated
in
bnn
the
problem
is
to
learn
function
r™
r™
based
on
adataset
{(x;,y,)|i
1
n],@
r",y
r™}
where
for
each
tuple
x
y
only
certain
elements
of
y
are
actually
measured
this
scenario
corresponds
to
any
application
of
machine
learning
where
measuring
raw
signal
represented
by
the
x;’s
is
easy
but
one
wants
to
train
an
algorithm
to
reconstruct
derived
signal
represented
by
the
y;’s
which
is
much
harder
to
acquire
on
top
of
that
the
elements
of
y
are
measured
with
certain
level
of
uncertainty
also
one
1.0
mee
within
sample
0.08
0.8+
mme
across
samples
061
0.06
oad
0.04
0.2
0.02
0.0
_
0.00
digits
a
results
on
seen
class
1.0
within
sample
0.8
0.3
mmm
across
samples
0.6
0.6
z02
o044
0.1
0.2
0.0
0.0
digits
digits
b
results
on
the
unseen
class
1.0
0.15
mmm
within
sample
0.8
mmm
across
samples
05
20.10
204
005
0.2
0.0
0.00
digits
digits
c
results
on
white
noise
fig
2
average
prediction
of
the
bayesian
mnist
practical
example
for
a
one
of
the
seen
class
b
the
unseen
class
and
c
white
noise
input
element
of
y
which
is
constant
but
unknown
has
much
higher
level
of
uncertainty
e.g
it
has
been
found
that
one
of
the
instruments
used
for
data
acquisition
is
defective
but
no
one
remembers
for
which
experiment
it
has
been
used
it
is
also
known
that
the
function
is
continuous
and
will
with
very
high
probability
map
any
given
input
to
point
near
known
hyperplane
of
r
the
dataset
is
generated
at
random
for
each
experiment
using
python
inputs
are
sampled
from
zero-mean
multi
variate
normal
distribution
with
random
correlation
matrix
the
input
is
then
multiplied
by
random
projection
matrix
to
r
before
non-linear
function
is
applied
random
set
of
three
orthonormal
vectors
in
r™
is
then
generated
and
used
to
reproject
the
values
in
the
final
output
space
constrained
on
known
hyperplane
final
non-linear
function
is
applied
on
the
input
and
its
results
are
added
to
the
previously
generated
outputs
with
very
low
gain
factor
to
ensure
that
the
actual
output
is
still
likely
to
be
near
the
known
hyperplane
finally
noise
is
added
to
the
training
data
with
one
channel
receiving
much
more
noise
than
the
other
channels
we
set
=m
9
the
measured
values
for
the
training
set
are
then
selected
at
random
the
other
values
being
set
to
0
set
of
binary
masks
are
returned
with
the
data
to
indicate
which
values
have
actually
been
measured
the
stochastic
model
is
the
most
interesting
part
of
this
case
study
note
that
learning
one
function
r®
r™
is
equivalent
to
learning
functions
r™
r
this
means
that
standard
bayesian
regression
would
be
sufficient
to
address
this
learning
problem
however
one
can
do
much
more
since
the
functions
under
consideration
can
be
correlated
with
one
another
first
the
model
can
be
extended
to
account
for
the
noisy
labels
to
learn
which
output
channel
is
actually
more
noisy
than
the
others
meta-learning
approach
can
be
used
with
hierarchical
bayes
applied
not
on
the
model
parameters
but
on
the
noise
model
if
applied
to
standard
point
estimate
networks
this
approach
would
be
seen
as
learning
loss
the
prior
can
be
extended
with
consistency
condition
to
account
for
the
fact
that
the
unknown
function
projects
points
near
known
hyperplane
last
but
not
least
semi-supervised
fig
3
bbn
corresponding
to
the
stochastic
model
used
for
the
sparse
measures
example
i’\
/’~
\po)_>\0'g)_
~
learning
model
can
be
used
to
share
information
across
the
training
samples
the
pgm
corresponding
to
this
approach
is
depicted
in
fig
3
we
kept
the
plate
for
the
dataset
as
we
will
use
second
consistency
condition
to
implement
the
semi-supervised
learning
strategy
we
also
chose
to
use
last
layer-only
bnn
mostly
to
illustrate
how
it
can
be
done
for
the
hierarchy
of
noise
quantification
variables
p
and
o
only
point
estimates
will
be
considered
to
simplify
the
model
the
functional
model
is
feedforward
artificial
neural
network
with
five
hidden
layers
fig
4
the
first
four
layers
are
point-estimate
layers
with
parameters
&
while
the
last
hidden
layer
and
the
output
layer
are
stochastic
with
parameters
6
we
also
have
three
monte-carlo
dropout
layers
at
the
end
of
the
network
the
hourglass
shape
of
the
network
is
supposed
to
match
the
expected
behaviour
of
the
studied
function
where
the
information
is
approximately
located
on
lower
dimensional
space
a
base
stochastic
model
the
base
stochastic
model
is
built
as
bayesian
regression
with
last-layer
bnn
architecture
the
base
probability
of
6
without
the
consistency
conditions
that
will
be
applied
later
on
has
been
set
to
normal
distribution
with
mean
and
standard
deviation
of
for
the
weights
and
10
for
the
bias
the
prior
distribution
of
the
output
knowing
the
input
and
the
parameters
and
is
then
given
by
mc-dropout
active
point
estimate
layers
leaky-relu
hidden
neurones
stochastic
layers
input
output
fig
4
the
neural
network
architecture
used
for
the
sparse
measure
example
p(ylz,0,8
d
0(2
0y
&
here
0
which
we
set
to
0.1
represent
the
small
uncertainty
due
to
the
fact
that
the
functional
model
will
never
be
able
to
perfectly
fit
the
actual
function
f
b
noisy
labels
model
for
this
regression
problem
we
assume
that
the
measure
ments
have
been
corrupted
by
zero-mean
gaussian
noise
this
is
not
only
convenient
formulation
but
also
reason
able
approximation
of
the
true
noise
model
for
continuous
measurements
due
to
the
central
limit
theorem
the
mean
of
the
noisy
measurements
is
thus
the
unobserved
true
function
value
y
for
the
standard
deviation
o3
we
assume
that
it
is
o4
0.1
for
inliers
and
o
for
outliers
the
problem
here
is
that
we
do
not
know
which
channel
has
been
corrupted
by
the
additional
noise
we
could
just
set
for
all
channels
constant
standard
deviation
that
is
slightly
above
o
but
instead
we
added
an
additional
variable
p
vector
representing
the
probability
that
given
channel
is
an
outlier
since
single
channel
is
corrupted
by
noise
we
could
constrain
p
to
lie
on
the
probability
simplex
instead
to
let
the
model
generalize
to
cases
with
more
than
single
noisy
channel
we
will
impose
that
p
0,1]™
for
convenience
we
will
consider
an
unconstrained
variable
and
define
po
as
function
of
to
simplify
the
optimisation
tyes
po
the
value
of
is
then
determined
by
bernoulli
distribution
with
parameter
po
bernoulli(p
o5
=b-0ou+(1—b
oin
the
only
thing
left
is
to
determine
the
prior
for
s
assuming
we
have
no
idea
which
channel
is
noisy
and
which
is
not
reasonable
hypothesis
is
gaussian
with
mean
and
large
covariance
matrix
x4
we
choose
x4
25001
yet
since
we
know
that
only
one
channel
is
noisy
we
can
add
consistency
condition
to
enforce
that
||po|l
1
or
any
expected
number
of
noisy
channels
1
here
7
is
scaling
factor
representing
the
confidence
one
has
in
the
model
capacity
to
fit
the
correct
number
of
noisy
channels
we
set
v
3000
as
we
are
almost
certain
the
model
should
be
able
to
fit
the
single
noisy
channel
the
probability
of
given
and
oy
is
then
given
by
1+e
p(s
0
sg)eap
{—%
h
p(gly.o5
n(y
o
5
since
we
are
not
so
interested
in
the
complete
posterior
distribution
for
or
even
oy
we
will
learn
point
estimates
of
those
variables
instead
and
then
derive
regularization
term
from
the
log
of
equation
4
the
final
contribution
for
the
learnable
parameters
to
the
total
loss
is
log(p(s
plus
an
unknown
constant
that
one
can
just
ignore
e(stes’is)-#%
(
1>
this
is
implemented
in
the
outlierawaresumsquareerror
learnable
loss
in
the
included
python
implementation
6
1+es
c
consistency
condition
the
function
is
likely
to
map
given
input
near
known
subspace
of
r™
which
can
be
represented
as
span(s
where
r™*3
is
given
orthonormal
matrix
the
distance
d
between
prediction
and
span(.s
is
then
given
by
dy.s
y
55ty
we
assume
priori
that
d
knowing
follows
normal
distribution
we
then
use
this
to
derive
consistency
condition
for
p(8
€|
)
secp
3l
pe0@
8
o@)|
t4
®
where
is
the
prior
standard
deviation
of
d
s
which
is
set
to
1
the
additional
contribution
to
the
loss
by
the
consistency
condition
is
thus
zgz
@579(1
sst(i>§79(i)“24
p(6,€d
p(o)p(€)e
d
semisupervised
learning
strategy
as
stated
in
section
iv-d1
of
the
main
paper
there
are
two
ways
one
can
implement
semi-supervised
learning
strategy
for
bnn
the
first
one
is
to
use
consistency
condition
i.e
data-driven
regularization
the
second
one
is
to
assume
some
kind
of
dependence
across
the
samples
we
use
the
former
for
this
practical
example
as
data-driven
regularization
is
generally
easier
to
implement
the
intuition
behind
this
example
is
that
if
two
points
from
the
input
space
are
close
to
one
another
then
their
image
by
should
also
be
close
to
one
another
the
simplest
approach
to
formally
measure
this
is
to
assume
that
the
norm
of
the
difference
in
the
output
space
normalized
by
the
distance
in
the
input
space
is
small
this
sets
the
consistency
condition
function
c(6
&
z
to
c(6,¢,x
%
x'€d,\{x}
averaging
this
consistency
condition
over
the
whole
training
set
d
is
equivalent
to
computing
the
sum
over
all
output
channels
of
®
of
the
quadratic
form
of
the
laplacian
matrix
of
the
dense
graph
spanned
by
the
training
set
d
where
the
weights
of
the
edges
are
given
by
the
inverse
euclidean
distances
of
the
points
graph
laplacian
regularization
is
itself
common
method
to
implement
semi-supervised
learning
3
this
approach
can
be
further
improved
assume
that
the
differences
between
the
components
of
two
random
input
b¢.6(x
peo(=
10
iz
2'il
fisher
pdf
mmm
observed
histogram
probability
density
200
400
600
relative
distance
squared
800
fig
5
comparison
between
the
observed
distribution
of
rela
tive
distances
squared
in
the
test
set
and
the
fisher—snedecor
distribution
of
parameters
and
9
vectors
and
x
as
well
as
the
differences
between
the
components
of
their
images
by
f
are
normally
distributed
then
tr
@
f2')]l5
f(n,m
d2a/
l
an
where
f(n,m
is
fisher-snedecor
distribution
with
pa
rameters
and
m
oa
is
the
prior
standard
deviation
of
the
difference
of
two
random
inputs
and
oay
is
the
prior
standard
deviation
of
the
difference
of
the
corresponding
outputs
as
shown
in
fig
5
such
assumptions
are
not
perfect
but
still
match
the
data
reasonably
well
especially
since
those
assumptions
are
just
prior
and
not
an
exact
model
the
most
important
point
to
notice
is
that
both
the
observed
and
prior
distributions
have
heavy
tail
this
would
not
be
the
case
with
the
consistency
condition
proposed
in
equation
10
this
naive
approach
would
penalize
outliers
too
heavily
we
thus
implemented
the
following
corrected
consistency
condition
function
in
the
sparse
measures
example
n+m
1
log(1
f
5
1
log(f
x’€d,\{z}
12
i3
where
f
/\am
is
the
fisher
statistic
ll
|
the
only
parameter
one
has
to
define
for
this
prior
is
aa
which
is
the
ratio
of
o
and
o3
;
we
set
aa
1
e
results
the
estimator
for
reaches
rmse
of
around
0.4
with
an
error
distribution
approximating
normal
distribution
variations
can
be
observed
over
mutliple
runs
as
different
function
is
generated
at
random
each
times
fig
6
the
python
code
also
computes
the
calibration
curve
using
the
hypothesis
for
regression
models
presented
in
section
vii
of
the
main
paper
the
results
fig
7
show
that
the
model
tends
to
be
slightly
underconfident
except
for
large
outliers
where
the
model
is
overconfident
this
is
probably
due
to
the
fact
that
our
simple
variational
inference
model
cannot
fit
the
exact
posterior
and
thus
tends
to
be
slightly
too
pessimistic
about
probability
density
==
4
3
2
1
actual
prediction
error
fig
6
prediction
error
distribution
for
the
sparse
measure
example
1.01
calibration
curve
==
ideal
curve
0.8
ee
0.6
0.4+
0.2
0.04
0.0
0.2
0.4
0.6
0.8
1.0
predicted
probability
fig
7
calibration
curve
for
the
sparse
measure
example
its
predictions
to
compensate
for
the
outliers
it
is
unable
to
efficiently
fit
iii
practical
example
paperfold
the
last
practical
example
we
introduce
is
entitled
pa
perfold
the
code
is
available
on
github
https://github.com/
french-paragon/paperfold-bnn
the
purpose
of
this
case
study
is
to
compare
different
inference
approaches
for
bnns
we
introduce
model
and
corresponding
dataset
which
is
small
enough
to
make
a
b
fig
8
stochastic
a
and
functional
b
model
for
the
paper
fold
case
study
all
existing
training
methods
for
bnns
tractable
when
used
for
the
corresponding
architecture
this
makes
the
number
of
parameters
small
enough
such
that
the
posterior
and
the
corresponding
approximations
are
low
dimensional
thus
they
can
be
easily
visualized
and
sampled
without
issues
using
exact
mcmc
methods
on
the
other
hand
the
model
is
complex
enough
to
display
most
of
the
issues
one
would
encounter
when
studying
bnn
posterior
the
problem
is
to
fit
one
dimensional
function
of
two
parameters
of
the
form
f(z,y
the
dataset
is
made
of
eight
x
y
z
samples
grouped
into
the
vectors
x,y
with
v2
15
v2
v2
7v2
e
1=
13
_z
2\/i
v2
~v2
z2
2\
2\
13
the
stochastic
model
fig
8a
is
the
usual
fully
supervised
baysian
regression
we
assume
each
measure
in
has
small
uncertainty
¢
n(0,02
with
o
being
small
positive
standard
deviation
that
one
can
set
depending
on
the
experiment
by
default
we
assume
that
o
0.1
the
functional
model
fig
8b
is
two-branch
and
two
layer
feedforward
neural
network
the
first
layer
branch
has
no
non-linearities
afterwards
while
the
second
branch
is
followed
by
relu
non-linearity
the
second
layer
simply
takes
the
weighed
sum
of
both
branches
and
thus
has
no
bias
this
model
reduces
to
function
of
the
form
vi(a1z
aoy
as
relu(b1z
by
b3
14
here
represents
the
parameters
of
the
first
branch
of
the
first
layer
the
parameters
of
the
second
branch
of
the
first
layer
and
the
parameters
of
the
second
layer
this
simple
model
represents
planar
function
with
single
fold
along
the
line
of
equation
81z
b2y
b3
0
this
model
has
always
two
and
only
two
non-equivalent
ways
of
fitting
the
proposed
dataset
see
fig
9
the
model
also
exhibits
some
weight-space
symmetry
in
other
words
the
half
planes
fitted
by
the
first
and
second
branches
of
the
first
layer
can
be
swapped
resulting
in
different
parametrization
but
equivalent
fitting
of
the
data
it
also
exhibits
scaling
symmetry
between
and
against
~
to
sample
the
posterior
we
first
have
to
choose
prior
for
the
parameters
o
and
of
the
bnn
the
following
procedure
works
for
any
choice
of
prior
p(a
3
p(+y
where
the
probability
distribution
of
the
parameters
of
the
first
layer
is
independent
of
the
probability
distribution
of
the
parameters
of
the
second
layer
for
simplicity
and
if
not
a
raw
data
b
first
possible
fold
c
second
possible
fold
fig
9
the
dataset
and
two
possible
folding
behaviors
of
the
model
for
the
paperfold
case
study
specified
otherwise
we
assume
normal
prior
with
diagonal
constant
covariance
~n(,0i
s
with
positive
standard
deviation
by
default
we
assume
that
we
do
not
know
much
about
the
model
and
set
for
this
case
study
a
comparison
of
training
methods
we
implemented
four
different
samplers
for
the
paperfold
example
fig
10
the
nuts
sampler
4
whose
implementation
is
pro
vided
by
the
pyro
python
library
this
is
state-of-the-art
general
purpose
mcmc
sampler
which
was
used
mainly
as
way
of
generating
samples
from
the
true
posterior
it
should
serve
as
base
of
comparison
with
other
methods
because
this
mcmc
sampler
is
much
slower
than
the
other
methods
we
consider
fig
13
variational
inference
model
based
on
gaussian
ap
proximation
of
the
posterior
an
ensemble
based
approach
and
an
ensemble
of
variational
inference
based
models
1
mcmc
the
mcmc
sampler
provides
very
good
approximation
of
samples
from
the
exact
posterior
in
fig
10
one
can
get
an
appreciation
of
the
complexity
of
the
ex
act
posterior
even
for
this
small
and
simple
model
each
row
and
column
represent
one
variable
with
the
graph
at
the
intersection
showing
the
samples
projected
in
the
2
dimensional
plane
spanned
by
those
variables
the
samples
form
perpendicular
and
diagonal
structures
which
probably
correspond
to
the
two
non-equivalent
ways
of
fitting
the
data
the
symmetric
lines
are
caused
by
the
weight-space
symmetry
the
hyperbolas
correspond
to
equivalent
parametrizations
of
the
network
under
scaling
symmetry
finally
the
few
outliers
that
are
visible
correspond
to
series
of
small
modes
in
comef
s
28
og
ge
&%
cowmrarces
ow
r
a3
cowes
0w
esem
aneee
beee
sl
b1
cmorn
ey
wwrwes
.
1
eo
e
csens
sues
l
%
euve
os
coete
cdrns
corees
tr
oo
catee
ot
ceiom
b3
vi
y2
a3
by
b2
b3
\a
y2
method
mcmc
vi
ensembling
vi+ensembling
fig
10
pairplot
of
the
samples
from
the
posterior
using
four
different
approaches
mean
standard
deviation
0.9
0.8
l0
0.8
0.7
05
0.7
0.6
0.6
05w
05~
04
04
03
05
0.3
02
1.0
0.2
0.1
0.1
10-05
00
05
1.0
1005
00
05
10
standard
deviation
0.9
0.8
0.7
0.6
05~
04
0.3
0.2
0.1
1005
00
05
10
mean
10-05
00
05
10
mean
standard
deviation
0.9
0.8
0.7
0.6
05w
04
0.3
02
0.1
1.0
0.5
0.0
0.5
1.0
c
d
standard
deviation
0.9
0.8
0.7
0.6
05~
04
0.3
0.2
0.1
1005
00
05
10
mean
0.9
0.8
07
0.6
05w
04
0.3
02
0.1
1.0
0.5
0.0
0.5
1.0
fig
11
mean
and
standard
deviation
of
the
marginal
distribution
of
knowing
d
and
for
the
paperfold
model
using
a
mcmc
b
variational
inference
c
ensembling
and
d
variational
inference
ensembling
y=0
x=0
y=0
1.00
1.00
0.75
0.75
1.04
0.50
0.50
0.5
0.25
0.25
0.00
0.00
0.0
1
1
1
x=y
x=-y
x=y
1.25
1.00
1.00
1o
1.00
0.75
0.75
0.75
0
0.50
50
oee
os
0.50
0.25
025
0.25
0.00
0.001
0.04
0.00
1
1
1.25
1.00
0.75
0.50
0.25
0.00
1.00
0.75
0.50
0.25
0.00
d
fig
12
predicted
values
across
four
different
lines
for
the
a
mcmc
b
variational
inference
c
ensembling
and
d
variational
inference
ensembling
version
of
the
paperfold
bnn
102
=z
10
—
mcmc
vi
ensembling
vi+ensembling
a
10
1072
1073
||
—
||
mcmc
vi
ensembling
vi+ensembling
b
fig
13
comparison
between
a
training
time
and
b
infer
ence
time
for
the
paperfold
example
with
standard
algorithms
for
the
different
training
approaches
the
posterior
around
the
parametrizations
fitting
the
constant
function
f(z,y
0.5
when
looking
at
the
actual
aggregate
results
fig
11a
one
can
see
that
the
average
prediction
of
the
bnn
fits
the
data
with
regular
function
similar
to
second
degree
polynomial
the
uncertainty
as
measured
by
the
standard
deviation
posteriori
is
low
along
the
lines
of
the
square
where
the
data
points
lie
and
increases
linearly
with
distance
creating
diamond
shape
sampling
the
predicted
value
for
along
series
of
lines
in
the
z,y
plane
of
coordinates
fig
12a
shows
that
both
folds
appear
as
clearly
distinct
from
one
another
but
bit
of
uncertainty
remains
around
the
exact
position
of
these
folds
2
variational
inference
as
variational
inference
approx
imation
to
the
posterior
we
used
normal
distribution
with
learnable
mean
and
diagonal
covariance
matrix
it
is
pretty
clear
that
for
this
specific
example
which
has
been
designed
to
generate
such
specific
problem
this
method
leads
to
very
poor
approximation
of
the
actual
posterior
since
it
can
only
fit
an
unimodal
distribution
fig
10
in
terms
of
the
marginal
only
one
of
the
two
possible
folds
fig
12b
has
been
fitted
this
translates
to
mean
estimate
for
as
well
as
an
uncertainty
level
and
distribution
well
off
the
actual
posterior
fig
11b
the
actual
side
of
the
fold
fitted
by
the
relu
branch
is
also
visible
as
its
uncertainty
is
higher
than
on
the
other
side
of
the
fold
this
highlights
the
major
limitations
of
simple
variational
inference
methods
for
bnns
since
the
underlying
models
are
complex
and
the
data
can
be
fitted
in
many
non-equivalent
manners
simple
variational
posteriors
lack
some
expressive
power
to
provide
good
estimate
of
the
actual
bnn
uncertainty
it
is
still
important
to
keep
in
mind
that
this
example
has
been
specifically
designed
to
make
those
problems
apparent
and
that
in
practice
mean
field
gaussian
and
similar
variational
inference
approximations
can
still
lead
to
good
estimations
of
the
actual
uncertainty
an
example
of
this
is
the
sparse
measure
example
provided
in
appendix
ii
variational
inference
is
also
several
orders
of
magnitude
faster
than
mcmc
fig
13
3
ensembling
the
most
straightforward
way
to
perform
ensemble
learning
with
an
artificial
neural
network
architec
ture
is
just
to
restart
the
learning
procedure
multiple
times
and
each
time
add
the
final
model
to
the
ensemble
using
this
strat
egy
to
learn
the
posterior
for
the
paperfold
example
already
gives
quite
good
results
even
better
than
naive
variational
inference
as
the
samples
can
belong
to
different
modes
of
the
posterior
fig
10
the
marginal
mean
and
standard
deviation
posteriori
are
clearly
different
from
the
ones
obtained
via
mcmc
but
the
diamond
shape
can
be
recongnized
in
both
predictions
fig
11c
the
training
is
slightly
longer
than
with
basic
variational
inference
as
it
has
to
be
run
multiple
times
however
the
time
per
sample
when
computing
the
marginal
is
extremely
low
fig
13
the
main
drawback
of
this
approach
is
that
it
provides
no
estimation
of
the
local
uncertainty
around
the
different
modes
of
the
posterior
as
shown
by
fig
12c
4
variational
inference
ensembling
combining
the
benefits
of
simple
variational
inference
and
ensembling
can
be
done
in
straightforward
manner
by
learning
an
ensemble
of
variational
approximations
the
resulting
distribution
is
gaussian
mixture
which
can
approximate
the
shape
of
mul
tiple
modes
of
the
posterior
in
this
example
it
cannot
match
the
complex
shapes
of
the
exact
posterior
fig
10
despite
those
limitations
the
resulting
marginal
for
is
very
good
match
with
the
one
generated
via
mcmc
sampler
fig
11
and
12
in
addition
the
time
per
sample
when
computing
the
marginal
is
still
very
similar
to
the
single
gaussian
variational
approximation
fig
13
this
shows
the
huge
benefits
of
ensembling
as
tool
for
approximate
bayesian
methods
in
deep
learning
references
1
y
lecun
l
bottou
y
bengio
and
p
haffner
gradient-based
learning
applied
to
document
recognition
proceedings
of
the
ieee
vol
86
no
11
pp
2278-2324
1998
2
d
ciregan
u
meier
and
j
schmidhuber
multi-column
deep
neural
networks
for
image
classification
in
2012
ieee
con
ference
on
computer
vision
and
pattern
recognition
2012
pp
3642-3649
3
m
belkin
p
niyogi
and
v
sindhwani
manifold
regular
ization
geometric
framework
for
learning
from
labeled
and
unlabeled
examples
j
mach
learn
res
vol
7
pp
2399
2434
dec
2006
4
m
d
hoffman
and
a
gelman
the
no-u-turn
sampler
adaptively
setting
path
lengths
in
hamiltonian
monte
carlo
journal
of
machine
learning
research
vol
15
no
1
pp
1593
1623
2014
