categorizing
ancient
documents
nizar
zaghden
remy
mullot
mohamed
adel
alimi®
1
regim
research
group
on
intelligent
machines
university
of
sfax
enis
department
of
electrical
engineering
bp
3038
sfax
tunisia
nizar.zaghden@gmail.com
adel.alimi
@ieee.org
l3i
laboratoire
informatique
image
interaction
université
de
la
rochelle
france
bp
17042
remy.mullot@univ-ir.fr
abstract
the
analysis
of
historical
documents
is
still
topical
issue
given
the
importance
of
information
that
can
be
extracted
and
also
the
importance
given
by
the
institutions
to
preserve
their
heritage
the
main
idea
in
order
to
characterize
the
content
of
the
images
of
ancient
documents
after
attempting
to
clean
the
image
is
segmented
blocks
texts
from
the
same
image
and
tries
to
find
similar
blocks
in
either
the
same
image
or
the
entire
image
database
most
approaches
of
offline
handwriting
recognition
proceed
by
segmenting
words
into
smaller
pieces
usually
characters
which
are
recognized
separately
recognition
of
word
then
requires
the
recognition
of
all
characters
ocr
that
compose
it
our
work
focuses
mainly
on
the
characterization
of
classes
in
images
of
old
documents
we
use
som
toolbox
for
finding
classes
in
documents
we
applied
also
fractal
dimensions
and
points
of
interest
to
categorize
and
match
ancient
documents
keywords
som
toolbox
fractal
dimension
points
of
interest
categorizing
ancient
documents
1
introduction
the
analysis
of
historical
documents
is
still
topical
issue
given
the
importance
of
information
that
can
be
extracted
and
also
the
importance
given
by
the
institutions
to
preserve
their
heritage
typically
an
analysis
of
ancient
documents
is
preceded
by
cleaning
phase
of
noisy
images
the
preprocessing
or
enhancement
of
the
quality
of
the
images
of
ancient
documents
seems
important
this
subject
is
vast
and
there
are
variety
of
methods
and
techniques
that
have
been
applied
in
order
to
achieve
analysis
or
extracting
information
from
images
of
old
documents
characterization
of
content
or
indexing
1
and
text
search
works
from
ancient
documents
2
are
examples
of
applications
that
we
can
apply
on
the
old
documents
in
fact
before
choosing
to
apply
such
an
application
on
ancient
documents
we
must
first
target
the
audience
who
will
use
our
application
such
applications
can
be
divided
into
three
categories
librarians
who
want
to
find
printed
ancient
history
students
or
researchers
in
this
paper
we
are
interested
in
categorizing
the
content
of
images
of
old
documents
we
apply
several
methods
in
order
to
characterize
the
content
of
images
of
old
documents
madonna
bases
library
tunisian
national
3
the
british
library
and
also
manuscripts
of
george
washington
will
be
the
scope
of
our
application
http://www.wdl.org/fr/).we
do
not
believe
this
until
we
can
develop
system
capable
of
characterizing
efficiently
and
equally
different
images
from
our
heterogeneous
database
however
we
try
to
find
systems
that
are
more
adaptable
to
each
of
the
bases
on
which
we
work
similarly
we
believe
that
the
techniques
based
on
the
segmentation
of
text
character
remain
valid
for
printed
documents
madonna
but
are
not
directly
used
for
handwritten
bases
manuscripts
of
george
washington
or
for
the
arabic
script
is
cursive
in
nature
to
characterize
the
classes
contained
in
the
same
images
we
made
use
k
means
classifier
with
which
we
were
able
to
identify
the
different
classes
belonging
to
our
image
shown
in
figure
1(fig
1
k-means
in
effect
is
tool
classical
classification
that
divides
set
of
data
into
classes
homogeneous
most
images
locally
verify
properties
of
homogeneity
particularly
in
terms
of
light
intensity
the
following
figure
shows
in
fact
the
application
of
k-means
on
document
image
old
five
different
figures
represent
the
five
different
classes
extracted
by
this
algorithm
the
major
drawback
of
k-means
is
the
number
of
classes
is
chosen
in
advance
x%
~==0>
=y
=y
erseias
fig
different
classes
of
an
image
after
application
of
k-means
this
can
lead
in
fact
or
the
overlapping
of
two
classes
number
of
classes
priori
selected
is
less
than
the
number
of
classes
in
the
image
or
the
excess
of
classes
that
can
be
classified
in
an
image
this
exceeded
classes
may
be
for
example
in
an
image
that
has
only
two
classes
while
we
selected
number
of
classes
in
advance
of
four
or
five
which
is
well
above
the
classes
may
be
contained
in
the
image
this
is
bit
similar
to
the
phenomenon
of
over-segmentation
estimated
by
al
badr
in
4]).we
also
used
the
toolbox
som
5
to
categorize
the
documents
the
results
we
found
are
relatively
average
fig
1
and
this
is
mainly
because
we
work
with
document
images
which
we
cannot
estimate
the
number
of
classes
that
can
be
contained
in
the
same
page
and
thus
to
remedy
the
defect
of
these
methods
it
would
be
more
reasonable
to
work
with
unsupervised
classification
tools
because
we
cannot
estimate
in
advance
the
number
classes
in
any
possible
basis
on
which
we
try
to
categorize
indeed
the
k
means
algorithm
is
used
to
determine
the
various
classes
and
from
an
image
these
images
we
can
find
picture
in
the
foreground
background
image
2
characterization
of
ancient
documents
by
fractal
dimensions
several
methods
have
been
developed
to
calculate
the
fractal
dimension
as
method
of
counting
box
box
counting
6
and
the
method
of
dilation
the
box
counting
dimension
is
considered
the
simplest
method
for
calculating
the
fractal
dimension
and
is
applied
to
pictures
that
are
not
empty
from
the
properties
of
the
self-similarity
fractal
dimension
of
set
is
defined
in
eq
1
d=log
n)/log(1/r
1
is
the
total
number
of
separate
copies
similar
to
a
and
/1
is
the
scale
factor
with
which
is
divided
we
opted
to
use
several
methods
to
characterize
fonts
first
we
have
chosen
to
apply
the
fractal
dimension
major
feature
of
fractal
geometry
for
this
we
implemented
two
algorithms
on
the
method
of
counting
box
and
an
algorithm
for
the
method
of
dilation
in
fact
the
method
of
counting
box
box-counting
is
based
on
the
size
of
hausdorff
as
for
the
method
of
dilation
it
is
based
on
the
dimension
of
minkowski
bouligand
7
the
latter
is
in
fact
an
increase
of
the
hausdorff
dimension
7
the
results
obtained
with
the
fractal
dimension
show
the
discriminating
power
of
this
method
in
recognizing
printed
arabic
fonts
in
fact
the
calculation
of
fractal
dimensions
of
images
is
simple
and
requires
no
preprocessing
step
of
images
of
writing
we
also
compared
the
algorithms
of
fractal
dimensions
that
we
have
developed
with
other
methods
on
the
same
images
and
textures
we
noticed
the
clear
superiority
of
the
new
approach
that
we
have
adopted
counting
density
per
box
cdb
the
performance
of
this
method
in
noised
images
pushed
us
to
test
this
method
in
old
documents
indeed
the
fractal
dimensions
from
the
cdb
method
for
resolutions
greater
than
60
dpi
achieved
rates
recognition
almost
similar
to
the
results
obtained
with
the
same
approach
for
clear
images
the
results
for
the
variation
of
resolution
are
shown
in
table
1
table
1
font
recognition
rate
for
various
resolutions
resolution
300
200
100
75|60
|50
40
rate
%
965
965
965
96
95
85.5
80
in
terms
of
recognition
rates
we
noticed
significant
performance
the
new
method
we
have
adopted
for
the
calculation
of
the
fractal
dimension
in
fact
this
method
alone
has
achieved
recognition
rate
of
96.5%
the
results
show
that
the
methods
we
have
adopted
for
the
recognition
of
multifont
texts
can
be
applied
to
develop
robust
system
of
character
recognition
indeed
the
methods
we
have
developed
can
be
applied
to
number
of
larger
fonts
and
from
heterogeneous
databases
with
variation
of
resolution
and
degradation
non
uniform
noise
as
if
images
of
old
documents
in
the
state
of
the
art
collection
of
images
with
tiles
there
is
different
practices
recovery
and
various
studies
show
that
they
lead
to
results
the
determination
method
called
method
of
boxes
commonly
used
in
analysis
of
images
using
grid
of
the
map
mesh
much
finer
fig
2
l
l
l
l
l
l
l
b
recovering
object
with
square
of
size
1/2
fig
box
counting
method
the
new
method
we
developed
to
calculate
the
fractal
dimension
is
inspired
by
the
box
counting
method
8
among
the
other
derivatives
this
method
there
is
the
method
of
counting
reticular
cells
as
the
box-counting
differential
6
3
characterization
of
ancient
documents
by
local
approaches
characterization
and
interpretation
of
the
content
of
textured
images
is
problem
important
for
many
application
areas
many
works
based
on
statistical
descriptors
but
recent
work
has
demonstrated
the
relevance
of
the
approaches
based
on
the
extraction
and
characterization
of
points
of
interest
9
we
studied
two
types
of
local
approaches
points
of
interest
methods
and
pixel
by
pixel
methods
the
interest
of
points
of
interest
approaches
lies
in
their
properties
including
invariance
change
of
contrast
and
affine
images
region
of
an
image
comprises
characteristic
points
of
an
image
carrier
in
particular
information
called
points
of
interest
such
points
belong
outline
of
picture
we
speak
in
this
case
of
contour
points
to
overcome
the
problems
of
characterization
of
pages
of
documents
execution
time
computation
time
of
the
feature
vectors
.
it
was
necessary
to
move
towards
new
methodology
to
accurately
locate
the
text
areas
identify
shapes
redundant
without
using
any
segmentation
of
the
page
and
thus
leading
to
form
intelligent
partition
of
the
image
centered
around
the
interesting
areas
figure
shows
the
points
of
interest
of
line
of
text
fig
a
as
well
as
points
of
interest
image
fig
b
comers
detected
/&7{,}%»
{4/'.”}
a
points
of
interests
for
text
line
comers
detected
/{,,7,4.“1
ém
lok
w»wd}
u
14
v%’—f
3
ae
e
_l/
/kw
=hizs
wu
b
image
points
of
interest
fig
detection
of
points
of
interest
on
images
of
ancient
documents
by
the
sift
method
we
can
recall
here
that
the
partitioning
of
the
image
is
the
basis
of
many
methods
and
recognition
that
can
be
achieved
ideally
through
research
guided
by
rois
in
the
context
of
global
characterization
of
words
an
approach
to
the
description
of
local
points
of
interest
in
an
image
was
studied
it
allows
information
to
calculate
local
level
of
the
contour
points
of
region
of
interest
of
the
image
all
these
local
information
can
then
calculate
signature
representative
of
the
region
of
interest
of
the
image
to
identify
the
form
in
its
entirety
in
this
context
it
is
to
measure
the
distances
between
shapes
by
matching
points
between
two
images
that
is
to
say
the
recognition
of
the
points
in
common
between
at
least
two
digital
images
we
compare
two
points
of
interest
images
based
on
british
library
function
from
the
xor
fig
4
fig
applying
xor
for
two
occurrences
of
the
word
alexandria
thus
we
can
characterize
different
text
blocks
of
the
same
size
with
the
function
xor
in
which
we
represent
in
the
resulting
image
white
pixel
if
it
is
white
pixel
in
one
of
the
first
two
images
and
it
is
the
opposite
in
the
other
file
so
xor
do
the
comparison
pixel
by
pixel
calculating
the
similarity
between
two
shapes
occurs
from
the
symmetric
differences
between
the
new
form
and
the
model
images
c
and
d
of
the
previous
figure
are
the
result
of
the
xor
function
on
both
photo
a
and
b
in
figure
we
present
the
result
of
the
xor
function
of
two
occurrences
of
the
same
line
fig
5
thus
we
can
conclude
that
the
xor
function
can
we
solve
the
problem
of
word
recognition
images
of
old
documents
when
forms
are
strictly
identical
and
repositioned
relative
to
each
other
in
fact
we
have
applied
the
euclidean
distance
map
edm
algorithm
euclidian
distance
map
to
calculate
the
error
between
images
in
fact
each
white
pixel
in
the
resulting
image
corresponds
to
an
error
and
the
edm
function
applied
to
xor
allows
us
to
obtain
vector
measuring
the
error
between
two
images
this
basic
approach
has
many
disadvantages
that
make
it
use
very
difficult
because
the
characterization
by
points
of
interest
can
bring
sometimes
non
similar
words
even
if
they
have
similar
characteristics
mayor
tanto
t
humilha
@
mayor
tanto
tc
humilha
b
foatoc
eante
&2
humilha
©
fig
applying
xor
for
two
occurrences
of
the
same
line
3.1
application
on
pseudowords
wordspotting
pixel
by
pixel
methods
is
also
studied
here
illustrated
by
the
wordspotting
method
which
can
actually
perform
the
matching
of
words
pictures
of
old
documents
the
only
problem
resided
in
fact
on
the
basis
of
images
that
we
have
george
washington's
manuscripts
have
many
redundant
words
and
many
researchers
have
worked
on
it
the
contribution
that
we
wanted
to
do
is
not
to
work
with
collection
of
words
but
to
work
with
images
characters
or
blocs
of
characters
documents
on
which
we
conduct
our
approach
are
only
old
documents
prints
indeed
we
perform
an
initial
image
segmentation
based
on
the
method
developed
in
10
to
group
blocks
of
text
pictures
then
we
treat
each
block
choosing
as
criterion
for
separation
between
the
characters
to
be
segmented
the
presence
of
white
pixels
admittedly
this
approach
allows
the
correspondence
between
characters
on
old
documents
printed
but
will
also
blobs
from
ancient
characterize
pseudowords
manuscripts
including
arabic
characters
printed
or
manuscripts
because
of
the
nature
of
the
arabic
script
which
is
cursive
in
nature
11
in
the
first
place
we
chose
to
work
with
the
xor
function
followed
by
the
algorithm
edm
to
calculate
the
errors
between
the
query
images
and
images
of
the
base
fig
6
images
c
and
d
respectively
and
the
results
complement
the
results
of
the
xor
function
applied
to
the
frames
a
and
b
figure
6
a
b
©
d
fig
6
applying
the
xor
function
of
the
two
images
character
m
secondly
we
calculated
the
projection
profile
character
images
figure
7).the
result
of
each
character
is
actually
compared
with
the
other
characters
in
the
document
one
of
the
main
advantages
is
that
we
do
not
know
the
number
of
classes
or
characters
in
advance
whether
in
document
image
or
any
former
base
fig
7
vertical
profile
of
the
character
m
3.2
performance
of
wordspotting
to
evaluate
different
algorithms
for
image
retrieval
by
content
we
require
measure
of
performance
there
are
several
types
of
performance
measurement
such
as
pwh
weighted
percentage
of
hits
rpp
recall
and
precision
pair
and
psr
percentage
of
similarity
ranking
we
will
detail
the
rpp
which
defines
the
measures
of
precision
and
recall
performance
research
is
evaluated
using
precision
and
recall
precision
is
defined
as
is
the
ratio
of
the
number
of
relevant
images
to
the
desired
total
number
of
images
found
it
therefore
measures
the
system's
ability
to
find
relevant
images
the
point
is
defined
as
the
number
of
images
on
the
relevant
desired
number
total
relevant
images
in
the
entire
database
so
it
measures
the
ability
of
system
to
find
only
relevant
images
precision
and
recall
are
given
in
eq
2
p=r/nandr=r/m
2
to
measure
these
parameters
we
assume
that
the
image
database
is
composed
of
classes
disjointed
images
where
we
know
the
cardinals
respective
m
all
pictures
the
base
are
taken
successively
as
images
and
queries
we
see
images
returned
to
rank
variable
for
setting
reminder
we
draw
curves
n
faster
the
curve
tends
to
and
the
system
will
perform
for
parameter
precision
we
draw
curves
p
n
the
system
is
more
accurate
and
more
curve
decreases
slowly
the
images
that
we
have
applied
our
algorithms
are
those
from
the
base
madonna
and
gutenberg
since
the
basis
of
the
national
library
contains
much
more
noise
and
that
the
techniques
we
developed
are
not
sufficiently
effective
on
such
images
also
degraded
other
studies
related
to
the
processing
of
old
documents
interested
firstly
on
the
identification
phase
defects
on
ancient
documents
we
chosen
in
our
case
study
work
with
the
gaussian
filter
in
the
above
objective
eliminate
pixels
away
but
throughout
our
report
we
avoid
making
the
preprocessing
of
document
images
for
causes
on
the
one
hand
with
some
pretreatments
we
will
lose
the
information
of
images
of
old
documents
that
may
be
relevant
for
the
description
and
characterization
of
this
image
on
the
other
hand
the
noise
varies
from
document
image
old
compared
to
another
so
that
we
do
not
let
the
choice
of
finding
suitable
filters
to
remove
noise
generated
in
base
heterogeneous
unless
it
is
the
same
basic
container
may
be
the
same
defects
tasks
ink
and
water
stains
or
other
our
experiments
have
touched
only
the
images
of
latin
characters
whose
number
is
7280
then
we
tested
images
of
words
in
printed
documents
which
number
is
100
these
test
images
are
repeated
135
times
in
the
images
seen
in
our
learning
base
there
are
words
that
are
repeated
more
than
once
we
have
calculated
the
terms
precision
and
recall
for
the
two
different
methods
we
have
adopted
xor
edm
after
applying
xor
vertical
projection
followed
by
edm
algorithm
we
present
in
table
the
results
obtained
with
the
three
methods
described
table
2
precision
rate
and
recall
methods
adopted
method
xor
edm
projection
profile
precision
%
63.14
78.43
41.67
recall
%
55.34
79.32
34.46
we
can
also
notice
that
the
best
recognition
rates
were
obtained
with
edm
method
while
this
rate
is
still
high
in
comparison
with
the
methods
wehave
tested
but
it
will
certainly
be
improved
by
adding
other
criteria
among
the
works
that
we
are
making
now
there
are
correct
and
improving
the
inclination
of
characters
from
old
documents
other
techniques
are
also
our
present
field
of
study
such
as
dtw
dynamic
time
wrapping
function
used
in
the
analysis
of
time
signals
but
we
will
use
in
our
field
study
despite
their
effectiveness
local
methods
have
the
following
disadvantages
calculation
time
prohibitive
depending
on
the
size
of
the
analysis
window
over-segmentation
faults
and
paper
texture
on
the
background
of
the
image
treatment
of
difficult
documents
font
size
is
very
variable
analysis
window
is
fixed
throughout
the
treatment
to
address
the
problem
of
variability
in
the
size
of
the
images
to
be
compared
we
use
the
detector
points
of
interest
we
tried
mainly
the
harris
detector
har
98
and
the
sift
descriptor
13
the
first
detector
can
bring
good
results
especially
for
images
standardized
however
it
fails
for
images
of
varying
size
the
solution
is
to
use
detectors
points
of
interest
such
as
multi-scale
sift
which
is
the
most
robust
sensor
currently
in
the
literature
14
3.3
application
of
points
of
interest
on
the
entire
of
the
document
image
generally
matching
algorithm
based
on
the
points
of
interest
is
divided
into
three
parts
1
detection
of
points
of
interest
harris
difference
of
gaussians
in
sift
.
2
characterization
of
points
of
interest
eg
each
point
of
interest
we
associate
vector
pixel
values
in
neighborhood
local
jet
successive
derivatives
.
or
even
histogram
sift
3
mapping
to
simplify
just
from
distance
euclidean
mabhanalobis
.
and
measure
the
distances
between
the
vectors
of
points
of
interest
from
the
image
and
those
of
the
image
2
if
the
distance
is
smaller
than
certain
threshold
we
set
then
we
put
mapping
the
points
harris
detector
12
is
corner
detector
and
it
is
not
multi
scale
that
is
to
say
that
if
we
compare
two
images
one
of
which
is
zoom
of
the
other
using
the
harris
detector
the
rate
of
correspondence
will
be
weakened
due
to
the
property
of
detector
non
multi-scale
the
solution
is
to
adopt
multi-scale
detector
such
as
sift
which
has
given
best
results
compared
to
other
detectors
14
characterization
is
to
find
information
on
point
of
interest
other
than
its
position
in
what
follows
we
will
detail
the
steps
taken
to
make
the
correspondence
pictures
of
old
documents
based
on
the
sensor
point
of
interest
sift
13
as
first
step
we
work
with
hundreds
of
pictures
of
old
documents
to
see
the
results
and
then
make
the
appropriate
steps
according
to
the
results
if
they
are
interesting
or
not
as
the
number
of
images
100
images
on
which
we
performed
correspondence
is
high
we
applied
first
phase
of
rejection
this
phase
is
based
on
characterizations
overall
image
by
the
application
of
the
fractal
dimension
indeed
we
have
shown
previously
that
this
technique
can
achieve
distinction
or
grouping
of
different
classes
of
images
but
it
remains
still
insufficient
to
match
the
images
of
old
documents
efficiently
this
is
why
we
chose
to
combine
those
techniques
to
local
overall
to
get
the
best
results
from
characterization
of
the
images
of
old
documents
we
illustrate
in
fig
the
overall
scheme
of
this
system
15
we
applied
our
system
to
approximately
1000
images
from
our
database
because
treatments
preprocessing
and
normalization
we
calculated
fractal
dimensions
for
all
images
from
our
database
the
choice
of
method
is
fractal
due
to
its
discriminating
power
but
also
to
the
gain
in
computation
time
compared
with
other
global
methods
16
in
the
image
below
we
present
the
principle
of
elimination
set
of
images
with
fractal
dimensions
above
certain
threshold
manually
at
first
in
the
second
step
we
consider
just
the
images
belonging
to
group
and
group
2
we
use
these
images
to
the
sift
descriptor
for
target
images
that
best
match
the
query
image
this
is
very
useful
because
we
working
with
large
number
of
images
the
number
of
resulting
images
is
not
always
fixed
but
depends
mainly
on
the
query
image
and
also
the
number
of
images
similar
to
the
query
image
global
features
fractal
dimensions
elimination
of
images
different
from
the
image
request
local
features
sift
descriptor
extract
images
which
correspond
well
to
the
image
request
fig
8
global
scheme
of
the
method
in
the
second
step
we
consider
just
the
images
belonging
to
group
and
group
2
we
use
these
images
to
the
sift
descriptor
for
target
images
that
best
match
the
query
image
this
is
very
useful
because
we
working
with
large
number
of
images
even
if
they
are
carried
out
from
arabic
and
latin
bases
17
the
number
of
resulting
images
is
not
always
fixed
but
depends
mainly
on
the
query
image
and
also
the
number
of
images
similar
to
the
query
image
we
performed
preprocessing
phase
on
1000
images
from
our
database
in
order
to
work
with
normalized
images
with
resolution
of
300
dpi
and
the
same
size
512
512
pixels
the
system
adopted
here
give
good
results
compared
with
other
methods
in
time
of
execution
and
number
of
iterations
but
it
can
be
more
precise
if
we
apply
other
parameters
issued
from
fuzzy
logic
18
to
add
the
membership
degree
of
each
class
4
conclusion
we
applied
several
experiments
to
categorize
the
content
of
images
old
documents
the
determination
of
number
of
classes
in
an
image
document
was
achieved
by
k-means
we
tried
also
to
categorize
images
through
global
approach
with
the
fractal
dimension
we
were
able
to
identify
portions
of
images
belonging
to
the
same
classes
given
the
variability
of
the
content
of
the
images
we
are
dealing
with
as
well
as
their
huge
number
we
decided
to
combine
the
approaches
with
holistic
approaches
local
descriptors
using
points
of
interest
with
the
harris
detector
we
could
apply
the
wordspotting
on
the
pseudowords
images
of
ancient
documents
then
we
calculated
the
errors
between
the
query
images
and
target
images
using
the
xor
function
and
algorithm
euclidian
distance
map»
edm
the
disadvantage
of
this
method
is
mainly
the
execution
time
since
we
work
with
local
approaches
on
huge
image
database
we
also
applied
the
sift
descriptor
with
images
of
old
documents
this
phase
was
carried
out
after
the
first
stage
of
rejection
distant
images
of
the
query
image
based
on
the
fractal
dimensions
the
choice
of
this
approach
is
due
to
its
overall
discriminating
power
but
also
to
the
execution
time
which
is
rapid
in
comparison
with
the
other
methods
references
1
n
journet
analyse
d'images
de
documents
anciens
une
approche
texture.phd
thesis
l3i
universit
de
la
rochelle
2006
2
yann
leydier
frank
lebourgeois
hubert
emptoz
text
search
for
medieval
manuscript
images
pattern
recognition
40(12
2007
pp
3552
3567
3
http://www.bnt.nat.tn/
4
b.al
badr
r.m.harralick
segmentation
free
word
recognition
with
application
to
arabic
ieee
3rd
proceedings
of
the
international
conference
on
document
analysis
and
recognition
icdar
95
vol
1
n°
1
montreal
canada
january
1995
pp355-359
5
som
toolbox
downloaded
from
ttp://www.cis.hut.fi/projects/somtoolbox/
2002
6
n.sarkar
and
b.b.chaudhuri
an
efficient
differential
box-counting
approach
to
compute
fractal
dimension
of
image
ieee
trans.systems
man
and
cybernitics
vol
24
ne°
1
january
1994
pp
115-120
7
j.b
florindo
o.m
bruno
closed
contour
fractal
dimension
estimation
bythe
fourier
transformoriginal
chaos
solitons
fractals
volume
44
issue
10
october
2011
pp
851
861
8
n.zaghden,s.b.moussa
a.m
alimi
reconnaissance
des
fontes
arabes
par
i’utilisation
des
dimensions
fractales
et
des
ondelettes
colloque
international
francophone
sur
1’ecrit
et
le
document
cifed
06),fribourg(suisse
september
18-21,2006
pp:277_282
9
c
schmid
r
mohr
c
bauckhage
comparing
and
evaluating
interest
points
iccv
1998
10]b
khelifi
n
elleuch
s
kanoun
a
m
alimi
enhanced
color
based
method
for
text
extraction
from
maps
international
conference
mm-cca
jordan
2007
11
n
zaghden
b
khelifi
a
alimi
r
mullot
text
recognition
in
both
ancient
and
cartographic
documents
conference
on
virtual
systems
and
multimedia
dedicated
to
digital
heritage
vsmm
2008
2008
pp
98-102
12]c
harris
and
m.j
stephens
combined
corner
and
edge
detector
in
alvey
vision
conference
1988
pp
147-152
13
d
lowe
distinctive
image
features
from
scale-invariant
eypoints
intl
j
of
comp
vision
60
2004
pp
91-110
14
m.awrangjeb
guojun
lu
efficient
and
effective
transformed
image
identification
multimedia
signal
processing
2008
ieee
10th
workshop
on
8-10
oct
2008
pp:563
568
15
nizar
zaghden
remy
mullot
slim
kanoun
and
adel
alimi
a
proposition
of
robust
system
for
historical
document
images
indexation
international
journal
of
computer
applications
december
2010
0975
8887),11(2):pp
10-15
16
b
khelifi
n
zaghden
a
m
alimi
unsupervised
categorization
of
heterogeneous
text
images
based
on
fractals
“
19th
international
conference
on
pattern
recognition
tampa
florida
usa
december
8-11
2008
17
n
zaghden
r
mullot
a.m
alimi
characterization
of
ancient
document
images
composed
by
arabic
and
latin
scripts
innovations
in
information
technology
iit
innovations’11
2011
pp
124-127
18
j.harikiran
d.ramakrishna
m.l.phanendra,.p.v.lakshmi
r.kiran
kumar
fuzzy
c-means
with
bi-dimensional
empirical
modedecomposition
for
segmentation
of
microarray
image
»
1jcsi
international
journal
of
computer
science
issues
vol
9
issue
5
no
3
september
2012
issn
online
1694-0814
nizar
zaghden
was
born
in
sfax
1978
he
received
the
b.s
degree
in
computer
science
from
the
sciences
faculty
fss
in
2003
and
the
master
degrees
in
computer
science
from
the
national
engineering
school
of
sfax
tunisia
enis
in
2005
on
march
2013
he
became
philosiphate
doctorate
after
defending
thesis
he
became
doctorate
in
computer
science
he
joined
the
sfax
university
uss
where
he
was
an
assistant
professor
in
the
department
of
computer
science
of
the
management
faculty
of
sfax
gestion
he
is
now
attached
to
the
university
of
gabes
where
he
works
as
assistant
in
the
higher
institute
of
computer
science
in
medenine
he
is
member
of
the
research
group
on
intelligent
machines
regim
his
research
interests
include
computer
vision
and
image
and
video
analysis
these
research
activities
are
centered
around
document
analysis
and
pattern
recognition
he
is
graduate
student
member
of
ieee
he
was
the
member
of
the
organization
committee
of
the
international
conference
on
machine
intelligence
acidca-icmi'2005
rémy
mullot
is
professor
and
director
of
the
l3i
laboratory
in
la
rochelle
in
france
his
research
activities
are
placed
in
the
context
of
recognition
and
indexing
documents
since
these
processes
are
associated
with
content
analysis
of
the
image
the
targets
are
very
wide
because
they
can
focus
on
ancient
documents
for
which
the
laboratory
has
developed
real
expertise
but
more
broadly
to
any
type
of
highly
structured
materials
mainly
based
on
textual
components
or
low
structured
mainly
based
on
graphical
data
these
research
themes
are
very
cross
because
they
are
both
extractors
indices
signatures
on
treatment
systems
through
the
categorization
of
content
structures
styles
these
activities
are
opportunities
direct
projects
larger
laboratory
in
conjunction
with
other
skills
in
l3i
ontologies
and
images
adel
m
alimi
was
born
in
sfax
tunisia
in
1966
he
graduated
in
electrical
engineering
1990
obtained
phd
and
then
an
hdr
both
in
electrical
computer
engineering
in
1995
and
2000
respectively
he
is
now
professor
in
electrical
computer
engineering
at
the
university
of
sfax
his
research
interest
includes
applications
of
intelligent
methods
neural
networks
fuzzy
logic
evolutionary
algorithms
to
pattern
recognition
robotic
systems
vision
systems
and
industrial
processes
he
focuses
his
research
on
intelligent
pattern
recognition
learning
analysis
and
intelligent
control
of
large
scale
complex
systems
he
is
associate
editor
and
member
of
the
editorial
board
of
many
international
scientific
journals
e.g
pattern
recognition
letters
neurocomputing
neural
processing
letters
international
journal
of
image
and
graphics
neural
computing
and
applications
international
journal
of
robotics
and
automation
international
journal
of
systems
science
etc
he
was
guest
editor
of
several
special
issues
of
international
journals
e.g
fuzzy
sets
systems
soft
computing
journal
of
decision
systems
integrated
computer
aided
engineering
systems
analysis
modelling
and
simulations
he
was
the
general
chairman
of
the
international
conference
on
machine
intelligence
acidca-icmi'2005
2000
he
is
an
ieee
senior
member
and
member
of
iapr
inns
and
prs
he
is
the
2009-2010
ieee
tunisia
section
treasurer
the
2009-2010
ieee
computational
intelligence
society
tunisia
chapter
chair
the
2011
|eee
sfax
subsection
the
2010-2011
ieee
computer
society
tunisia
chair
the
2011
ieee
systems
man
and
cybernetics
tunisia
chapter
the
smcs
corresponding
member
of
the
ieee
committee
on
earth
observation
and
the
ieee
counselor
of
the
enis
student
branch
10
