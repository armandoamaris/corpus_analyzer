category
theory
in
machine
learning
dan
shiebler
bruno
gavranovié
paul
wilson
university
of
oxford
university
of
strathclyde
university
of
southampton
daniel.shiebler@kellog.ox.ac.uk
bruno@brunogavranovic.com
paul@statusfailed.com
over
the
past
two
decades
machine
learning
has
permeated
almost
every
realm
of
technology
at
the
same
time
many
researchers
have
begun
using
category
theory
as
unifying
language
facilitating
communication
between
different
scientific
disciplines
it
is
therefore
unsurprising
that
there
is
burgeoning
interest
in
applying
category
theory
to
machine
learning
we
aim
to
document
the
motivations
goals
and
common
themes
across
these
applications
we
touch
on
gradient-based
learning
probability
and
equivariant
learning
contents
introductio
gradient-based
learning
2.2
computing
the
gradient|
.
.
r.2.1
cartesian
differential
categories
paral
o
.
r.5.1
learners
and
symmetric
lenses
.
.
.
.
.
r.5.2
learners
languages
o
2.6
parameter
updates
and
learning
probability
and
statistic
bl
overview
|i
o1
ot
1d
marginalization
00
oo
.
20
shiebler
gavranovi
wilson
this
work
is
licensed
under
the
attribution|
license
brief
overview
of
category
theoretic
machine
learning
b.2.5
examples
of
markov
categories
b.2.6
cartesian
closedness
and
quasi-borel
spaces
.
.
.
23
4.2
functorial
unsupervised
learning|
.
.
.
.
.
29
u.2.1
functorial
clustering
.
.
.
.
.
.
.
30
-
32
u.2.3
flattening
hierarchical
clustering
.
34
u.2.4
multiparameter
hierarchical
clustering
34
.
55
36
37
39
introduction
compared
to
mathematics
or
physics
machine
learning
is
young
field
despite
its
young
age
it
has
experienced
sprawling
growth
with
its
applications
now
permeating
almost
every
realm
of
technology
dozen
of
its
subfields
have
become
entire
areas
of
study
in
their
own
right
this
includes
computational
learning
theory
deep
learning
bayesian
inference
normalizing
flows
clustering
reinforcement
learning
and
meta
learning
and
yet
this
explosive
growth
has
not
come
without
its
costs
as
the
field
keeps
growing
it
is
becoming
harder
and
harder
to
manage
its
complexity
and
to
understand
how
parts
of
this
immense
body
of
research
interact
with
each
other
while
different
subfields
of
machine
learning
share
the
same
intellectual
framework
it
hard
to
talk
across
boundaries
many
subfields
have
their
own
theoretical
underpinnings
best
practices
evaluation
measures
and
often
very
different
languages
and
ways
of
thinking
about
the
field
as
whole
furthermore
the
fast
moving
pace
of
the
field
is
giving
rise
to
bad
incentives
(
)
leading
to
papers
with
confounding
variables
missing
details
bad
notation
and
suffering
from
narrative
fallacy
and
research
debt
while
these
issues
are
in
general
not
exclusive
to
machine
learning
the
subfield
of
deep
learning
is
notoriously
ad-hoc
@
in
his
neurips
test
of
time
award
speech
(
ali
rahimi
has
compared
modern
machine
learning
to
alchemytl
citing
examples
of
machine
learning
models
performance
dropping
drastically
after
internals
of
frameworks
they
used
changed
the
default
way
of
rounding
numbers
models
in
deep
without
our
knowledge
of
modern
chemistry
chemistry
and
physics
alchemists
attempted
to
find
an
elixir
for
immortality
and
cure
any
disease
shiebler
gavranovié
wilson
reinforcement
learning
are
drtmuldrly
brittle
often
changing
their
performance
drastically
with
different
initial
seeds
in
general
the
construction
of
most
non-trivial
machine
learning
systems
is
largely
guided
by
heuristics
about
what
works
well
in
practice
while
the
individual
components
of
complex
models
are
generally
well-developed
mathematically
their
composition
and
combinations
tend
to
be
poorly
understood
the
machine
learning
community
is
well
aware
of
this
problem
some
researchers
have
begun
to
organize
workshops
focused
on
the
compositionality
of
machine
learning
components
&
)
others
have
called
for
broad
overarching
frameworks
to
unify
machine
learning
theory
and
practice
(
nonetheless
there
does
not
seem
to
be
widespread
consensus
about
how
exactly
achieve
that
goal
on
the
other
hand
the
field
of
category
theory
has
steadily
been
growing
it
is
becoming
unifying
force
in
mathematics
and
physics
spreading
in
recent
years
into
chemistry
statistics
game
theory
causality
and
database
theory
as
the
science
of
compositionality
it
helps
structure
thoughts
and
ideas
find
commonalities
between
different
branches
of
science
and
transfer
ideas
from
one
field
to
another
fong
and
spivak|
2018
lbraldleyi7
|2018
as
most
modern
machine
learning
systems
are
inherently
compositional
this
makes
it
un
surprising
that
number
of
authors
have
begun
to
study
them
through
the
lens
of
category
theory
in
this
survey
we
will
describe
category
theoretic
perspectives
on
three
areas
gradient-based
methods
building
from
the
foundations
of
automatic
differentiation
to
neural
network
architectures
loss
functions
and
model
updates
probabilistic
methods
building
from
the
foundations
of
probability
to
simple
bayesian
models
invariant
and
equivariant
learning
characterizing
the
invariances
and
equivariances
of
unsupervised
and
supervised
learning
algorithms
while
our
aim
is
to
provide
comprehensive
account
of
the
approaches
above
we
note
that
there
are
many
category
theoretic
perspectives
on
machine
learning
that
we
do
not
touch
on
for
example
we
leave
the
field
of
natural
language
processing
which
has
seen
recent
interest
from
the
categorical
community
rucke
notation
in
this
paper
we
write
functlon
composition
in
diagrammatic
order
using
when
it
comes
to
the
direction
of
string
diagrams
we
follow
the
notation
conventions
of
the
authors
therefore
in
the
first
section
the
string
diagrams
flow
from
left
to
right
while
in
the
second
section
they
flow
from
bottom
to
top
to
future
work
gradient-based
learning
2.1
overview
the
research
in
this
section
corresponds
mostly
to
deep
learning
starting
from
the
foundations
of
backpropagation
automatic
differentiation
and
moving
on
to
the
concept
of
neural
networks
gradient
descent
and
loss
function
where
updating
proceeds
in
an
iterative
fashion
2.1.1
applications
successes
and
motivation
models
based
on
deep
neural
networks
have
enjoyed
the
most
high-profile
successes
of
the
three
fields
we
discuss
for
example
in
reinforcement
learning
alphago
(
2017
achieved
brief
overview
of
category
theoretic
machine
learning
super-human
performance
in
playing
the
game
of
go
while
openail's
gpt-3
natural
language
model
is
able
to
generate
realistic
human-like
text
typical
examples
of
machine
learning
problems
addressable
with
the
methods
in
this
section
are
classification
and
regression
given
dataset
of
input/output
examples
a
b
learn
ing
function
f
mapping
inputs
to
outputs
classification
is
when
is
finite
set
regression
when
is
real-valued
generative
models
given
dataset
of
examples
learning
to
generate
new
samples
which
are
close
to
those
in
the
dataset
for
example
training
on
and
generating
images
of
faces
reinforcement
learning
problems
framed
as
an
agent
taking
actions
in
some
envi
ronment
for
example
the
simple
cart-pole
system
where
cart
the
agent
must
move
along
track
in
order
to
balance
pole
vertically
2.1.2
background
for
the
purposes
of
this
section
we
will
take
the
view
that
machine
learning
model
is
simply
morphism
f
p®
in
some
monoidal
category
c,®,i
with
an
object
representing
the
type
of
parameters
representing
some
observed
data
and
some
kind
of
prediction
for
example
if
our
goal
is
to
classify
28
28-pixel
images
into
two
classes
we
might
have
a=r?®2
and
b=[0,1
with
the
latter
representing
probability
training
such
model
consists
of
finding
specific
parameter
value
p
thereby
giving
trained
model
fy
b
string-diagramatically
we
can
write
this
as
follows
z\f—b
aqpf—b
untrained
model
trained
model
in
essence
the
parameters
serve
to
index
collection
of
maps
and
so
searching
for
map
f®id)s
f
a
reduces
to
searching
for
value
§
p
2.1.3
big
ideas
and
challenges
we
have
organised
the
research
in
this
section
into
three
areas
computing
the
gradient
gradient-based
optimization
is
ubiquitous
in
machine
learning
especially
neural
networks-so
computing
the
gradient
efficiently
is
key
in
this
section
we
discuss
categorical
approaches
to
this
computation
learning
with
lenses
lenses
are
specific
type
of
optics
which
provide
read
and
write
access
to
value
in
context
in
this
section
we
explore
how
lenses
can
capture
the
forward
predictive
and
backward
update
behaviours
of
learning
parameter
updates
and
learning
finally
we
discuss
how
lens-based
formalisms
for
learning
capture
the
various
machine
learning
algorithms
used
in
practice
2since
pixels
are
not
actually
real-valued
we
may
instead
use
f28%28
where
is
the
set
of
all
floating
point
numbers
shiebler
gavranovié
wilson
some
challenges
remain
however
for
example
current
categorifications
of
forward
and
reverse
derivatives
are
simply-typed
it
is
assumed
that
the
type
of
changes
is
the
same
as
the
type
of
values
addressing
this
would
allow
for
gradient-based
learning
in
categories
with
more
complex
structure
further
there
has
been
little
work
from
category-theoretic
perspective
on
convergence
properties
of
the
procedures
discussed
here
this
may
be
important
for
full
end-to-end
understanding
of
learning
2.2
computing
the
gradient
gradient
descent
is
ubiquitous
approach
for
training
machine
learning
models
where
one
views
learning
model
f
p®
as
iteratively
improving
some
initial
guess
of
parameters
6:1
in
order
to
minimise
some
choice
of
loss
function
the
gradient
of
this
loss
function
is
interpreted
as
the
direction
of
steepest
ascent
gradient
descent
makes
repeated
steps
in
the
negative
direction
of
the
gradient
to
converge
on
local
minimum
it
is
important
that
the
computation
of
the
gradient
is
efficient
since
it
must
be
recomputed
at
each
of
the
many
itererations
made
by
gradient
descent
one
of
the
first
incarnations
of
an
algorithm
for
efficiently
computing
the
gradient
of
loss
function
with
respect
to
some
parameters
is
backprop
originally
appearing
in
the
machine
learning
literature
7
the
first
examination
of
backpropagation
in
categorical
setting
is
the
seminal
paper
backprop
as
functor
fong
et
)
whose
title
alone
concisely
summarises
the
scope
of
this
section
however
since
this
paper
examines
the
end-to-end
learning
process
in
full
we
shall
leave
its
discussion
until
section
instead
we
begin
in
section
with
discussion
of
cartesian
differential
categories
which
categorify
the
notion
of
differential
operator
however
we
will
see
that
this
is
not
quite
what
we
need
to
train
machine
learning
model
v@adient
descent
for
this
we
need
reverse
differential
operator
which
we
discuss
in
section
2.2.1
cartesian
differential
categories
blute
et
al
seely
et
all
introduce
cartesian
differential
categories
which
are
defined
as
having
differential
combinator
which
sends
map
f
to
generalized
derivative
map
d[f
ax
a
b
the
authors
build
on
their
earlier
paper
seely
et
al
7
which
defines
differential
category
as
additive
symmetric
monoidal
category
equipped
with
differential
combinator
and
comonad
cartesian
differential
categories
are
introduced
to
explicitly
characterize
the
notion
of
an
infinitely
differentiable
category
cartesian
differential
categories
are
defined
in
terms
of
left-additive
structure
which
we
first
recall
definition
2.1
def
in
cockett
et
all
let
be
cartesian
monoidal
category
is
said
to
be
cartesian
left-additive
when
it
canonically
bears
the
structure
of
commutative
monoid
with
addition
+
axa
and
zero0,:1
diagrammatically
we
write
the
addition
and
zero
maps
as
follows
>—a
e
note
that
this
gives
way
to
add
maps
f,g:c(a,b
brief
overview
of
category
theoretic
machine
learning
f+y
aﬂ<
\/\h
f+0
a4<
>-7a
using
this
left-additive
structure
we
are
now
able
to
define
cartesian
differential
categories
definition
2.2
def
in
7
cartesian
differential
category
is
cartesian
left-additive
category
equipped
with
differential
combinator
which
assigns
to
each
map
f:a
in
map
d[f
ax
a
b
satisfying
the
equations
cdc.1
to
cdc.7
of
cockett
et
all
2019
definition
4
while
we
don’t
list
each
of
the
axioms
cdc.1
to
cdc.7
we
highlight
one
in
particular
cdc.5
this
axiom-the
chain
rule-defines
the
differential
operation
on
composite
maps
fi
h(dmjd[g
familiar
example
of
reverse
differential
category
is
smooth
the
category
of
euclidean
spaces
and
infinitely
differentiable
maps
between
them
example
example
in
(
)
for
map
f:r
r®
in
smooth
the
derivative
d[f
r
r
r
is
the
following
smooth
function
odr
odf
d[fl(z,2
jy(z)-2
adf
adf
o
d
ta
note
that
d[f
is
linear
in
its
first
argument
and
that
it
maps
vector
of
coefficients
r
and
vector
of
values
2/
r®
to
the
projection
of
x
along
the
jacobian
of
at
we
are
to
think
of
the
second
argument
2
as
vector
of
changes
so
that
d[f
is
the
following
linear
approximation
f@+a
f(a)+
dlfl(a.a
the
particular
notion
of
linearity
here
is
discussed
by
blute
et
al
(
md
where
map
is
defined
to
be
linear
if
d[f
=
f
this
is
generalization
of
the
idea
that
the
derivative
of
linear
map
is
the
map
itself
since
linear
maps
are
preserved
under
composition
and
tensor
any
cartesian
differential
category
contains
subcategory
of
linear
maps
shiebler
gavranovié
wilson
2.2.2
reverse
derivative
categories
although
cartesian
differential
categories
give
suitably
generalised
definition
of
the
derivative
they
do
not
provide
quite
what
we
need
for
gradient-based
learning
consider
the
following
supervised
learning
scenario
where
we
have
parametrised
model
f
px
a
training
example
a,b):1
ax
choice
of
parameters
6:1
intuitively
speaking
we
would
like
to
use
our
training
data
a,b
to
compute
some
new
parameter
such
that
f(6,a
is
better
approximation
of
than
f(6,a
the
derivative
of
gives
us
morphism
d[f
p
a
p
a
b
but
what
we
actually
want
is
morphism
of
type
px
a)x
this
is
precisely
the
type
of
the
reverse
derivative
combinator
introduced
by
cockett
et
al
|
ockett
et
al
definition
2.3
def
13
in
m
reverse
derivative
category
is
cartesian
left-additive
category
equipped
with
reverse
derivative
combinator
which
assigns
to
each
ma
a
in
c
map
r[f
ax
a
satisfying
the
equations
rdc.1
to
rdc.7
of
corkett
et
al
deﬁmtwn
13
we
again
leave
full
hstmg
of
the
axioms
to
the
original
paper
7
7
but
highlight
rdc.5
the
reverse
chain
rule
figure
<
r[fl
tn
rlg
figure
1
reverse
chain
rule
in
graphical
form
intuitively
this
axiom
captures
the
of
backpropagation
given
point
a
output
changes
flow
backwards
through
r[g
and
then
r[f
to
compute
change
in
the
initial
inputs
a
returning
to
our
previous
example
smooth
also
forms
reverse
derivative
category
example
2
for
map
f:r
r
in
smooth
the
reverse
derivative
r[f
r
r®
r®
is
the
following
smooth
function
r[f)(z,y
jp(x
o/
by
analogy
with
the
differential
combinator
d
note
that
r[f
is
also
linear
in
its
first
argument
and
that
we
are
to
think
of
the
second
argument
as
vector
of
changes
so
that
r[f
gives
the
following
linear
approximation
f@)+y
f(z+r[f](z,y
brief
overview
of
category
theoretic
machine
learning
2.2.3
automatic
differentiation
in
the
simple
essence
of
automatic
differentiation
conal
elliott
takes
different
perspective
whereas
fong
et
al
fong
et
al
2019
first
construct
framework
for
transmitting
information
through
update
and
request
functions
and
then
demonstrate
that
the
backpropagation
algorithm
fits
into
that
framework
elliott
generalizes
the
automatic
differen
tiation
algorithm
itself
by
replacing
linear
maps
with
an
arbitrary
cartesian
category
elliott
7
writes
from
functional
programming
perspective
and
the
paper’s
generalization
of
automatic
differentiation
is
essentially
program
specification
the
central
construction
in
the
paper
is
framework
for
differentiable
functional
programming
that
relies
on
typed
higher-order
partial
function
that
tracks
and
computes
derivatives
of
programs
and
elliott
defines
efficient
implementations
of
derivatives
by
using
program
transformation
strategy
burstall
and
darlington
to
be
specific
elliott
defines
the
derivative
to
be
operator
of
the
form
d:(a—b
a—(a—b
that
satisfies
the
chain
rule
d(f3g)a
=dfa3dg(fa
the
cross
rule
d(f
g)(a,b
=dfax
dgb
the
linear
rule
for
all
linear
functions
f
dfa
all
of
these
theorems
are
satisfied
by
the
classical
notion
of
derivatives
and
elliott
demon
strates
the
generality
of
his
construction
by
relying
only
on
these
properties
in
order
to
enable
the
composition
of
derivatives
he
also
defines
the
operator
d*f=(f,df
which
acts
as
an
endofunctor
over
category
of
differentiable
functions
from
the
perspective
of
elliott’s
construction
the
difference
between
reverse-mode
and
forward-mode
automatic
differentiation
reduces
to
the
difference
between
left/right
associated
compositions
of
derivatives
this
allows
elliott
to
define
an
implementation
of
reverse-mode
differentiation
from
continuation-passing
style
7
rather
than
dealing
with
gra
dient
tape
furthermore
because
the
differentiation
operator
decorates
the
inference
function
elliott’s
construction
(
explicitly
specifies
how
computation
between
the
forward
pass
and
backward
pass
are
shared
in
reverse
mode
automatic
differentiation
this
is
key
aspect
of
why
reverse
mode
automatic
differentiation
is
particularly
efficient
for
gradient-based
learning
we
note
that
the
framework
of
differential
and
reverse
differential
categories
introduced
in
the
previous
subsection
can
be
seen
as
categorical
formalization
of
elliott’s
work
if
one
restricts
to
the
non-closed
setting
in
other
words
by
taking
the
codomain
a
b
of
elliott’s
operator
under
the
tensor-hom
adjunction
we
obtain
b
which
agrees
with
the
differential
operator
of
cartesian
differential
categories
2.3
optics
and
lenses
the
story
of
cartesian
reverse
differential
categories
and
automatic
differentiation
lends
itself
naturally
into
the
story
of
lenses
and
more
generally
optics
optics
are
general
construction
shiebler
gavranovié
wilson
which
can
be
thought
of
as
pair
of
processes
that
move
in
opposite
directions
for
example
we
might
think
of
the
map
f
of
neural
network
as
forward
and
its
reverse
derivative
r[f
pxax
xa
as
backwards™-in
the
same
sense
as
backpropagation
but
more
importantly
optics
formalize
more
than
just
the
flow
of
derivatives
and
capture
wide
array
of
data
accessor
patterns
7
md
as
well
as
bayesian
inversion
section
definition
2.4
def
2.0.1
in
(
let
be
symmetric
monoidal
category
for
any
two
pairs
s,s
and
a
a
of
objects
in
c
an
optic
p
2
2
is
an
element
of
the
following
set
mec
c(s,m®a)x
c(mea",s
explicitly
an
optic
is
an
equivalence
class
of
triples
m,l:s—>mar:ma
5
such
that
for
any
m
m
and
f
m
m
in
c
we
have
m
15
f
®@id
a
my
1
f
®id,)57
by
specializing
to
cartesian
symmetric
monoidal
category
we
can
obtain
concrete
description
of
these
optics
without
the
coend
prop
2.0.4
these
constructions
are
called
lenses
clarke
et
al
020|
ohannon
et
al
2008
roman
2021
hedges
2018
we
present
definition
of
so-called
simple
lenses
whose
forward
and
backward
types
are
the
same
definition
2.5
def
in
7
)
lens
a
is
pair
f,f
of
maps
f
a
and
f#:axb
one
can
think
of
lenses
as
actually
having
the
type
a
a
b
b
where
information
first
flows
from
the
top
to
the
top
using
the
map
f
the
environment
then
uses
the
top
to
compute
and
updated
b
together
with
the
original
a
this
is
used
by
the
map
f
to
produce
an
updated
a
in
fact
this
original
used
in
the
backward
pass
is
exactly
the
residual
when
we
think
of
this
lens
as
an
optic
in
this
survey
we
will
write
object
simply
as
a
and
not
as
tuples
while
keeping
in
mind
that
each
object
actually
is
used
both
as
an
input
and
an
output
lens
composition
f
f*)3(g,g
is
given
by
the
map
f$g
in
the
first
component
and
=
sff
~
in
the
second
we
note
here
the
similarity
to
axiom
rdc.5
of
reverse
differential
categories
figure
whose
reverse
derivatives
compose
in
the
same
way
this
is
one
of
the
main
insights
of
(
they
show
that
for
any
reverse
differential
category
there
is
canonical
embedding
into
lenses
in
other
words
each
reverse
differential
function
can
be
augmented
with
its
reverse
derivative
in
way
which
respects
lens
composition
10
brief
overview
of
category
theoretic
machine
learning
proposition
prop
2.12
in
cruttwell
et
al
)
let
be
reverse
derivative
category
then
there
is
canonical
product-preserving
identity-on-objects
functor
r:c
lens(c
which
sends
map
to
the
pair
f,r[f
this
is
important
since
it
shows
us
that
reverse
derivative
categories
cockett
et
al
md
naturally
fit
into
the
story
of
lenses
this
connects
two
disparate
fields
of
categorical
differenti
ation
and
bidirectional
processes
under
the
common
language
of
lenses
2.4
para
while
the
previous
section
described
categorical
foundations
behind
backprop
nothing
was
said
about
learning
itself
in
addition
to
the
forward-backward
interaction
of
derivatives
in
machine
learning
algorithm
behind
the
scenes
there
is
yet
another
forward-backward
interaction
influencing
the
learning
process
this
where
the
story
of
parameters
of
machine
learning
model
come
in
originally
described
in
fong
et
all
)
the
construction
para
makes
it
precise
what
is
meant
by
parameterization
here
we
state
the
more
general
version
described
in
t
al
021
definition
2.6
let
be
symmetric
monoidal
category
then
para(c
is
bicategory
with
the
same
objects
as
c
morphism
in
para(c
is
pair
p
f
where
is
an
object
of
and
f:p®
a
2-cell
from
p
f
to
p
f
is
morphism
inr
p
in
such
that
the
following
diagram
commutes
in
c
ppoa
pga
ts
the
composition
of
1-cells
p.f
q.9
a—b———c
i.e
of
prasb
and
q@b
is
given
by
q®p.aq
43(q®
39
aq
p
qrf
qep)®a
qe
pod
qeb
consider
1-cell
p
f
in
the
bicategory
para(€
this
is
map
from
to
with
an
extra
input
that
has
to
be
accounted
for
these
inputs
can
be
thought
of
as
the
private
knowledge
of
the
morphism
and
note
that
they
make
sense
in
any
monoidal
category
€©
the
composition
of
two
parameterized
maps
f
p®
and
g
collects
the
parameters
into
the
monoidal
product
as
shown
in
the
figure
below
shiebler
gavranovié
wilson
11
l9—c
the
authors
show
para
is
also
natural
with
respect
to
base
change
this
becomes
important
when
augmenting
differentiable
map
with
its
derivative
prop
and
)
proposition
prop
2.3
in
cruttwell
et
al
)
let
and
be
symmetric
monoidal
categories
and
lax
symmetric
monoidal
functor
then
there
is
an
induced
lax
functor
para(f
para(c
para(d
the
para
construction
captures
the
idea
that
the
inputs
of
machine
learning
model
f
p®
are
the
values
to
be
learned
in
the
next
subsection
we
describe
how
the
authors
in
cruttwell
et
al
2021
fong
et
al
describe
these
machine
learning
models
2.5
learners
the
story
of
machine
learning
models
starts
with
the
seminal
paper
backprop
as
functor
of
fong
spivak
and
tuyeras
fong
et
al
they
call
machine
learning
models
learners
and
give
concrete
description
instantiated
in
the
category
set
however
this
definition
is
not
based
on
any
of
the
categorical
constructions
related
to
differentiation
outlined
in
the
previous
section
cruttwell
et
al
cruttwell
et
al
2021
solve
this
problem
by
defining
learners
as
high-level
categorical
construct
inyolving
para
lenses
and
reverse
derivative
categories
subsuming
the
definition
of
fong
et
ale
this
is
the
perspective
we
take
in
this
text
taking
detour
to
fong’s
learners
as
needed
definition
2.7
lemma
2.13
in
cruttwell
et
all
the
bicategory
of
learners
is
the
bicategory
para(lens(c
consisting
of
the
following
data
objects
in
para(lens(c
are
the
same
as
those
of
lens(c
i-cell
a
is
bidirectional
parameterized
lens
tuple
p
f,f
consisting
of
choice
of
parameter
and
lens
f,f
px
a
b
2-cell
between
p
f,f
and
q,g,9%
is
reparameterization
lens
r,7
satisfying
the
commuting
triangle
condition
from
definition
|2.6
the
1-cell
in
this
bicategory
are
the
learners
where
we
think
of
the
parameters
as
the
input
to
this
machine
learning
model
and
as
the
output
unpacking
this
even
further
we
see
that
1-cell
consists
of
two
parts
f:pxa—b
ff1pxaxb—pxa
the
map
f
px
is
the
map
propagates
values
forward
it
takes
takes
as
input
parameter
value
p
datapoint
a
and
computes
the
prediction
b
the
map
f
performs
propagates
errors
backward
for
value
of
p
and
it
computes
an
error
for
the
parameter
3modulo
difference
in
2-cells
see
_
sec
6
12
brief
overview
of
category
theoretic
machine
learning
and
an
error
for
the
parameter
a
which
we
think
of
as
the
backpropagated
error
used
by
the
previous
learner
this
is
the
required
component
to
make
composition
of
learners
well
defined
originally
seen
as
the
necessary
but
slightly
more
mysterious
comionent
of
fong
et
al
)
here
it
falls
out
of
the
definition
of
lens
composition
cruttwell
et
al}
2021
def
2.7
this
is
because
the
composition
in
para(lens(€
is
defined
in
terms
of
the
composition
in
the
base
category
which
is
in
this
case
the
category
lens(()c
2-cell
in
this
bicategory
relates
two
learners
with
different
parameter
sets
according
to
the
reparameterization
rule
in
def
which
here
unpack
to
lens
composition
the
authors
in
7
show
that
variety
of
optimizers
in
machine
learning
including
gradient
descent
momentum
and
more
can
be
seen
as
2-cells
in
this
category
using
the
fact
that
para
is
natural
with
respect
to
base
change
prop
e
cruttwell
et
al
sec
3.1
show
that
augmenting
reverse
derivative
map
prop
l
lifts
coherently
to
the
parameterized
case
via
the
functor
para(r
para(c
para(lens(c
whose
unpacking
we
leave
to
the
original
paper
we
proceed
to
describe
para(lens(€
in
more
detail
namely
we
unpack
the
composition
of
1-cells
by
showing
how
it
generalizes
the
category
of
learners
of
fong
et
al
fong
et
al
plur
definition
2.8
def
il.1
in
7
)
learner
is
tuple
a
where
is
set
the
parameter
space
and
i
u
and
are
functions
with
types
i:pxa—b
u:pxaxb—p
r:pxaxb
the
maps
i
u
and
are
called
the
implementation
update
and
request
maps
respectively
it
is
easy
to
see
that
in
this
case
the
implementation
map
corresponds
to
map
in
para(lens(c
and
that
the
update
and
request
map
can
be
joined
into
the
map
ur
ax
a
yielding
the
same
backwards
f
map
from
cruttwell
et
al’s
para(lens(c
construction
the
learners
of
fong
et
al
fong
et
all
form
category
learn
where
objects
are
sets
morphisms
are
learners
and
the
composition
of
the
learners
p,i,u,r
and
q,j,v,s
is
pr®q,i15j,u3v
s3r
where
i3j)(p,q,a
j(q,1(p,a
usv)(p,q,a
u(p,a,5(q,1(p,a),¢c
v(g
i(p,a),c
s37)(p,q,a
=7(p,a,5(¢,i(p,a),c
this
concrete
definition
of
morphism
composition
in
learn
is
in
fact
the
first
categorical
description
of
backpropagation
and
one
of
the
main
ideas
of
fong
et
al
fong
et
al
2.5.1
learners
and
symmetric
lenses
the
connection
of
learners
to
symmetric
lenses
is
examined
in
follow-up
paper
by
the
by
fong
and
johnson
fong
and
johnson
representative
of
symmetric
lens
from
to
shiebler
gavranovié
wilson
13
p1,91
p2:92
bis
span
«
s|
—
where
p;,9
and
p,,g
are
asymmetric
lenses
symmetric
p1,91
p2:92
p1,97
p3,92
lenses
compose
via
pullback
the
composition
of
¢
s
—
and
b+
s5
—
is
the
symmetric
lens
91.94)5(p1.91
p2,92)3(p%,9%
pl9l
banda
where
s|
¢
—
s
is
the
pullback
in
set
of
the
cospan
p2,92
1,97
s
b+
5
the
authors
demonstrate
that
we
can
define
faithful
identity-on-objects
symmetric
monoidal
functor
from
learn
to
the
category
of
symmetric
lenses
slens
this
functor
maps
the
learner
p,1,u,r
—
to
the
symmetric
lens
k
u,r).i
a+—pxa
where
k
is
the
constant
complement
lens
bancilhon
and
spyratos
1981
2.5.2
learners
languages
spivak
(
2020b
takes
an
interesting
approach
by
making
connection
between
learn
and
the
world
of
poly
category
of
polynomial
functors
in
one
variable
using
the
fact
that
poly
is
monoidal
closed
this
allows
spivak
to
interpret
learners
as
dynamical
systems
through
the
language
of
coalgeras
of
polynomials
they
show
that
the
space
of
learners
between
objects
and
forms
topos
and
consider
logical
propositions
that
can
be
stated
in
its
internal
language
opening
up
avenues
to
specification
of
what
learner
is
doing
internal
to
the
language
of
learner
in
the
pure
categorical
sense
2.6
parameter
updates
and
learning
machine
learning
in
the
wild
consists
of
diverse
set
of
techniques
for
training
models
in
this
section
we
examine
how
category
theory
has
been
applied
to
give
theoretical
foundation
for
some
of
these
techniques
delayed
trace
and
backpropagation-through-time
we
begin
with
the
work
of
sprunger
et
al
)
which
answers
the
question
of
how
to
train
models
with
notion
of
state
more
concretely
suppose
instead
of
training
model
f
px
from
examples
y
we
instead
wish
to
learn
from
time-series
data
where
we
have
sequences
of
training
points
list(x
y
the
idea
of
recurrent
neural
networks
is
essentially
to
model
such
sequences
by
using
stateful
model
14
brief
overview
of
category
theoretic
machine
learning
with
the
model
inputs
the
predictions
and
the
state
the
morphism
in
the
diagram
above
is
called
delay
gate
it
can
be
thought
of
as
delaying
its
input
until
the
next
iteration
of
the
model
the
authors
construct
category
of
these
stateful
computations
in
which
the
cartesian
differential
operator
shares
the
same
structure
as
the
backpropagation
through
time
algorithm
that
is
used
to
train
recurrent
neural
networks
given
strict
cartesian
category
the
authors
construct
double
category
dbi(c
in
which
1-cells
are
the
objects
in
and
2-cells
operate
as
stateful
morphisms
by
vertically
composing
these
2-cells
the
authors
construct
stateful
morphism
sequences
they
use
these
sequences
to
define
category
st(c
in
which
objects
are
n-indexed
families
of
c-objects
and
morphisms
are
equivalence
classes
of
stateful
morphism
sequences
reverse
derivative
ascent
wilson
and
zanasi
exploit
the
gener
ality
of
the
categorical
reverse
derivative
section
2.2.2
to
define
an
analogue
of
gradient-based
methods
although
their
procedure
has
general
applications
they
focus
on
the
special
case
of
learning
boolean
circuits
for
model
f
b
they
give
the
following
map
to
update
the
model’s
parameters
although
not
explicitly
mentioned
the
authors
make
use
of
lenses
in
their
accompanying
implementation
where
model
is
in
fact
simple
lens
as
in
definition
update
displacement
and
functoriality
cruttwell
et
al
(
define
framework
for
categorical
gradient-based
learning
which
subsumes
reverse
derivative
ascent
as
well
as
number
of
variants
of
gradient
descent
algorithms
rudeﬂ
including
stateful
variants
like
momentum
gradient
descent
by
way
of
comparison
to
rda
the
authors
define
more
general
update
step
as
follows
s(p
s(p
shiebler
gavranovié
wilson
15
while
the
model
f
and
reverse
derivative
r[f
components
remain
in
common
their
approach
generalises
update
maps
up,u}
defining
how
to
update
parameters
given
the
gradient
displacement
maps
d
defining
the
error
or
distance
between
model
prediction
and
the
true
label
their
approach
also
gives
conditions
under
which
the
mapping
of
morphisms
into
this
cate
gory
of
lenses
is
functorial
namely
that
there
must
exist
an
inverse
displacement
map
d;‘l
probability
and
statistics
3.1
overview
the
research
in
this
section
focuses
on
understanding
how
randomness
and
probability
can
be
characterized
and
implemented
in
machine
learning
this
area
called
probabilistic
machine
learning
includes
studying
the
random
nature
of
the
relationship
of
data
we
are
trying
to
model
but
also
the
random
nature
of
more
concrete
processes
such
as
data
sampling
in
our
iterative
learning
algorithms
unlike
the
setting
of
neural
networks
where
the
learning
is
being
done
on
categories
with
appropriate
differential
structure
here
the
learning
is
being
done
on
categories
with
appropriate
probabilistic
structure
3.1.1
applications
successes
and
motivation
the
use
of
category
theory
in
probabilistic
machine
learning
can
be
divided
two
areas
the
first
one
attempts
to
formalize
and
treat
the
notion
of
random
variable
as
fundamental
one
equivalent
to
notions
such
as
space
group
or
function
the
second
one
attempts
to
use
this
formalization
to
describe
how
learning
works
in
categorical
setting
the
former
has
been
extensively
studied
going
back
to
lawvere
lawvere
1962
and
spanning
dozens
of
papers
)
the
latter
is
less
popular
but
has
seen
number
of
papers
in
recent
years
studying
causaht
ong|
conjugate
priors
learning
culbertson
and
sturt
in
this
paper
we
do
not
aim
to
examine
all
papers
related
to
probability
theory
and
cate
gory
theory
but
are
instead
focusing
on
the
ones
providing
general
principles
most
relevant
to
learning
and
general
aspects
of
bayesian
synthetic
probability
theory
the
systemization
of
basic
concepts
from
probability
theory
into
an
axiomatic
framework
this
enables
understanding
of
joint
distributions
marginalization
conditioning
bayesian
inverses
and
more
(
0
fritz
et
al
ho
and
jacobs
purely
in
terms
of
interaction
of
morphisms
probablllstlc
prog
ramml
the
iildillpllldthil
and
study
of
programs
which
involve
randomness
eunen
et
al|
01
probabilistic
machine
learnlng
the
study
of
updating
distribution
with
samples
from
dataset
and
reasoning
about
uncertamty
arising
from
noisy
measurements
h
iﬂl
sertson
and
sturtz
16
brief
overview
of
category
theoretic
machine
learning
3.1.2
background
given
fixed
dataset
most
machine
learning
problems
reduce
to
optimization
problems
how
ever
solving
machine
learning
problem
effectively
requires
reasoning
about
the
source
and
limitations
of
the
dataset
essentially
there
are
two
sources
of
uncertainty
that
separate
ma
chine
learning
problems
from
optimization
problems
more
generally
the
first
is
epistemic
uncertainty
or
uncertainty
that
is
due
to
hidden
information
that
is
information
that
is
not
available
to
us
in
the
process
of
scientific
modeling
but
is
available
in
principle
we
can
get
more
precise
data
thus
reducing
epistemic
uncertainty
but
often
we
also
experience
experiment-dependent
uncertainty
unknowns
that
differ
each
time
we
run
the
same
experiment
this
is
called
aleatoric
uncertainty
and
it
represents
inherent
uncertainty
and
variability
in
what
we
are
trying
to
model
consider
the
case
when
we
have
zero
aleatoric
uncertainty
in
the
problem
we
are
trying
to
model
then
by
reducing
epistemic
uncertainty
by
collecting
more
data
for
instance
we
can
essentially
reduce
our
machine
learning
problem
to
an
optimization
one
but
if
our
problem
contains
aleatoric
uncertainty
then
collecting
more
data
has
diminishing
returns
we
can
never
perfectly
deterministically
model
relationships
that
possesses
inherent
variability
in
most
practical
applications
we
have
some
form
of
aleatoric
uncertainty
and
this
is
what
probability
and
bayesian
inference
are
modelling
they
shift
the
supervised
learning
from
function
approximation
problem
to
distribution
approximation
problem
in
the
learning
theory
literature
this
is
known
as
agnostic
learning
3.1.3
big
ideas
and
challenges
the
main
idea
of
this
section
is
that
many
machine
learning
algorithms
contain
an
irreducible
as
pect
of
randomness
that
we
can
use
recent
categorical
advances
to
reason
about
internal
to
some
category
these
advances
can
help
us
understand
the
high-level
picture
of
what
probabilistic
learning
is
and
elucidate
its
connections
to
other
fields
nonetheless
there
are
many
things
still
to
do
this
includes
more
complete
synthetic
framework
for
reasoning
about
probability
making
sense
for
instance
in
the
enriched
setting
likewise
there
seem
to
be
two
disjoint
trends
of
research
markov
categories
and
quasi-borel
spaces
understanding
how
these
interact
is
something
that
has
not
been
explored
in
the
liter
ature
more
precise
characterizations
about
bayesian
machine
learning
in
synthetic
setting
is
also
something
that
seems
to
be
missing
from
the
literature
as
well
as
closer
connections
to
the
other
streams
of
research
in
this
survey
sections
and
hf
3.2
categorical
probability
the
field
of
categorical
probability
aims
to
reformulate
core
components
of
probability
theory
on
top
of
category
theoretic
constructions
this
includes
things
such
as
composition
of
probabilistic
maps
formation
of
joint
probability
distributions
marginalization
disintegration
independence
and
conditionals
several
authors
including
cho
and
jacobs
(
and
fritz
et
al
(
claim
that
categories
with
similar
monoidal
and
comonoidal
structures
serve
as
the
right
abstraction
for
reasoning
about
probability
each
of
these
authors
introduce
frameworks
within
which
we
can
describe
common
manipulations
of
joint
probablhty
dlstnbutlons
in
terms
of
category
theoretic
operations
on
the
other
hand
heunen
et
al
shiebler
gavranovié
wilson
17
define
the
category
of
quasi-borel
spaces
in
the
concrete
setting
of
probabilistic
programming
we
will
see
that
quasi-borel
spaces
are
an
instance
of
markov
categories
even
though
it
appears
that
both
heunen’s
quasi-borel
spaces
and
fritz
are
disjoint
trends
in
work
these
constructions
allow
us
to
reason
about
complex
probabilistic
relationships
in
terms
of
algebraic
axioms
rather
than
low
level
analytical
machinery
in
this
paper
we
will
mostly
be
taking
the
vantage
point
of
fritz
markov
categories
(
which
generalize
number
of
existing
constructions
and
possess
many
but
not
all
core
concepts
of
probability
theory
definition
3.1
definition
2.1
in
(
)
markov
category
is
symmetric
monoidal
category
c,®,1
in
which
every
object
is
equipped
with
the
commutative
comonoid
structure
a
comultiplication
map
cp
and
counit
map
del
1
satisfying
coherence
laws
with
the
monoidal
structure
and
where
additionally
the
del
map
is
natural
with
respect
to
every
morphism
f
this
seemingly
opaque
construction
will
be
dissected
in
the
following
subsections
we
will
see
how
each
role
plays
an
important
part
which
is
directly
related
to
some
aspect
of
the
concept
of
randomness
the
naturality
of
del
makes
the
monoidal
unit
terminal
and
markov
categories
semicartesian
categories
this
is
in
contrast
with
cd-categories
(
which
do
not
have
this
requirement
in
other
words
cd-categories
are
slightly
more
general
and
include
markov
categories
as
affine
cd-categories
the
naturality
of
del
tells
us
that
there
is
only
one
way
to
delete
information
in
markov
categories
but
observe
that
the
cp
map
is
not
necessarily
natural
with
respect
to
every
morphism
in
markov
category
we
remark
on
this
in
section
where
the
naturality
of
morphism
with
respect
to
cp
implies
the
morphism
is
deterministic
3.2.1
distributions
channels
joint
distributions
and
marginalization
markov
categories
are
synthetic
framework
for
reasoning
about
random
mappings
this
can
most
readily
be
seen
in
the
idea
that
morphism
p
from
the
monoidal
unit
can
be
thought
of
as
distribution
p(z
over
some
object
x
or
simply
as
random
element
of
x
to
understand
how
this
map
truly
acts
as
distribution
or
random
element
we
need
to
understand
how
it
interacts
with
the
rest
of
the
markov
category
structure
general
morphism
f
in
markov
category
is
often
called
channel
the
sugges
tive
notation
f(y|z
is
often
used
denoting
that
the
probability
distribution
on
is
dependent
on
x
the
composition
of
and
yields
morphism
ps§
f
which
can
be
interpreted
causally
first
random
element
of
is
generated
which
is
subsequently
fed
into
f
outputting
an
element
of
y
this
allows
us
to
interpret
p§
as
random
element
of
y
morphism
from
the
monoidal
unit
into
the
tensor
of
two
objects
h
®y
canonically
gives
us
the
notion
of
joint
distribution
over
and
y
classically
denoted
as
h(x,y
this
joint
distribution
can
then
be
marginalized
over
to
produce
distribution
over
x
this
done
with
the
help
of
the
delete
map
i.e
by
composition
of
with
idy
®dely
x®y
x
while
composing
the
joint
distribution
individually
with
the
two
projections
dely
®y
and
dely
x®y
does
give
us
two
marginal
distributions
and
y
this
is
process
that
cannot
generally
be
inverted
this
makes
intuitive
sense
the
space
of
joint
4equivalently
markov
category
is
semicartesian
category
where
every
object
is
coherently
equipped
with
the
copy
map
18
brief
overview
of
category
theoretic
machine
learning
distributions
contains
more
information
than
the
product
of
their
marginals
and
there
is
no
bijective
correspondence
between
€(i,x®y
and
€(i,x
c(1,y
this
1s
closely
related
to
the
notion
of
deterministic
morphism
and
also
central
to
the
iol
work
of
cho
and
jdcob
and
coecke
and
spekken
3.2.2
deterministic
morphisms
the
idea
that
morphism
in
markov
category
is
like
generalized
stochastic
map
can
be
seen
in
the
idea
that
we
can
specify
what
it
means
for
morphism
to
be
deterministic
simply
by
saying
how
it
interacts
with
other
morphisms
in
this
markov
category
definition
3.2
def
10.1
in
7
)
morphism
f
=y
in
markov
category
is
deterministic
if
it
is
cp
homomorphism
an
intuitive
way
to
see
how
not
respecting
the
cp
structure
gives
rise
to
randomness
is
to
think
of
like
rolling
the
dice
the
process
of
rolling
the
dice
and
then
copying
the
number
we
see
on
the
top
is
not
the
same
process
as
rolling
two
dice
the
satisfaction
of
this
constraint
would
make
into
comonoid
homomorphism
since
del
is
already
natural
in
markov
category
when
every
morphism
is
deterministic
in
markov
category
the
markov
category
becomes
cartesian
however
most
markov
categories
are
not
cartesian
and
this
lack
of
determinism
gives
markov
categories
their
rich
structure
3.2.3
conditional
probabilities
given
joint
distribution
p(z,y
we
are
often
interested
in
computing
the
probability
of
y
given
that
occurred
in
other
words
when
conditioning
on
we
are
are
computing
the
channel
y\
such
that
it
agrees
with
the
information
in
p(z,y
this
is
usually
written
as
p(z,y
x
although
special
care
has
to
be
taken
when
interpreting
that
notation
on
the
nose
|
nt
2.8
notation
condlnondls
which
in
cho
and
jacobs
cho
and
jdcob
are
referred
to
as
admitting
disintegration
can
be
formulated
in
markov
category
although
not
every
markov
category
has
conditionals
table
definition
3.3
def
2.3
in
given
f
a
xqy
in
€
morphism
shiebler
gavranovié
wilson
19
fix
xq®a—y
inc
is
called
conditional
of
with
respect
to
if
the
equation
xy
fix
xy
\]‘c\
70
holds
we
say
that
has
conditionals
provided
that
such
conditional
exists
for
all
a
x,y
and
for
all
f
a
xqy
in
c
the
equation
e|
can
be
written
with
classical
notation
as
f(z,yla
fix(y|z,a)f(z]a
in
markov
category
with
all
conditionals
we
can
form
an
analogous
conditional
ﬁy(x,y\rz
and
write
bayes
theorem
in
the
general
form
fixwlz,a)f(zla
fiy
zly,a
f(yla
where
the
special
case
=1
has
been
treated
by
cho
and
jacobs
cho_and
jacobs
even
though
conditionals
appear
to
be
valuable
in
practice
section
fritz
notes
that
conditioning
did
not
end
up
being
relevant
notion
for
the
rest
of
the
paper
on
markov
categories
remark
11.4
nonetheless
conditionals
enable
fritz
to
define
general
synthetic
definition
of
what
it
means
to
be
bayesian
inverse
definition
3.4
def
2.5
in
7
)
given
two
morphismsm:i
and
f
a
x
bayesian
inverse
of
with
respect
to
the
prior
is
conditional
of
®3
when
the
choice
of
the
prior
m
is
clear
from
the
context
fritz
et
al
m
denotes
bayesian
inverse
of
simply
by
ft
a
that
is
bayesian
inverse
is
defined
to
be
the
morphism
satisfying
the
equation
jacobs
(
introduces
categorical
formulation
of
conjugate
priors
this
allows
them
to
study
the
closure
properties
of
bayesian
inversions
and
answer
the
question
when
is
the
posterior
distribution
in
the
same
class
of
distributions
as
the
prior
one
20
brief
overview
of
category
theoretic
machine
learning
3.2.4
independence
one
of
the
most
important
concepts
in
probability
theory
is
independence
concept
describ
ing
situations
where
an
observation
is
irrelevant
or
redundant
when
evaluating
hypothesis
typically
independence
is
defined
in
terms
of
random
variables
or
measurable
functions
out
of
probab
lit
spdce
but
independence
can
be
defined
abstractly
in
any
markov
category
fritz
nt
starts
to
define
it
in
markov
category
by
showing
there
are
two
equivalent
chamcterlzatlons
lemma
lemma
12.11
in
(
)
given
morphism
f
a
qy
the
following
are
equivalent
f(z,yla
f(z|a)f(yla
meaning
that
there
are
g
a
and
h
y
such
that
the
definition
is
now
straightforward
definition
3.5
def
12.12
in
let
be
markov
category
if
morphism
f:a—=
xqy
satisfies
the
equivalent
conditions
in
lemma
|1
then
we
say
that
displays
conditional
independence
ly
||
a
this
tells
us
that
we
get
the
same
result
if
we
i
generate
from
and
ii
independently
generate
both
and
from
and
then
tensor
them
together
franz
(
definition
3.4
proposes
similar
definition
of
independence
for
any
semicartesian
monoidal
category
€
consider
the
objects
a
b,c
and
morphisms
x
b,x,:a
in
€
the
morphisms
x;,x
are
independent
if
there
exists
some
morphism
h:a
bq®c
such
that
the
following
diagram
commutes
note
that
7,7
are
the
projections
of
the
tensor
product
careful
comparison
of
how
these
two
definitions
are
related
is
given
in
it
o2
p
72
xl
b¢—p
brc
shiebler
gavranovié
wilson
21
while
conditionals
section
are
often
used
to
define
independence
it
can
stlll
be
meaningful
to
introduce
and
work
with
independence
in
the
absence
of
conditionals
|
ch
12
as
we’ve
done
here
3.2.5
examples
of
markov
categories
there
are
large
number
of
examples
of
markov
categories
and
it
is
possible
to
organize
them
in
many
families
firstly
every
cartesian
category
is
markov
category
in
trivial
way
in
essence
containing
only
the
deterministic
markov
morphisms
but
many
interesting
markov
categories
are
not
cartesian
and
contain
many
non-deterministic
morphisms
two
examples
are
gauss
the
category
of
gaussian
conditionals
and
finstoch
the
category
of
markov
kernels
between
finite
sets
these
are
some
of
the
markov
categories
that
do
not
arise
as
kleisli
categories
the
most
common
way
to
construct
markov
categories
is
as
kleisli
categories
of
various
affine
monoidal
monads
on
various
base
categories
the
converse
of
this
result
is
only
partially
understood
and
is
described
in
(
sec
3.2
proposition
prop
3.1
in
fritz
et
al
)
let
be
category
with
finite
products
let
be
commutative
monad
on
with
d(1
1
then
the
kleisli
category
ki(d
is
markov
category
he
distribution
monad
thoroughly
explored
in
nce
the
simplest
example
is
that
of
set
with
)
see
table
|l
for
more
examples
base
category
monad
z(hd
has
all
conditionals
refere
set
distribution
meas
giry
stoch
finmeas
giry
finstoch
cgmeas
giry
cgstoch
pol
giry
borelstoch
qbs
6
chaus
radon
6
frit7
2009
definition
3.3
table
1
bird’s-eye
view
of
some
markov
categories
that
arise
as
kleisli
categories
of
various
monads
one
of
the
most
common
and
widely
used
examples
is
stoch
and
many
of
its
subcategories
the
category
stoch
naturally
arises
as
the
kleisli
category
of
the
giry
monad
which
is
monad
on
the
category
meas
of
measurable
spaces
definition
3.6
measurable
spaces
and
measurable
functions
form
category
meas
the
objects
in
meas
are
pairs
a
4
where
¥
is
o-algebra
over
a
mmph[sm
from
a
4
to
b,xp
in
meas
is
measurable
function
such
that
for
an
hop)€x
the
category
meas
is
the
base
of
the
giry
monad
m
an
afhne
symmetric
monoidal
monad
that
sends
measurable
space
to
the
measurable
space
of
probability
mea
sures
over
x
we
skip
the
complete
definition
and
instead
simply
define
stoch
as
the
category
with
measurable
spaces
as
objects
and
markov
kernels
as
morphisms
5also
called
the
finitary
giry
monad
or
the
convex
combination
monad
6this
construction
does
not
have
particular
name
22
brief
overview
of
category
theoretic
machine
learning
definition
3.7
markov
kernel
between
the
measurable
space
a,x
and
the
measurable
space
b,xp
is
function
p
ax
x5
0,1
such
that
for
all
o
¥
the
function
p(_,0p
a
0,1
is
measurable
forallz
a
p(z
xp5—1[0,1
is
probability
measure
on
b,xg
in
particular
e
b)=1
p(z,,0)=0
for
example
markov
kernel
between
the
one-point
set
and
the
measurable
space
4,3
4
is
just
probability
measure
over
a
)
we
define
the
composition
of
the
markov
kernels
p:axyp—10,1
and
bx
0,1
to
be
the
following
where
z
and
o
131
50r0
0
y
z,eb
the
identity
morphism
at
a,%
is
where
=z
5(20,7
{0
the
tensor
product
of
the
markov
kernels
p
ax
x5
0,1
and
4
0,1
in
stoch
is
the
following
markov
kernel
where
53
¥
is
the
product
sigma-algebra
w
®p
axc)x
zp®zp
0,1
w
@
4
20),00
04
jutq,00
t
0q
originally
introduced
by
lawvere
ie
the
category
stoch
is
the
paradigmatic
example
of
markov
category
|
r1t
ch
4
one
of
the
most
interesting
aspects
of
stoch
is
that
many
of
the
most
unportant
operations
in
probability
including
marginalization
disintegration
and
independence
can
all
be
characterized
internally
to
stoch
while
one
of
the
most
used
constructions
stoch
admits
few
pathological
examples
2013
p
26
and
it
is
not
known
whether
it
has
conditional
probabilities
in
general
fritz
2020
ex
11.7
furthermore
the
induced
functor
meas
stoch
is
not
faithful
2020
ex
10.4
fritz
notes
that
stoch
might
not
be
actually
the
best
markov
category
for
measure-theoretic
probability
7
p
31
these
are
the
reasons
that
many
authors
often
work
with
its
subcategories
fong
7
and
culbertson
et
al
sturtﬂ
work
with
cgstoch
while
fritz
et
al
fritz
et
alf
2020
works
with
among
others
borelstoch
and
finstoch
the
subcategory
cgstoch
of
stoch
has
countably
generated
measurable
spaces
as
ob
jects
and
markov
kernels
over
perfect
probability
measures
as
morphisms
the
subcategory
borelstoch
of
cgstoch
adds
the
additional
condition
of
separability
and
is
defined
as
the
kleisli
category
of
the
giry
monad
on
pol
the
category
of
polish
saces
and
medsurdble
maps
both
cgstoch
and
borelstoch
have
all
conditionals
fong
the
restriction
of
stoch
to
the
finite
case
as
subcategory
finstoch
dlso
ylelds
category
with
conditionals
7
ex
11.6
this
is
the
example
of
canonical
functor
ki(d
for
some
monad
defined
on
€
shiebler
gavranovié
wilson
23
3.2.6
cartesian
closedness
and
quasi-borel
spaces
when
considering
probabilistic
programs
the
distributions
we
manipulate
are
not
arbitrary
but
come
from
particular
random
source
similarly
in
statistics
and
probability
theory
we
focus
primarily
on
random
variables
over
some
fixed
global
sample
space
rather
than
arbitrary
probability
measures
7
sec
2
furthermore
in
the
context
of
probabilistic
programming
it
is
often
desirable
to
reason
about
the
space
of
probabilistic
mappings
internal
to
the
probabilistic
category
at
hand
however
while
meas
is
symmetric
monoidal
it
is
not
cartesian
closed
since
the
space
of
measurable
maps
into
spaces
of
measurable
maps
is
not
always
measurable
culbertson
et
al
(
attempts
to
address
this
problem
by
simply
equipping
meas
with
different
monoidal
product
heunen
et
al
(
use
different
strategy
and
instead
generalize
measurable
spaces
to
quasi-borel
spaces
qbs
which
are
cartesian
closed
(
prop
18
definition
3.8
def
in
(
)
quasi-borel
space
is
set
x
its
carrier
together
with
subset
my
of
the
function
space
satisfying
the
following
conditions
closure
with
respect
to
precomposition
with
measurable
morphisms
for
every
my
and
every
measurable
f:r
the
following
holds
fsoe€
m
inclusion
of
constants
for
every
constant
we
have
that
m
closure
under
gluing
with
disjoint
borel
domains
for
every
countable
measurable
partition
s
and
every
sequence
o
in
my
we
have
that
my
and
is
defined
as
b(r
=a;(r
for
allr:s
there
are
number
of
ways
to
characterize
quasi-borel
spaces
including
as
structured
measurable
spaces
7
ii1.b.1
or
as
conservative
extension
of
standard
borel
spaces
that
supports
simple
type
theory
products
coproducts
and
function
spaces
iv
heunen
et
al
(
demonstrate
the
use
of
quasi-borel
spaces
by
showing
that
well-known
construction
of
probability
theory
involving
random
functions
gains
clear
expression
they
also
generalize
de
finetti’s
theorem
to
quasi-borel
spaces
3.3
causality
and
bayesian
updates
there
are
wide
variety
of
category
theoretic
approaches
to
simple
bayesian
learning
for
example
fong
7
uses
graphical
framework
to
generalize
bayesian
networks
he
first
defines
causal
theory
to
be
strict
symmetric
monoidal
category
generated
by
directed
acyclic
graph
and
equipped
with
comonoidal
structure
on
each
object
next
he
shows
how
functor
f
from
causal
theory
with
vertex
set
v
into
stoch
is
equivalent
to
bayesian
network
with
variables
in
v
given
some
edge
in
the
causal
theory
the
markov
kernel
that
maps
this
edge
to
encodes
the
probabilistic
relationship
between
and
b
forward
aka
predictive
inference
is
then
mediated
by
the
compositions
of
morphisms
in
the
dag
and
their
images
under
f
this
construction
is
very
general
and
stoch
can
be
replaced
with
any
other
markov
category
howver
fong
does
not
describe
how
causal
theories
could
be
used
for
backwards
inference
or
how
they
can
be
learned
from
data
jacobs
8
takes
slightly
different
perspective
and
generalizes
bayesian
networks
with
construction
similar
to
gavranovié’s
(
formulation
of
neural
network
see
section
{.3
24
brief
overview
of
category
theoretic
machine
learning
proposition
4
let
be
directed
acyclic
multi-graph
and
some
probability
monad
then
the
conditional
tables
of
bayesian
network
are
given
by
strong
monoidal
functor
from
the
free
category
of
to
the
kleisli
category
of
d
jacobs
(
describes
similar
framework
for
bayesian
updates
to
prior
over
joint
distribution
he
focuses
on
multinomial
distributions
and
treats
multisets
with
unique
elements
as
the
parameter
vectors
for
n-category
multinomial
distributions
he
describes
two
natural
transformations
out
of
the
multiset
monad
which
commute
with
an
expected
value
operation
the
normalization
function
[
m(n
2d(n
which
maps
multiset
in
n
to
its
corresponding
maximal
likelihood
estimate
this
estimate
is
multinomial
distribution
in
d(n
where
is
the
distribution
monad
the
dirichlet
distribution
function
dir
m(n
g(d(n
which
maps
multiset
in
n
to
the
dirichlet
distribution
parameterized
by
that
multiset
this
distribution
is
probability
measure
over
multinomial
distributions
and
is
therefore
an
element
of
g(d(n
where
is
the
giry
monad
jacobs
characterizes
the
normalization
and
dirichlet
distribution
natural
transformations
as
frequentist
and
bayesian
learning
procedures
respectively
while
an
update
to
the
parameters
of
the
dirichlet
distribution
smoothly
adjusts
the
distribution
over
multinomials
applying
the
same
procedure
to
the
output
of
the
normalization
function
will
cause
the
distribution
to
collapse
as
it
overcorrects
to
the
new
observation
these
works
also
hint
at
how
categorical
probability
can
serve
as
basis
for
structural
perspective
on
machine
learning
cho
and
jacobs
7
use
their
generalized
string
diagram
formulation
of
bayesian
inversion
to
step
through
small
example
of
naive
bayes
classification
they
first
disintegrate
channels
from
table
of
data
and
then
invert
the
combined
channels
to
yield
classification
it’s
worth
noting
how
different
this
formulation
is
from
the
optics-based
perspective
on
learning
described
in
section
2l
whereas
the
optics
papers
abstract
away
the
objective
function
and
focus
on
the
mechanics
of
model
updates
cho
and
jacobs
objective
is
inextricably
tied
to
the
marginalization
inversions
and
disintegrations
in
the
learning
procedure
3.3.1
bayes
law
concretely
while
bayes
law
was
described
abstractly
in
section
and
grounded
in
markov
category
with
conditionals
in
this
subsectlon
we
describe
the
more
ractlcdl
aspect
of
bayes
law
in
the
work
of
culbertson
et
al
appearing
six
years
before
markov
categories
the
authors
work
w1thln
the
concrete
settmg
of
cgstoch
permitting
them
to
talk
about
conditionals
unlike
the
more
general
setting
of
stoch
the
authors
focus
entirely
on
supervised
learning
but
note
that
the
general
framework
applies
to
any
bayesian
machine
learning
problem
they
also
develop
categorical
framework
for
up
dating
prior
with
data
and
study
stochastic
processes
using
functor
categories
demonstrating
the
kalman
filter
as
an
archetype
for
the
hidden
markov
model
this
enables
culbertson
et
al
culbertson
and
sturtz
to
define
bayesian
model
to
be
diagram
that
consists
of
the
following
components
shiebler
gavranovié
wilson
25
h——d
hypothesis
space
measurable
space
that
serves
as
the
space
of
potential
models
e«
data
space
measurable
space
that
serves
as
the
space
of
possible
experiment
outcomes
prior
probability
measure
an
arrow
py
1
h
sampling
distribution
an
arrow
§
that
relates
the
models
in
to
distributions
over
the
data
space
d
inference
map
an
arrow
j
that
relates
new
data
observations
to
posterior
probabilities
over
h
the
authors
construct
such
that
for
o
doy
h
detd
7(-'17d7(7h)11pd:/
s(zy,0p)dpy
theth
that
is
is
the
regular
conditional
probability
constructed
from
the
joint
distribution
over
that
py
and
define
given
new
data
observation
which
the
authors
represent
with
probability
measure
over
d
or
an
arrow
d
the
inference
map
defines
the
posterior
probability
over
to
be
ph(”h):/
i(xp,0y)dp
zp€d
culbertson
et
al’s
construction
iterates
the
following
process
for
each
new
data
point
p
d
1
set
py(oy
=/
ided](.’l'd,(th)d[l
2
set
py
to
}5h
3
define
from
py
and
s
culbertson
et
al
also
describe
how
this
process
can
work
in
the
eilenberg-moore
category
of
the
giry
nonad
g
which
they
dub
the
category
of
decision
rules
as
the
kleisli
category
of
the
giry
monad
stoch
embeds
into
this
category
the
objects
in
this
category
are
g-algebras
or
pairs
of
measurable
space
and
measurable
map
a
gx
x
since
gx
is
the
space
of
probability
distributions
over
x
we
can
think
of
as
decision
rule
that
collapses
distribution
over
to
single
value
in
machine
learning
application
we
would
expect
that
the
elements
of
the
hypothesis
space
are
maps
over
the
data
space
d
the
cleanest
way
to
represent
such
maps
would
be
as
exponential
objects
however
meas
is
not
closed
over
the
cartesian
product
26
brief
overview
of
category
theoretic
machine
learning
in
order
to
solve
this
culbertson
et
al
culbertson
and
sturtz
define
new
monoidal
product
such
that
meas
becomes
closed
this
is
in
contrast
to
the
solution
in
a
convenient
category
for
higher-order
probability
theory
where
the
authors
choose
to
instead
redefine
meas
itself
under
the
new
tensor
product
in
culbertson
et
al
culbertson
)
the
o-algebra
on
@y
is
the
largest
o-algebra
such
that
the
constant
graph
functions
i
x®y
and
i
®y
that
respectively
send
to
z,y
andy
€y
to
z,y
are
measurable
culbertson
et
al
also
demonstrate
that
when
we
equip
meas
with
this
monoidal
product
it
becomes
symmetric
monoidal
closed
category
such
that
the
constant
graph
functions
x—>x®
yx
ti(z
z,f
and
the
evaluation
maps
evxy
xy
=y
evy
y(z
f
f(z
are
both
measurable
this
allows
the
authors
to
define
bayesian
models
over
function
spaces
for
example
suppose
we
have
gaussian
process
gp(m
k
where
and
is
kernel
function
k
r
then
the
following
diagram
shows
how
the
composite
of
prior
distribution
over
hypotheses
that
this
gaussian
process
defines
and
sampling
distribution
§
=4
defined
by
evy
an
observation
defines
data
distribution
over
and
an
inference
map
from
to
yx
p
1~
m(x
k(x.x
pgp
m,k
in
this
framework
an
arrow
of
the
form
is
stochastic
process
and
an
arrow
of
the
form
yx
is
parameterized
stochastic
process
shiebler
also
explores
categorical
perspective
on
learning
algorithms
like
culbertson
et
al
)
shiebler
focuses
on
parameterized
stochastic
processes
which
he
represents
as
morphisms
in
category
shiebler
explores
two
ways
in
which
stochastic
processes
can
compose
co-kleisli
composition
in
which
randomness
is
shared
and
para
composition
in
which
each
stochastic
process
is
equipped
with
its
own
probability
space
unlike
culbertson
et
al’s
framework
shiebler’s
framework
is
built
on
frequentist
perspec
tive
shiebler
particularly
focuses
on
the
composition
of
likelihood
functions
and
the
derivation
shiebler
gavranovié
wilson
27
of
loss
functions
from
likelihoods
to
do
this
shiebler
constructs
monoidal
semicategory
of
generalized
likelihood
functions
and
he
defines
strict
monoidal
semifunctor
from
category
of
stochastic
processes
into
this
category
shiebler
then
uses
this
construction
to
develop
back
propagation
functors
into
learn
in
which
the
error
function
is
derived
from
the
likelihood
3.4
optics
for
probability
some
authors
have
begun
to
leverage
similar
strategies
to
those
described
in
section
{2/
to
study
lenses
and
optics
in
probability
for
example
smithe
leverages
cho
and
jacob’s
synthetic
approach
to
bayesian
inversion
(
2019
to
demonstrate
that
bayesian
updates
compose
optically
since
his
construction
relies
on
the
cartesian-closedness
of
the
base
category
smithe’s
primary
exam
ple
base
category
is
the
category
of
quasi-borel
spaces
qbs
(
rather
than
the
category
meas
of
measurable
spaces
given
probability
monad
on
qbs
smithe
writes
ki
for
its
kleisli
category
and
con
structs
the
ki(p)-state-indexed
category
stat
ki()°
qbs
cat
where
qbs
cat
is
the
category
of
qbs-enriched
categories
stat
is
functor
that
maps
qbs
to
the
category
stat(x
with
the
same
objects
as
qbs
the
morphisms
in
stat(x)(a
b
are
stochastic
chan
nels
qbs(a,pb
smithe
then
applies
spivak’s
grothendieck
lens
construction
(
to
stat
to
form
the
category
grlensg
in
which
objects
are
pairs
of
objects
of
ki(p
and
morphisms
are
stat-lenses
or
elements
of
the
set
ki(p)(x,y
qbs(ki(p)(i,x),ki(p)(b,a
smithe
then
demonstrates
that
stat-lenses
compose
as
optics
and
uses
this
fact
to
define
category
of
lenses
bayeslens
within
which
grlensg
is
full
subcategory
in
particular
every
pair
of
stochastic
channel
and
its
inverse
as
in
cho
and
jacobs
(
forms
bayesian
lens
smithe
calls
such
lenses
exact
bayesian
lenses
smithe’s
main
result
is
essentially
that
the
composition
of
two
exact
bayesian
lenses
yields
another
exact
bayesian
lens
that
is
bayesian
inversion
is
almost-everywhere
preserved
upon
lens
composition
one
of
the
benefits
of
this
perspective
is
that
smithe
can
study
the
lawfulness
of
bayesian
lenses
similarlﬁ
fong
and
johnson’s
exploration
of
the
lawfulness
of
learners
as
lenses
due
to
the
inherent
uncertainty
in
the
processes
that
bayesian
lenses
model
it
is
not
surprising
that
none
of
the
three
laws
getput
putget
putput
hold
in
general
in
particular
getput
and
putget
only
hold
with
respect
to
states
as
in
cho
and
jacobs
)
since
bayesian
update
after
observing
data
that
exactly
matches
our
prediction
does
not
result
in
change
to
our
prior
3.5
functorial
statistics
some
authors
have
also
used
functoriality
to
explore
how
the
different
pieces
of
statistical
learn
ing
systems
can
fit
together
mccullagh
7
attempts
to
unearth
the
invariants
at
the
heart
of
statistical
modeling
he
does
so
by
forming
categories
over
which
we
can
define
statistical
designs
and
model
parameters
as
functors
such
that
statistical
models
form
natural
transformations
between
them
the
commutativity
of
these
natural
transformations
enforces
that
parameter
maps
maintain
their
structure
independent
of
sample
space
transformations
and
28
brief
overview
of
category
theoretic
machine
learning
that
the
meaning
of
parameter
does
not
change
in
response
to
modifications
of
the
statistical
design
furthermore
allison
(
defines
system
for
representing
components
and
cat
egories
of
statistical
models
as
types
and
classes
he
uses
this
system
to
identify
and
factor
out
the
common
components
between
different
types
of
algorithms
such
as
mixture
modeling
and
decision
trees
he
also
describes
how
recombinations
of
these
common
components
can
lead
to
new
algorithms
although
he
doesn’t
explore
this
idea
in
detail
invariant
and
equivariant
learning
4.1
overview
the
research
in
this
section
focuses
on
the
symmetry-preserving
properties
of
machine
learning
algorithms
the
authors
whom
we
cite
use
variety
of
tools
for
exploring
the
relationships
between
transformations
of
datasets
and
the
outputs
of
machine
learning
models
that
run
on
those
datasets
many
of
these
tools
are
explicitly
category
theoretic
such
as
functors
and
natural
transformations
4.1.1
applications
successes
and
motivation
the
authors
in
this
section
discuss
variety
of
different
kinds
of
machine
learning
algorithms
that
are
used
widely
in
practice
clustering
section
)
clustering
algorithms
group
points
in
the
same
dataset
to
gether
social
networks
use
clustering
algorithms
to
identify
users
who
share
interests
or
social
connections
satuluri
et
all
manifold
learning
section
)
manifold
learning
algorithms
map
the
points
in
dataset
to
low-dimensional
embeddings
in
r™
which
can
then
be
used
as
features
for
machine
learning
models
manifold
algorithms
are
also
commonli
used
aloni
with
nearest
neighbor
search
algorithms
to
power
recommendation
engines
shiebler
et
all
convolutional
neural
networks
section
@
convolutional
neural
networks
process
image
data
in
way
that
exploits
the
position
invariance
inherent
to
most
image
processing
tasks
these
are
used
in
basically
every
modern
image
processing
application
ft
al
borg
graph
neural
networks
section
@
graph
neural
networks
process
graph
data
in
way
that
exploits
both
node
features
and
internode
connectivities
graph
neural
networks
are
used
widely
for
traffic
forecasting
7
4.1.2
background
several
of
the
authors
whom
we
cite
in
this
section
rely
heavily
on
the
vocabulary
of
topological
data
analysis
including
simplicial
complexes
and
filtrations
we
recommend
chazal
chazal
for
good
introduction
to
these
topics
briefly
finite
simplicial
complex
is
family
of
finite
sets
that
is
closed
under
taking
subsets
the
finite
sets
in
simplicial
complex
are
called
faces
and
the
n-simplices
are
subcomplexes
that
contain
all
of
the
subsets
of
single
n+1)-element
face
in
the
complex
for
shiebler
gavranovié
wilson
29
example
the
0-simplices
also
called
vertices
are
points
the
1-simplices
are
pairs
of
points
the
2-simplices
are
triples
of
intersecting
1-simplices
that
we
can
visualize
as
triangles
etc
note
that
any
n-simplex
in
simplicial
complex
is
completely
determined
by
the
set
of
vertices
that
span
it
simplicial
map
between
simplicial
complex
sy
with
vertices
and
simplicial
complex
sy
with
vertices
is
function
f
such
that
if
z,,2,,---,2
span
an
n-simplex
in
sy
then
f(z
f(zy
f(z
span
an
m-simplex
in
sy
where
<n
simplicial
complexes
and
simplicial
maps
form
category
scpx
given
metric
space
x,dy
and
choice
of
r
the
/-vietoris-rips
complex
is
the
simplicial
complex
whose
n-simplices
are
the
n-element
subsets
of
with
all
pairwise
distances
no
greater
than
d
note
that
if
<4
then
the
set
of
n-simplices
of
the
d-vietoris-rips
complex
is
subset
of
the
set
of
n-simplices
of
§’-vietoris-rips
complex
sequence
§
<d
.
induces
sequence
of
vietoris-rips
complexes
each
of
which
contains
the
simplices
of
the
previous
complex
such
sequence
is
called
filtration
4.1.3
big
ideas
and
challenges
the
main
idea
of
this
section
is
that
many
powerful
machine
learning
algorithms
are
invariant
or
equivariant
to
certain
kinds
of
dataset
transformations
understanding
these
properties
allows
us
to
better
understand
how
algorithms
separate
signal
from
noise
by
characterizing
an
algorithm
as
functor
or
natural
transformation
we
can
encode
these
invariances
and
equivariances
in
the
morphisms
of
the
source
and
target
category
however
for
more
sophisticated
algorithms
these
invariance
and
equivariance
properties
often
hold
only
approximately
they
may
fall
apart
completely
at
the
edges
like
convolution’s
position
invariance
or
require
properties
that
hold
only
in
special
cases
like
the
invertibility
of
the
data
matrix
in
linear
regression
this
makes
it
difficult
to
formalize
these
properties
in
the
language
of
functors
and
natural
transformations
the
authors
in
this
section
use
number
of
strategies
to
get
around
this
challenge
including
approximate
commutation
(
and
exploiting
symmetry
in
the
loss
function
rather
than
4.2
functorial
unsupervised
learning
an
unsupervised
learning
algorithm
is
any
algorithm
that
aims
to
extract
insights
from
data
without
explicit
supervision
the
class
of
unsupervised
algorithms
is
much
more
general
than
that
of
supervised
algorithms
but
most
unsupervised
algorithms
operate
by
doing
one
or
both
of
the
following
determining
the
shape
of
the
probability
distribution
that
the
data
was
drawn
from
estimating
low
dimensional
manifold
that
the
data
is
assumed
to
lie
upon
in
order
for
an
unsupervised
algorithm
to
be
useful
the
properties
of
the
lower
dimensional
manifold
or
probability
distribution
that
the
algorithm
uses
to
describe
the
observed
data
must
be
somewhat
in
line
with
the
structure
of
that
data
one
way
to
formalize
this
is
to
cast
these
algorithms
as
various
kinds
of
functors
and
characterize
this
property
in
term
of
functoriality
30
brief
overview
of
category
theoretic
machine
learning
algorithm
description
produces
is
functor
is
functor
non
from
met
from
overlapping
met
clusters
single
linkage
connected
components
yes
yes
yes
of
rips
complex
robust
single
linkage
connected
components
yes
no
yes
of
rescaled
rips
complex
maximal
linkage
simplices
of
rips
no
yes
yes
complex
kmeans
centroids
that
yes
no
no
minimize
within-cluster
variance
table
2
clustering
algorithms
4.2.1
functorial
clustering
one
of
the
most
common
classes
of
unsupervised
learning
algorithms
is
clustering
algorithms
clustering
of
metric
space
x,dy
is
essentially
partitioning
of
such
that
the
points
x,2
are
more
likely
to
be
in
the
same
partition
if
d(z,2
is
small
definition
4.1
given
partition
py
of
the
set
and
partition
py
of
the
set
y
refinement-preserving
map
from
py
to
py
is
function
f:x
=y
such
that
for
any
py
there
exists
some
s
py
with
f(s
cs
carlsson
and
memoli
carlsson
and
mémoli
describe
clustering
algorithms
as
functors
from
one
of
the
following
categories
of
metric
spaces
into
the
category
part
of
partitions
and
refinement-preserving
maps
met
the
category
of
metric
spaces
and
non-expansive
maps
between
them
non
expansive
map
between
the
metric
spaces
x,dy
and
y,dy
is
function
f
y
such
that
for
z
2y
x
dx(21,79
dy
f(z1
f(22
met
the
category
of
metric
spaces
and
injective
non-expansive
maps
between
them
met
the
category
of
metric
spaces
and
isometries
distance-preserving
isomorphisms
between
them
note
that
met
is
subcategory
of
met
;
which
is
in
turn
subcategory
of
met
an
example
clustering
functor
from
met
to
the
category
of
partitions
and
refinement-preserving
maps
is
the
following
definition
4.2
the
single
linkage
at
distance
functor
maps
metric
space
x,dy
to
the
partition
of
such
that
the
points
x,,x
are
the
same
partition
only
if
there
exists
some
such
that
dy(z;,x
<e
chain
of
points
xq,z
.
2
the
clusters
defined
by
single
linkage
clustering
are
the
connected
components
of
the
vietoris
rips
complex
and
can
be
viewed
as
discrete
approximation
of
the
path
components
in
topological
space
shiebler
gavranovié
wilson
31
particularly
important
result
in
clustering
theory
is
the
kleinberg
impossibility
theo
rem
this
theorem
states
that
it
is
impossible
to
define
clustering
algorithm
that
satisfies
all
three
of
the
following
properties
scale
invariance
the
clustering
function
should
not
be
sensitive
to
the
units
of
mea
surement
any
algorithm
that
uses
distance
hyperparameter
such
as
single
linkage
clustering
fails
to
satisfy
scale
invariance
richness/surjectivity
the
clustering
function
should
be
able
to
produce
any
cluster
ing
given
an
appropriate
distance
function
note
that
clustering
algorithm
that
satisfies
this
condition
is
morally
similar
to
classification
algorithm
that
can
shatter
any
set
of
points
i.e
infinite
vc
dimension
vapnik
and
chervonenkis
)
any
algorithm
that
requires
pre-set
number
of
clusters
such
as
k-means
fails
to
satisfy
this
condition
consistency
shrinking
the
distances
between
points
in
the
same
cluster
and
increasing
the
distances
between
points
in
different
clusters
does
not
change
the
clustering
any
centroid-based
algorithm
such
as
mixture
of
gaussians
fails
to
satisfy
this
condition
one
strategy
for
getting
around
this
restriction
is
to
define
hierarchical
clustering
al
gorithms
that
generate
series
of
clusterings
each
at
different
scale
thereby
relaxing
scale
invariance
there
are
number
of
formalizations
of
hierarchical
clustering
algorithms
for
example
carlsson
and
memoli
7
define
hierarchical
clustering
al
gorithm
to
be
functor
from
category
of
metric
spaces
to
category
of
persistent
sets
and
persistence-preserving
maps
definition
4.3
persistent
set
is
pair
x,0
where
is
finite
set
and
is
function
from
the
non-negative
real
line
0,00
to
the
set
of
partitions
of
so
that
the
following
properties
hold
1
if
then
6(r
refines
6(s
2
for
any
r
there
is
number
>0
so
that
6(r
=0(r
for
all
v
r,r+
definition
4.4
persistence-preserving
map
is
function
f:(x,0
y,n
such
that
any
partitioning
that
f~
induces
on
is
refinement
of
the
partitioning
induces
we
can
alternatively
define
persistent
sets
as
functors
from
the
non-negative
real
line
to
the
category
in
which
objects
are
partitions
of
and
morphisms
are
refinement-preserving
maps
in
this
formulation
persistence-preserving
maps
are
natural
transformations
between
persistent
sets
while
variety
of
agglomerative
hierarchical
clustering
algorithms
are
functorial
over
met
.
much
smaller
set
of
algorithms
is
functorial
over
met
and
the
following
is
the
unique
well
behaved
functor
over
met
ing»
definition
4.5
the
single
linkage
functor
s£
maps
metric
space
x,dy
to
the
persistent
set
x,0
where
for
some
0,00
the
points
x|,x
are
in
the
same
partition
in
6(€
only
if
there
exists
some
chain
of
points
x,t,,...,x
such
that
dy(x
x
;
<e
there
are
number
of
major
issues
with
the
single
linkage
clustering
algorithm
that
make
it
perform
poorly
in
practice
for
example
it
is
extremely
sensitive
to
noise
points
which
can
fall
between
distinct
clusters
and
cause
them
to
be
erroneously
bridged
for
this
reason
practical
applications
of
single
linkage
clustering
generally
include
pre/post-processing
stages
that
rescale
distances
or
remove
points
to
reduce
the
impact
of
noise
32
brief
overview
of
category
theoretic
machine
learning
for
example
wishart
7
proposes
first
defining
density
estimate
over
the
space
and
then
removing
points
that
appear
in
low-density
regions
there
are
several
ways
to
implement
this
idea
for
example
chaudhuri
et
al
chaudhuri
and
dasguptal
md
introduce
the
robust
single
linkage
algorithm
which
identifies
the
low-density
points
to
be
points
with
fewer
than
neighbors
within
an
open
ball
of
radius
e
in
another
paper
7
carlsson
and
memoli
explore
how
we
can
define
broad
family
of
clustering
algorithms
in
metm7
that
factor
through
single
linkage
clustering
since
maps
in
metm7
must
be
injective
clustering
functors
over
met
can
be
sensitive
to_the
number
of
points
in
region
such
as
chaudhuri
et
al’s
robust
single
linkage
approach
chaudhuri
and
ddsgupt
functorial
over
met
because
morphism
in
met
may
collapse
multiple
points
into
the
same
point
the
authors
note
that
there
are
many
uninteresting
functorial
mappings
from
met
to
the
category
of
partitions
and
refinement-preserving
maps
they
define
an
additional
property
excisiveness
to
restrict
to
more
interesting
class
of
maps
an
excisive
clustering
functor
is
one
that
is
idempotent
in
that
applying
the
functor
to
any
of
the
partitions
it
forms
will
not
further
subpartition
that
partition
such
density
sensitive
mapping
would
not
be
carlsson
and
memoli’s
primary
result
is
an
explicit
generative
modeling
framework
for
ex
pressing
any
excisive
clustering
functor
from
met
to
the
category
of
partitions
and
refinement
preserving
maps
this
framework
can
represent
any
such
functor
as
finite
collection
of
finite
metric
spaces
the
represented
functor
then
maps
the
points
z,x
in
the
metric
space
to
the
same
partition
if
and
only
if
there
exist
sequence
of
k+1
points
z,z4,...,x;_
2
sequence
of
metric
spaces
w,,...,w
asequence
of
morphisms
f1
.
f
from
f
w
such
that
there
exist
points
a
3
w
where
fi(a
=z
1
f;(8
the
single
linkage
functor
§£(4
at
distance
is
then
represented
by
the
collection
{a,(d)}
where
a
(
is
the
finite
2-point
metric
space
where
the
points
are
separated
by
distance
4
this
functor
is
fundamental
in
the
sense
that
any
clustering
functor
f
expressible
by
this
framework
factors
through
§£(4
such
that
§8£(d
where
is
functor
that
commutes
with
the
forgetful
functor
from
metric
spaces
to
set
i.e
it
maps
the
metric
space
x,dy
to
x,d
it
is
worth
noting
that
transform
the
distance
metric
and
then
apply
single
linkage
clusterils
powerful
recipe
for
noise-resistant
clustering
algorithms
such
as
dbscan
99
4.2.2
functorial
overlapping
clustering
culbertson
et
d
use
different
approach
from
carlsson
and
memoli
8
and
study
clustering
algorithms
that
can
produce
overlapping
clusters
this
produces
dnother
lever
to
mitigate
the
negative
impact
of
chaining
that
occurs
in
single
linkage
clustering
since
the
relation
that
groups
points
into
clusters
does
not
need
to
be
transitive
the
algorithm
can
enforce
maximum
distance
constraints
on
the
points
in
the
same
clusters
the
authors
particularly
focus
on
non-nested
flag
covers
or
non-nested
coverings
whose
associated
abstract
simplicial
complexes
are
flag
complexes
or
simplicial
complexes
that
can
be
expressed
as
the
cliques
of
its
1-skeleton
shiebler
gavranovié
wilson
33
definition
4.6
in
the
category
cov
objects
are
covers
x,cx
and
the
morphisms
between
x,cyx
and
y,dy
are
consistent
maps
or
functions
f
=y
such
that
is
refinement
of
fh(dy
note
that
refinement-preserving
maps
are
just
consistent
maps
between
partitionings
and
that
part
is
subcategory
of
cov
one
particularly
important
kind
of
flag
cover
is
the
following
definition
4.7
given
symmetric
reflexive
relation
on
the
set
x
the
set
of
maximally
linked
subsets
of
consists
of
all
where
1
xry
for
all
x,y
and
2
is
not
properly
contained
in
any
subset
of
satisfying
1
culbertson
et
al
define
overlapping
clustering
functors
to
map
from
category
of
metric
spaces
and
non-expansive
mappings
such
as
met
met
.
or
met
to
cov
they
then
use
this
definition
to
define
generative
model
for
representing
non-overlapping
clusterings
in
terms
of
finite
metric
spaces
given
set
of
finite
metric
spaces
t
the
clustering
functor
ml
maps
the
metric
space
x,dy
to
the
flag
cover
formed
from
the
maximally
linked
subsets
of
the
relation
such
that
zra
if
for
some
t',dp
there
exists
morphism
non-expansive
map
t
t,dy
x,dy
such
that
z,2
im(t
the
main
dlﬁerence
between
this
construction
and
carlsson
and
memoli’s
generative
model
is
the
lack
of
transitivity
clustering
functor
characterized
by
carlbbon
and
memoh
loiith‘lllthii
maps
the
points
z
2
to
the
same
partition
if
there
exists
sequence
of
metric
spaces
in
whose
images
connect
them
in
contrast
clustering
functor
characterized
by
culbertson
et
al’s
construction
(
will
connect
and
z
if
there
is
morphism
from
single
metric
space
in
that
maps
onto
both
of
them
to
be
specific
consider
the
clustering
functor
characterized
by
the
collection
{a,(d)}
where
a
8
is
the
finite
2-point
metric
space
where
the
points
are
separated
by
distance
4
according
to
carlsson
and
memoli’s
generative
model
(
)
this
functor
is
the
single
linkage
functor
however
according
to
culbertson
et
al’s
generative
model
this
is
instead
the
maximal
linkage
at
distance
functor
defined
as
follows
definition
4.8
the
mazimal
linkage
at
distance
functor
maps
metric
space
x,dy
to
the
set
of
mazimally
linked
subsets
of
the
relation
where
xy
rz
when
d(z,,z
4
in
culbertson’s
model
the
single
linkage
functor
is
instead
formed
from
the
collection
{a(0),a1(6),25(8)),a3(6),...}
where
a
d
is
the
finite
n-point
metric
space
where
each
pair
of
points
is
separated
by
distance
0
in
order
to
corral
these
differences
culbertson
et
al
use
the
language
of
simplicial
complexes
both
single
and
maximal
linkage
can
be
defined
in
terms
of
the
vietoris-rips
complex
the
clusters
formed
by
single
linkage
are
the
connected
components
of
the
complex
which
are
non
overlapping
whereas
the
clusters
formed
by
maximal
linkage
are
the
simplices
of
the
complex
which
may
overlap
along
the
faces
in
follow-up
paper
culbertson
et
al
extend
this
construction
to
handle
hierarchical
clustering
algorithms
with
overlaps
they
ﬁrst
generalize
carlsson
and
memoli’s
definition
of
persistent
sets
to
persistent
covers
by
replacing
the
partitions
in
the
codomain
with
non-nested
flag
covers
their
construction
is
then
built
on
top
of
functors
from
category
of
metric
space-like
objects
which
they
call
weight
categories
to
the
category
of
sieves
or
persistent
covers
that
contain
the
trivial
cover
{x}
such
functor
weight
sieve
is
then
stationary
sieving
functor
if
commutes
with
the
forgetful
functor
to
set
34
brief
overview
of
category
theoretic
machine
learning
given
the
functor
sieve
weight
that
maps
the
sieve
x,0y
to
x,dy
where
dy(z,2")=min{t
ja
€04(t),x
a,2
a}
the
composition
€§j
weight
weight
is
idempotent
and
non-expansive
like
carlsson
and
memoli
carlsson
and
mémoli
7
culbertson
et
al
(
@
demonstrate
that
there
exists
universal
sieving
functor
through
which
every
stationary
sieving
functor
factors
however
unlike
carlsson
and
memoli’s
construction
this
functor
is
the
maximal
linkage
functor
aka
the
rips
sieving
functor
rather
than
the
single
linkage
functor
this
highlights
fundamental
difference
between
overlapping
and
non-overlapping
clustering
functors
maximal
linkage
is
universal
for
overlapping
clustering
and
single
linkage
is
universal
for
non-overlapping
clustering
one
thing
that
both
carlsson
and
memoli
7
and
culbertson
et
al
highlight
is
the
relationship
between
clusterings
and
simplicial
complexes
for
example
we
can
decompose
carlsson
and
memoli’s
single
linkage
functor
into
the
composition
fj§m
where
the
functor
fs
met
scpx
maps
metric
space
to
its
6-rips
complex
and
7
scpx
set
is
the
connected
components
functor
m
ierney
mclinnes
et
al
reframe
hierarchical
clustering
from
topological
perspective
and
shiebler
(
md
builds
on
this
to
explicitly
characterize
hierarchical
overlapping
clustering
algorithms
as
functors
from
0
1]°p
to
cov
that
factor
through
category
of
simplicial
coﬁﬂexes
shiebler
also
unites
mcinnes
perspective
with
culbertson
et
al’s
(
2016
by
characterizing
both
the
maximal
and
single
linkage
clustering
functors
in
terms
of
factorization
through
finite
singular
set
functor
4.2.3
flattening
hierarchical
clustering
one
of
the
core
theories
of
topological
data
analysis
is
that
homological
structures
that
exist
at
multiple
scales
are
particularly
important
descriptors
of
dataset
chazal
and
michel
2017|
for
example
the
important
connected
components
of
filtration
are
those
which
have
large
differences
between
the
indices
of
the
simplicial
complexes
in
which
they
first
and
last
appear
meclimes
et
al
mclnnes
and
healy
po17
and
chazal
ot
al
chazal
et
al|
po13
use
this
insight
to
define
strategies
to
flatten
hierarchical
clustering
that
are
akin
to
the
derivation
of
persistence
diagram
in
topological
data
analysis
chazal
and
michel
rolle
and
scoccola
rolle
and
scoccolal
md
use
chazel
et
al’s
chazal
et
al
inter
leaving
distance
to
explore
the
stability
of
these
flattening
algorithms
definition
4.9
the
interleaving
distance
between
the
functors
f,g
r
<
is
the
smallest
such
that
there
exist
pair
of
commuting
natural
transformations
g(r
f(r
+
and
f(r
g(r+e
if
we
cast
an
algorithm
as
series
of
transformations
between
functors
out
of
r
<
we
can
prove
that
the
algorithm
is
stable
by
proving
that
each
transformation
is
uniformly
continuous
with
respect
to
the
interleaving
distance
scoccola
202
4.2.4
multiparameter
hierarchical
clustering
one
of
the
shortcomings
of
the
robust
single
linkage
algorithm
is
that
it
requires
an
additional
density
estimation
parameter
the
choice
of
which
can
be
arbitrary
since
this
parameter
behaves
similarly
to
the
scale
parameter
over
which
single
linkage
clustering
varies
natural
question
is
shiebler
gavranovié
wilson
35
whether
hierarchical
clustering
can
be
extended
to
multiple
parameters
this
would
both
sim
plify
the
theoretical
presentation
of
the
algorithm
and
enable
the
algorithm
to
take
advantage
of
the
structure
exposed
by
varying
the
scale
parameter
carlsson
and
memoli
explore
this
theme
multiparameter
hierarchical
clustering
methods
carlsson
and
mémoli
)
defining
multiparameter
hierarchical
clustering
scheme
to
be
functor
from
category
of
metric
spaces
to
the
category
of
persistent
structures
which
are
multiparameter
extensions
of
per
sistent
sets
they
then
extend
the
uniqueness
theorem
of
single
linkage
clustering
e
idr
anc
8
to
demonstrate
that
robust
single
linkage
is
the
unique
2-dimensional
hierarchical
clustering
functor
that
satisfies
2-dimensional
extension
of
the
conditions
in
this
theorem
one
challenge
with
using
multiparameter
hierarchical
clustering
algorithms
is
that
the
most
common
procedures
for
flattening
hierarchical
clusterings
rely
on
the
possibility
of
reresentrng
the
clustering
with
merge
tree
or
dendrogram
01
when
the
hierarchical
structure
is
indexed
by
partial
order
hke
2
rdther
thdil
totdl
order
like
r,<
this
is
no
longer
an
option
it
is
possible
that
there
are
two
clusters
in
h(ay,ay,...,a
and
¢
in
h(a},a},...,a
such
that
and
¢
are
neither
disjoint
nor
in
con
tainment
relationship
this
is
closely
related
to
the
phenomenon
that
unlike
single
parameter
persistence
modules
multiparameter
persistence
modules
cannot
be
decomposed
into
direct
sum
of
simple
modules
lesnick
and
wright
2015
order
to
et
dround
this
limitation
some
authors
use
adaptations
of
lesnick
et
al’s
strategy
of
using
persistent
structures
that
are
parameterized
along
dfhne
slices
of
r
for
example
rolle
and
scoccola
7
define
hierarchical
clusterrng
algorithm
that
is
indexed
by
curve
in
r™
<
rather
than
all
of
r™,<
in
this
way
the
clustering
represents
structure
across
different
values
of
each
parameter
they
use
this
strategy
to
define
the
«-linkage
algorithm
which
is
multiparameter
hierarchical
generalization
of
robust
single
linkage
shiebler
takes
different
perspective
and
instead
develops
an
algorithm
based
on
binary
integer
programming
and
prior
distribution
over
hyperparameter
values
to
solve
the
multiparameter
flattening
problem
directly
he
demonstrates
that
this
algorithm
can
produce
better
results
than
selecting
an
optimal
hyperparameter
value
4.2.5
functorial
manifold
learning
some
duthors
have
also
be
gun
exploring
functorial
frameworks
for
manifold
learning
mcinnes
et
al
build
dimensionality
reduction
algorithm
umap
uniform
manifold
approxrrndtron
dild
projection
that
uses
simplicial
complexes
and
filtrations
to
define
coherent
way
to
combine
multiple
local
approximations
of
geodesic
distance
umap
exploits
the
property
that
we
can
locally
approximate
the
geodesic
distance
between
points
that
lie
on
manifold
that
is
embedded
within
r™
in
order
to
extend
these
local
approx
imations
into
an
approximation
of
the
geodesic
distance
between
any
point
and
its
neighbors
umap
defines
custom
distance
metric
for
such
that
the
data
is
uniformly
distributed
with
respect
to
this
metric
this
metric
defines
the
distance
from
to
any
point
to
be
%dkn
x,9
where
dg
z,y
is
the
r"-distance
from
to
and
is
the
distance
from
to
its
nearest
neigh
bor
this
technique
is
similar
to
the
strategies
used
in
hdbscan
and
robust
single
linkage
but
in
the
opposite
direction
rather
than
expand
distances
in
regions
of
low
density
umap
contracts
them
this
effectively
normalizes
all
regions
to
uniform
density
this
rescaling
creates
family
of
distinct
metric
spaces
one
for
each
data
point
x
in
36
brief
overview
of
category
theoretic
machine
learning
order
to
combine
these
spaces
into
coherent
structure
umap
builds
rips
complex
for
each
metric
space
and
then
combines
the
complexes
with
fuzzy
set
union
the
resulting
combined
simplicial
complex
acts
as
representation
of
the
local
and
global
connectivity
of
the
dataset
this
representation
is
used
to
learn
an
matrix
of
d-dimensional
embeddings
for
the
elements
of
x,dy
by
applying
stochastic
gradient
descent
to
minimize
the
cross
entropy
between
the
reference
simplicial
complex
and
the
rips
complex
of
the
metric
space
defined
by
the
embeddings
in
a
shiebler
20211}
builds
on
this
to
introduce
hierarchy
of
manifold
learning
algo
rithms
based
on
the
dataset
transformations
over
which
the
are
equivariant
shlebler
shows
that
umap
is
equivariant
to
1sometr1es
and
both
isomap
enenbdum
et
al
and
metric
multidimensional
scaling
a
bd
re
eluvarldnt
to_sur
ectlve
non-expansive
maps
like
rolle
and
scoccola
coccol
0
shiebler
also
uses
interleaving
distance
to
bound
on
how
well
the
embeddmgs
that
manifold
learning
algorithms
learn
on
noisy
data
approximate
the
embeddings
they
learn
on
noiseless
data
4.3
functorial
supervised
learning
some
authors
have
begun
to
use
similar
techniques
to
those
described
in
section
to
character
ize
the
invariances
of
supervised
learning
algorithms
harris
builds
framework
in
which
learning
algorithms
are
natural
transformations
between
training
dataset
functor
and
prediction
model
functor
p
given
category
of
input
spaces
category
of
output
spaces
and
an
index
category
i
d:xxy
xi®
set
maps
an
input
output
index
tuple
to
the
set
of
all
possible
training
datasets
{(x;,y
i€
i}
p:xpxy
xi®
set
maps
an
input
output
index
tuple
to
the
set
of
all
possible
prediction
functions
f
y
harris
defines
an
invariant
learning
algorithm
as
transformation
p
that
is
natural
in
and
and
dinatural
in
x
intuitively
such
an
algorithm
defines
collection
of
functions
each
of
which
maps
set
of
training
datasets
to
set
of
prediction
functions
he
then
characterizes
learn
ing
algorithms
in
terms
of
the
categories
x
y
over
which
they
satisfy
this
naturality
condition
for
example
the
linear
regression
algorithm
is
natural
when
finvec
the
category
of
finite
dimensional
vector
spaces
and
invertible
linear
maps
finvec
the
category
of
finite
dimensional
vector
spaces
and
linear
maps
and
euc
the
category
of
euclidean
spaces
and
monomorphisms
however
linear
regression
is
not
natural
when
finvec
because
the
normal
equations
require
inverting
the
data
matrix
an
another
example
healy
et
al
7
model
the
relationship
between
concepts
which
they
represent
as
theories
in
formal
logic
and
components
of
cognitive
neural
network
that
learns
these
concepts
they
define
category
concpt
that
has
concepts
as
objects
and
subconcept
relationship
as
morphisms
and
category
neural
with
architectures
as
objects
and
sets
of
priming
states
which
represent
how
one
part
of
the
network
can
activate
another
as
morphisms
they
define
functor
concpt
neural
to
model
how
cognitive
neural
networks
pass
information
about
concepts
and
they
use
colimits
to
formalize
the
construction
of
complex
concepts
from
simpler
ones
gavranovié
(
takes
slightly
different
perspective
and
focuses
directly
on
the
relationship
between
the
neural
network
architecture
and
the
associated
optimization
shiebler
gavranovié
wilson
37
problem
he
bases
most
of
his
constructions
on
the
free
category
free(g
of
graph
g
which
behaves
somewhat
like
the
neural
network
equivalent
of
database
schema
(
one
of
the
core
concepts
is
the
arch
functor
which
maps
arrows
in
free(g
to
parameterized
functions
or
arrows
in
the
category
para
since
an
arrow
in
free(g
can
be
interpreted
as
sequence
of
composed
edges
in
graph
arch
acts
somewhat
like
deep
learning
library
it
transforms
connection
template
into
parameterized
function
in
order
to
describe
the
process
of
resolving
parameterized
function
to
particular
model
the
training
process
by
partially
applying
vector
in
rp
gavranovié
uses
the
following
de
pendently
typed
function
pspec
arch
ob(parafree(c
p(arch
ob(eucfme(g
this
function
maps
pairs
of
architectures
and
parameters
to
model
functors
model
free(g
euc
although
it
seems
as
if
there
could
exist
some
functor
f
that
serves
similar
role
to
pspec
but
such
that
model
factors
into
model
arch$
f
there
are
some
problems
with
this
in
order
for
this
to
work
it
would
need
to
be
the
case
that
for
architectures
a;,a
the
functor
maps
a
ja
to
fa
fa
which
is
difficult
to
enforce
it
is
also
very
limiting
consider
the
networks
a
§a
and
a
§as
there
is
no
reason
to
believe
that
the
best
weights
for
a
within
a
§a
are
the
same
as
the
best
weights
for
a
within
a
§a
this
problem
sheds
light
on
one
of
the
main
challenges
with
using
categorical
models
of
optimization
and
neural
networks
to
reason
about
machine
learning
relating
an
optimization
system
to
the
task
it
is
trained
with
machine
learning
task
is
the
optimization
problem
that
machine
learning
system
is
designed
to
solve
typically
this
problem
is
defined
in
terms
of
an
objective
function
over
data
samples
that
are
assumed
to
be
drawn
from
some
probability
distribution
there
are
machine
learning
tasks
for
which
the
best
weights
for
a
within
a
§a
are
the
same
as
the
best
weights
for
a
within
a
as
but
specifying
these
tasks
to
be
compatible
with
our
specifications
for
machine
learning
systems
is
challenging
gavranovi
also
takes
particularly
abstract
perspective
on
defining
tasks
he
uses
the
objects
in
free(g
as
templates
for
concepts
sets
such
as
pictures
of
horse
odel
then
maps
arrows
in
free(g
to
parameterized
functions
that
transform
between
concepts
for
example
classification
model
will
map
the
pictures
of
horse
concept
to
the
boolean
true
concept
gavranovié
formalizes
this
notion
with
the
embedding
functor
|[free(g)|
set
and
describes
dataset
dp
as
subfunctor
of
that
assigns
particular
dataset
to
each
concept
4.4
equivariant
neural
networks
particularly
important
stream
of
machine
learning
research
focuses
on
the
equivariance
and
invariance
properties
of
neural
network
architectures
although
much
of
this
work
has
category
theoretic
overtones
many
of
the
authors
who
contribute
to
this
stream
do
not
go
through
the
trouble
of
defining
categories
and
functors
between
them
and
instead
derive
equivariance
properties
explicitly
function
is
equivariant
to
transformation
if
applying
that
transformation
to
its
inputs
results
in
an
equivalent
transformation
of
its
outputs
invariance
is
special
case
of
equivariance
in
which
any
transformation
of
the
inputs
results
in
an
identity
transformation
of
the
outputs
for
example
the
weight-sharing
regimes
of
cnns
and
rnns
make
them
equivariant
to
image
translations
and
sequence
position
shifts
respectively
up
to
edge
effects
38
brief
overview
of
category
theoretic
machine
learning
for
this
reason
researchers
are
actively
exploring
neural
network
architectures
with
equiv
ariance
properies
cohen
and
welling
(
generalize
cnns
to
g-cnns
which
use
generalized
convolution
and
pooling
layers
to
exhibit
equivariance
to
group
transfor
mations
like
rotations
and
reflections
as
well
as
translations
intuitively
g-convolutions
replace
the
position
shifting
in
discrete
convolution
operation
with
general
group
of
transformations
as
group
equivariance
has
grown
in
popularity
some
authors
have
begun
to
dig
deeper
into
its
theoretical
foundations
kondor
and
trivedi
kondor
and
trivedi
2018
show
that
neural
network
layer
is
g-equivariant
only
if
it
has
convolution
structure
and
cohen
et
al
show
that
linear
equivariant
maps
between
the
feature
spaces
are
in
one-to-one
correspondence
with
convolutions
using
equivariant
kernels
cohen
et
al
(
build
on
this
work
to
develop
spherical
cnn
that
uses
generalized
fourier
transformation-powered
spherical
convolutions
to
exhibit
equivariance
to
sphere
rotations
the
authors
demonstrate
that
their
network
is
particularly
effective
at
classifying
3d
shapes
that
have
been
projected
onto
sphere
with
ray
casting
cohen
et
al
cohen
et
al
extend
neural
network
equivariance
beyond
group
symme
tries
they
develop
strategy
to
make
neural
networks
that
consume
data
on
general
manifold
dependent
only
on
the
intrinsic
geometry
of
the
manifold
in
particular
the
authors
develop
convolution
operator
that
is
equivariant
to
the
choice
of
gauge
or
the
tangent
frame
on
the
manifold
relative
to
which
the
orientation
of
filters
are
defined
the
authors
implement
gauge
equivariant
cnns
for
convenient
type
of
manifold
the
icosahedon
and
demonstrate
that
this
strategy
yields
state
of
the
art
performance
at
learning
spherical
signals
one
data
type
with
particularly
complex
symmetry
properties
is
graph
data
maron
et
al
develop
linear
operators
that
are
equivariant
to
graph
permutations
and
de
haan
et
al
(
make
explicit
use
of
category
theory
to
build
more
powerful
neural
networks
that
can
exploit
graph
symmetries
they
start
by
introducing
the
concept
of
graph
representation
which
is
essentially
functor
from
the
category
of
graphs
and
graph
isomorphisms
to
vector
spaces
and
linear
maps
they
then
generalize
maron
et
al’s
equivariant
graph
networks
7
to
global
natural
graph
networks
each
layer
in
natural
graph
network
is
natural
transformation
of
graph
representations
that
is
it
is
linear
map
that
commutes
with
graph
isomorphisms
this
construction
is
both
flexible
and
powerful
unlike
layer
in
an
equivariant
graph
network
7
that
applies
the
same
equivariant
transformation
to
each
graph
layer
in
global
natural
graph
networks
can
apply
different
transformations
to
different
graphs
as
long
as
the
transformations
commute
with
the
chosen
graph
representation
related
stream
of
work
by
bronstein
et
al
bronstein
et
al
aims
to
generalize
equivariant
and
invariant
neural
network
operators
to
non-euclidean
domains
e.g
graphs
and
manifolds
this
geometric
deep
learning
perspective
largely
focuses
on
the
characteriza
tion
and
generalization
of
the
equivariance
properties
of
convolution
and
therefore
naturally
benefits
from
the
universality
of
the
convolution
as
an
invariant
operator
(
bronstein
et
al
7
m
illustrate
that
convolution
commutes
with
the
laplacian
operator
and
they
use
this
insight
to
define
both
spatially
and
spectrally
motivated
generalizations
of
convolution
for
non-euclidean
domains
shiebler
gavranovié
wilson
39
discussion
the
role
of
category
theory
in
machine
learning
research
is
still
nascent
and
we
are
excited
to
see
how
it
grows
over
the
next
decade
the
synthetic
characterizations
of
gradient-based
learning
and
probability
theory
are
constantly
developing
and
the
success
of
geometric
deep
learning
7
has
drawn
the
mainstream
machine
learning
community
towards
category
theoretic
ideas
looking
forward
there
are
two
areas
in
particular
where
we
expect
to
see
large
strides
first
although
some
authors
have
begun
to
explore
categorical
generalizations
of
class
learning
techniques
like
gradient
descent
and
bayesian
updating
(
)
there
has
been
very
little
exploration
of
the
convergence
properties
of
these
generalized
algorithms
there
is
need
for
categorical
perspective
on
learning
theory
to
evolve
along
with
the
categorical
perspective
on
machine
learning
second
categorical
perspectives
on
machine
learning
have
not
yet
been
the
unifying
force
that
they
have
the
potential
to
be
the
synthetic
perspectives
on
probability
and
gradient-based
learning
are
largely
disjoint
and
are
even
farther
removed
from
the
research
on
equivariant
and
invariant
learning
we
hope
to
see
al
machine
more
work
focused
on
unification
in
the
even
longer
term
understanding
how
optimal
solutions
concept
at
the
heart
of
machine
learning
can
be
understood
through
the
lens
of
category
theory
is
something
that
has
not
yet
been
studied
in
the
literature
optimal
solutions
to
abstract
problems
are
in
category
theory
often
characterized
by
their
universal
properties
and
finding
best
approximations
to
problems
is
done
by
computing
kan
extensions
we
conjecture
that
applying
these
ideas
to
concrete
problems
could
be
fruitful
in
providing
deep
overarching
foundation
to
machine
learning
and
optimization
theory
finally
for
the
most
part
we
have
avoided
discussing
the
more
application-focused
inter
sections
between
category
theory
and
machine
learning
in
this
work
this
includes
categorical
perspectives
on
natural
language
processing
automata
learning
quantum
machine
learning
and
many
other
areas
these
subfields
are
developing
extremely
quickly
and
we
look
forward
to
future
surveys
which
cover
them
references
context
and
compositionality
in
biological
and
artificial
neural
systems
neurips
workshop
2019
url
https://context-composition.github.io/
hervé
abdi
metric
multidimensional
scaling
mds
analyzing
distance
matrices
encyclopedia
of
measurement
and
statistics
2007
lloyd
allison
types
and
classes
of
machine
learning
and
data
mining
in
proceedings
of
the
26th
australasian
computer
science
conference-volume
16
pages
207-215
australian
computer
society
inc
2003
frangois
bancilhon
and
nicolas
spyratos
update
semantics
of
relational
views
acm
transac
tions
on
database
systems
tods
6(4):557-575
1981
aaron
bohannon
j
nathan
foster
benjamin
c
pierce
alexandre
pilkiewicz
and
alan
schmitt
boomerang
resourceful
lenses
for
string
data
in
proceedings
of
the
35th
an
nual
acm
sigplan-sigact
symposium
on
principles
of
programming
languages
popl
40
brief
overview
of
category
theoretic
machine
learning
08
page
407-419
new
york
ny
usa
2008
association
for
computing
machinery
isbn
9781595936899
doi
10.1145/1328438.1328487
url
https://doi.org/10.1145/1328438
1328487
tai-danae
bradley
what
is
applied
category
theory
arxiv
e-prints
arxiv:1809.05923
2018
denny
britz
ai
research
replicability
and
incentives
2020
https://dennybritz.com/blog/ai
replication-incentives/
michael
m
bronstein
joan
bruna
yann
lecun
arthur
szlam
and
pierre
vandergheynst
geometric
deep
learning
going
beyond
euclidean
data
corr
abs/1611.08097
2016
url
http://arxiv.org/abs/1611.08097
tom
b
brown
benjamin
mann
nick
ryder
melanie
subbiah
jared
kaplan
prafulla
dhariwal
arvind
neelakantan
pranav
shyam
girish
sastry
amanda
askell
sandhini
agarwal
ariel
herbert-voss
gretchen
krueger
tom
henighan
rewon
child
aditya
ramesh
daniel
m
ziegler
jeffrey
wu
clemens
winter
christopher
hesse
mark
chen
eric
sigler
mateusz
litwin
scott
gray
benjamin
chess
jack
clark
christopher
berner
sam
mccandlish
alec
radford
ilya
sutskever
and
dario
amodei
language
models
are
few-shot
learners
corr
abs/2005.14165
2020
url
https://arxiv.org/abs/2005.14165
joe
brucker
category
theory
and
natural
language
processing
2020
url
https://github
om/
jbrkr/category_theory_natural_language_processing_nlp
r
m
burstall
and
john
darlington
transformation
system
for
developing
recursive
programs
1977
issn
0004-5411
doi
10.1145/321992.321996
url
ttps://doi.org/10.1145/321992.321996
gunnar
carlsson
and
facundo
mémoli
persistent
clustering
and
theorem
of
j
kleinberg
arxiv
e-prints
arxiv:0808.2241
2008
gunnar
carlsson
and
facundo
mémoli
multiparameter
hierarchical
clustering
methods
in
classification
as
tool
for
research
pages
63-70
springer
2010
gunnar
carlsson
and
facundo
mémoli
classifying
clustering
schemes
foundations
of
compu
tational
mathematics
13(2):221-252
2013
kamalika
chaudhuri
and
sanjoy
dasgupta
rates
of
convergence
for
the
cluster
tree
in
ad
vances
in
neural
information
processing
systems
pages
343-351
2010
frédéric
chazal
and
bertrand
michel
an
introduction
to
topological
data
analysis
fundamental
and
practical
aspects
for
data
scientists
arxiv
e-prints
arxiv:1710.04019
2017
frédéric
chazal
david
cohen-steiner
marc
glisse
leonidas
guibas
and
steve
oudot
proximity
of
persistence
modules
and
their
diagrams
in
proceedings
of
the
twenty-fifth
annual
symposium
on
computational
geometry
pages
237-246
2009
frédéric
chazal
leonidas
guibas
steve
oudot
and
primoz
skraba
persistence-based
clustering
in
riemannian
manifolds
journal
of
the
acm
jacm
60(6):1-38
2013
kenta
cho
and
bart
jacobs
disintegration
and
bayesian
inversion
via
string
diagrams
math
ematical
structures
in
computer
science
29(7):938-971
2019
bryce
clarke
derek
elkins
jeremy
gibbons
fosco
loregian
bartosz
milewski
emily
pillmore
and
mario
romén
profunctor
optics
categorical
update
2020
shiebler
gavranovié
wilson
41
robin
cockett
geoffrey
cruttwell
jonathan
gallagher
jean-simon
pacaud
lemay
benjamin
macadam
gordon
plotkin
and
dorette
pronk
reverse
derivative
categories
arxiv
e-prints
arxiw:1910.07065
2019
bob
coecke
and
robert
w
spekkens
picturing
classical
and
quantum
bayesian
inference
arxiv
e-prints
art
arxiv:1102.2368
february
2011
taco
cohen
mario
geiger
and
maurice
weiler
general
theory
of
equivariant
cnns
on
homo
geneous
spaces
arxiv
e-prints
arxiv:1811.02017
2020
taco
s
cohen
and
max
welling
group
equivariant
convolutional
networks
arxiv
e-prints
arxiw:1602.07576
2016
taco
s
cohen
mario
geiger
jonas
koehler
and
max
welling
spherical
cnns
arxiv
e-prints
arxiw:1801.10130
2018
taco
s
cohen
maurice
weiler
berkay
kicanaoglu
and
max
welling
gauge
equivariant
convolutional
networks
and
the
icosahedral
cnn
arxiv
e-prints
arxiv:1902.04615
2019
david
corfield
collection
of
references
on
probability
2021
url
https://ncatlab.org/
eavidcorfield/show/probability#references
g
s
h
cruttwell
bruno
gavranovi
neil
ghani
paul
wilson
and
fabio
zanasi
categorical
foundations
of
gradient-based
learning
arxiv
e-prints
arxiv:2103.01931
2021
jared
culbertson
and
kirk
sturtz
bayesian
machine
learning
via
category
theory
arxiv
e-prints
arxiv:1312.1445
2013
jared
culbertson
and
kirk
sturtz
categorical
foundation
for
bayesian
probability
applied
categorical
structures
22(4):647-662
2014
jared
culbertson
dan
guralnik
jakob
hansen
and
peter
stiller
consistency
constraints
for
overlapping
data
clustering
arxiv
e-prints
arxiv:1608.04331
2016
jared
culbertson
dan
guralnik
and
peter
stiller
functorial
hierarchical
clustering
with
overlaps
discrete
applied
mathematics
236:108-123
2018
pim
de
haan
taco
cohen
and
max
welling
natural
graph
networks
arxiv
e-prints
arxiv:2007.08349
2020
conal
elliott
the
simple
essence
of
automatic
differentiation
proceedings
of
the
acm
on
programming
languages
2(icfp):1-29
2018
martin
ester
hans-peter
kriegel
jorg
sander
xiaowei
xu
et
al
density-based
algorithm
for
discovering
clusters
in
large
spatial
databases
with
noise
in
knowledge
discovery
and
data
mining
kdd
volume
96
pages
226-231
1996
arnold
faden
et
al
the
existence
of
regular
conditional
probabilities
necessary
and
sufficient
conditions
the
annals
of
probability
13(1):288-298
1985
brendan
fong
causal
theories
categorical
perspective
on
bayesian
networks
arxiv
e-prints
arxiv:1301.6201
2013
brendan
fong
and
michael
johnson
lenses
and
learners
arxiv
e-prints
arxiv:1903.03671
2019
brendan
fong
and
david
spivak
seven
sketches
in
compositionality
an
invitation
to
applied
category
theory
2018
42
brief
overview
of
category
theoretic
machine
learning
brendan
fong
david
spivak
and
rémy
tuyéras
backprop
as
functor
compositional
per
spective
on
supervised
learning
in
84th
annual
acm/ieee
symposium
on
logic
in
computer
science
lics
pages
1-13
ieee
2019
uwe
franz
what
is
stochastic
independence
in
non-commutativity
infinite-dimensionality
and
probability
at
the
crossroads
pages
254-274
world
scientific
2002
tobias
fritz
convex
spaces
i
definition
and
examples
arxiv
e-prints
art
arxiv:0903.5522
march
2009
tobias
fritz
synthetic
approach
to
markov
kernels
conditional
independence
and
theorems
on
sufficient
statistics
advances
in
mathematics
370:107239
2020
tobias
fritz
tom4as
gonda
paolo
perrone
and
eigil
fjeldgren
rischel
representable
markov
categories
and
comparison
of
statistical
experiments
in
categorical
probability
arxiv
e
prints
art
arxiv:2010.07416
october
2020
bruno
gavranovic
compositional
deep
learning
arxiv
e-prints
arxiv:1907.08292
2019
bruno
gavranovié
learning
functors
using
gradient
descent
applied
category
theory
2019
neil
ghani
jules
hedges
viktor
winschel
and
philipp
zahn
compositional
approach
to
economic
game
theory
arxiv
e-prints
arxiv:1603.04641
2016
michele
giry
categorical
approach
to
probability
theory
in
categorical
aspects
of
topology
and
analysis
pages
68-85
springer
1982
william
guss
and
ruslan
salakhutdinov
on
universal
approximation
by
neural
networks
with
uniform
guarantees
on
approximation
of
infinite
dimensional
maps
arxiv
e-prints
arxiv:1910.01545
2019
kenneth
d
harris
characterizing
the
invariances
of
learning
algorithms
using
category
theory
arxiv
e-prints
arxiv:1905.02072
2019
michael
healy
category
theory
applied
to
neural
modeling
and
graphical
representations
in
proceedings
of
the
ieee-inns-enns
international
joint
conference
on
neural
networks
ijcnn
2000
neural
computing
new
challenges
and
perspectives
for
the
new
millennium
volume
3
pages
35-40
ieee
2000
jules
hedges
limits
of
bimorphic
lenses
arxiv
e-prints
art
arxiv:1808.05545
august
2018
chris
heunen
ohad
kammar
sam
staton
and
hongseok
yang
convenient
category
for
higher-order
probability
theory
in
32nd
annual
acm/ieee
symposium
on
logic
in
computer
science
lics
pages
1-12
ieee
2017
chris
heunen
ohad
kammar
sam
staton
sean
moss
matthijs
vakdr
adam
scibior
and
hongseok
yang
the
semantic
structure
of
quasi-borel
spaces
probabilistic
programming
languages
semantics
and
systems
2018
alex
irpan
deep
reinforcement
learning
doesn’t
work
yet
https://www.alexirpan.com/2018/
02/14/rl1-hard.html
2018
bart
jacobs
channel-based
perspective
on
conjugate
priors
corr
abs/1707.00269
2017
bart
jacobs
categorical
aspects
of
parameter
learning
arxiv
e-prints
arxiv:1810.05814
2018
weiwei
jiang
and
jiayun
luo
graph
neural
network
for
traffic
forecasting
survey
arxiv
e-prints
arxiw:2101.11174
2021
shiebler
gavranovié
wilson
43
michael
kearns
robert
schapire
and
linda
sellie
toward
efficient
agnostic
learning
machine
learning
17(2-3):115-141
1994
andrew
kennedy
compiling
with
continuations
continued
in
proceedings
of
the
12th
acm
sigplan
international
conference
on
functional
programming
pages
177-190
2007
jon
kleinberg
an
impossibility
theorem
for
clustering
in
advances
in
neural
information
processing
systems
pages
463-470
2003
risi
kondor
and
shubhendu
trivedi
on
the
generalization
of
equivariance
and
convolution
in
neural
networks
to
the
action
of
compact
groups
arxiv
e-prints
arxiv:1802.03690
2018
william
lawvere
the
category
of
probabilistic
mappings
preprint
1962
url
/ncatlab.org/nlab/files/lawvereprobability1962.pdf|
michael
lesnick
and
matthew
wright
interactive
visualization
of
2-d
persistence
modules
arxiv
e-prints
arxiv:1512.00180
2015
drew
linsley
dan
shiebler
sven
eberhardt
and
thomas
serre
learning
what
and
where
to
attend
with
humans
in
the
loop
in
international
conference
on
learning
representations
2019
url
https://openreview.net/forum?id=bjglg3rikq
haggai
maron
heli
ben-hamu
nadav
shamir
and
yaron
lipman
invariant
and
equivariant
graph
networks
arxiv
e-prints
arxiv:1812.09902
2019
peter
mccullagh
what
is
statistical
model
annals
of
statistics
pages
1225-1267
2002
leland
mclnnes
topological
methods
for
unsupervised
learning
in
international
conference
on
geometric
science
of
information
pages
343-350
springer
2019
leland
mcinnes
and
john
healy
accelerated
hierarchical
density
clustering
arxiv
e-prints
arxiw:1705.07321
2017
leland
mcinnes
john
healy
and
james
melville
umap
uniform
manifold
approximation
and
projection
for
dimension
reduction
arxiv
e-prints
arxiv:1802.03426
2018
chris
olah
and
shan
carter
research
debt
distill
2017
doi
10.23915/distill.00005
https://distill
pub/2017
/research-debt
christopher
olah
neural
networks
types
and
functional
programming
2015
https://colah.github.io/posts/2015-09-nn-types-fp
/
ali
rahimi
machine
learning
has
become
alchemy
2018
https://www.youtube.com/watch?v=x7tpsghgatgm
bastian
rieck
machine
learning
needs
langlands
programme
2020
https://bastian.rieck.me/blog/posts/2020/langlands/
mitchell
riley
categories
of
optics
arxiv
e-prints
arxiv:1809.00738
2018
alexander
rolle
and
luis
scoccola
stable
and
consistent
density-based
clustering
arxiv
e
prints
arxiv:2005.09048
2020
44
brief
overview
of
category
theoretic
machine
learning
mario
roman
open
diagrams
via
coend
calculus
electronic
proceedings
in
theoretical
computer
science
333:65-78
feb
2021
issn
2075-2180
doi
10.4204/eptcs.333.5
url
http://dx.doi.org/10.4204/eptcs.333.5
sebastian
ruder
an
overview
of
gradient
descent
optimization
algorithms
arxiv
e-prints
arxiv:1609.04747
2017
david
e
rumelhart
geoffrey
e
hinton
and
ronald
j
williams
learning
representations
by
back-propagating
errors
nature
323(6088):533-536
october
1986
issn
1476-4687
doi
10.1038/323533a0
url
https://www.nature.com/articles/323533a0
number
6088
publisher
nature
publishing
group
venu
satuluri
yao
wu
xun
zheng
yilei
qian
brian
wichers
qieyun
dai
gui
ming
tang
jerry
jiang
and
jimmy
lin
simclusters
community-based
representations
for
heteroge
neous
recommendations
at
twitter
in
proceedings
of
the
26th
acm
sigkdd
international
conference
on
knowledge
discovery
and
data
mining
kdd
20
page
3183-3193
new
york
ny
usa
2020
association
for
computing
machinery
isbn
9781450379984
doi
10.1145/3394486.3403370
url
https://doi.org/10.1145/3394486.3403370
luis
scoccola
locally
persistent
categories
and
metric
properties
of
interleaving
dis
tances
thesis
university
of
western
ontario
2020
url
https://ir.lib.uwo.ca/cgi/
iewcontent.cgi?article=9630&context=etd
robert
a.g
seely
richard
f
blute
and
j
r
cockett
differential
categories
mathematical
structures
in
computer
science
16(6):1049-1083
2006
robert
a.g
seely
richard
f
blute
and
j
r
cockett
cartesian
differential
categories
theory
and
applications
of
categories
22(23):622-672
2009
dan
shiebler
categorical
stochastic
processes
and
likelihood
arxiv
e-prints
arxiv:2005.04735
2020a
dan
shiebler
functorial
clustering
via
simplicial
complexes
topological
data
analysis
and
beyond
workshop
at
neurips
2020
2020b
url
https://openreview.net/pdf?id=
zkdlcxcp5sv
dan
shiebler
flattening
multiparameter
hierarchical
clustering
functors
international
con
ference_on
geometric
science
of
information
2021a
url
https://arxiv.org/pdf/2104
_4734.pdf
14734
pat
dan
shiebler
functorial
manifold
learning
arxiv
e-prints
arxiv:2011.07435
2021b
dan
shiebler
luca
belli
jay
baxter
hanchen
xiong
and
abhishek
tayal
fighting
redundancy
and
model
decay
with
embeddings
corr
abs/1809.07703
2018
url
abs/1809.07703
david
silver
julian
schrittwieser
karen
simonyan
ioannis
antonoglou
aja
huang
arthur
guez
thomas
hubert
lucas
baker
matthew
lai
adrian
bolton
yutian
chen
timothy
lillicrap
fan
hui
laurent
sifre
george
van
den
driessche
thore
graepel
and
demis
has
sabis
mastering
the
game
of
go
without
human
knowledge
nature
550(7676):354-359
october
2017
issn
1476-4687
doi
10.1038/nature24270
url
number
7676
publisher
nature
publishing
group
toby
st
clere
smithe
bayesian
updates
compose
optically
arxiv
e-prints
arxiv:2006.01631
2020
shiebler
gavranovié
wilson
45
david
spivak
functorial
data
migration
information
and
computation
217:31-51
2012
david
i
spivak
generalized
lens
categories
via
functors
c°®
cat
arxiv
e-prints
arxiv:1908.02202
2020a
david
i
spivak
poly
an
abundant
categorical
setting
for
mode-dependent
dynamics
arxiv
e-prints
arxiw:2005.01894
2020b
david
sprunger
and
shin-ya
katsumata
differentiable
causal
computations
via
delayed
trace
in
34th
annual
acm/ieee
symposium
on
logic
in
computer
science
lics
pages
1-12
1ieee
2019
joshua
tenenbaum
vin
de
silva
and
john
langford
global
geometric
framework
for
nonlinear
dimensionality
reduction
science
290(5500):2319-2323
2000
vladimir
vapnik
and
ya
chervonenkis
on
the
uniform
convergence
of
relative
frequencies
of
events
to
their
probabilities
in
measures
of
complezity
pages
11-30
springer
2015
paul
wilson
and
fabio
zanasi
reverse
derivative
ascent
categorical
approach
to
learn
ing
boolean
circuits
electronic
proceedings
in
theoretical
computer
science
333:247-260
feb
2021
issn
2075-2180
doi
10.4204/eptes.333.17
url
http://dx.doi.org/10.4204/
eptcs.333.17
david
wishart
256
note
an
algorithm
for
hierarchical
classifications
biometrics
pages
165-170
1969
