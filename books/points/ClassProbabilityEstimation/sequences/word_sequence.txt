arxiv:1503.01436v7
cs.lg
11
feb
2016
class
probability
estimation
via
differential
geometric
regularization
qinxun
bai
qinxun@(cs.bu.edu
department
of
computer
science
boston
university
boston
ma
02215
usa
steven
rosenberg
sr@math.bu.edu
department
of
mathematics
and
statistics
boston
university
boston
ma
02215
usa
zheng
wu
the
mathworks
inc
natick
ma
01760
usa
stan
sclaroff
wuzheng@bu.edu
sclaroff@cs.bu.edu
department
of
computer
science
boston
university
boston
ma
02215
usa
abstract
we
study
the
problem
of
supervised
learning
for
both
binary
and
multiclass
classification
from
unified
geometric
perspective
in
particular
we
propose
geometric
regularization
technique
to
find
the
submanifold
corresponding
to
robust
estimator
of
the
class
probability
p(y|x
the
regularization
term
measures
the
volume
of
this
submanifold
based
on
the
intuition
that
overfit
ting
produces
rapid
local
oscillations
and
hence
large
volume
of
the
estimator
this
technique
can
be
applied
to
regularize
any
classification
func
tion
that
satisfies
two
requirements
firstly
an
es
timator
of
the
class
probability
can
be
obtained
secondly
first
and
second
derivatives
of
the
class
probability
estimator
can
be
calculated
in
ex
periments
we
apply
our
regularization
technique
to
standard
loss
functions
for
classification
our
rbf-based
implementation
compares
favorably
to
widely
used
regularization
methods
for
both
binary
and
multiclass
classification
1
introduction
in
supervised
learning
for
classification
the
idea
of
regu
larization
seeks
balance
between
perfect
description
of
the
training
data
and
the
potential
for
generalization
to
un
seen
data
most
regularization
techniques
are
defined
in
the
form
of
penalizing
some
functional
norms
for
instance
one
of
the
most
successful
classification
methods
the
sup
port
vector
machine
svm
vapnik
1998
scholkopf
smola
2002
and
its
variants
bartlett
et
al
2006
stein
wart
2005
use
rkhs
norm
as
regularizer
while
functional
norm
based
regularization
is
widely-used
in
ma
chine
learning
we
feel
that
there
is
important
local
geomet
ric
information
overlooked
by
this
approach
in
many
real
world
classification
problems
if
the
feature
space
is
meaningful
then
all
samples
that
are
locally
within
small
enough
neighborhood
of
training
sample
should
have
class
probability
p(y|x
similar
to
the
training
sam
ple
for
instance
small
enough
perturbation
of
rgb
values
at
some
pixels
of
human
face
image
should
not
change
dramatically
the
likelihood
of
correct
identifica
tion
of
this
image
during
face
recognition
however
such
small
local
oscillations
of
the
class
probability
are
not
explicitly
incorporated
by
penalizing
commonly
used
func
tional
norms
for
instance
as
reported
by
goodfellow
et
al
2014
linear
models
and
their
combinations
can
be
eas
ily
fooled
by
hardly
perceptible
perturbations
of
correctly
predicted
image
even
though
l2
regularizer
is
adopted
geometric
regularization
techniques
have
also
been
studied
in
machine
learning
belkin
et
al
2006
employed
geo
metric
regularization
in
the
form
of
the
l2
norm
of
the
gra
dient
magnitude
supported
on
manifold
this
approach
exploits
the
geometry
of
the
marginal
distribution
p(z
for
semi-supervised
learning
rather
than
the
geometry
of
the
class
probability
p(y|z
other
related
geometric
reg
ularization
methods
are
motivated
by
the
success
of
level
set
methods
in
image
segmentation
cai
sowmya
2007
varshney
willsky
2010
and
euler’s
elastica
in
image
processing
lin
et
al
2012
2015
in
particular
the
level
learning
set
cai
sowmya
2007
combines
counting
function
of
training
samples
and
geometric
penalty
on
the
surface
area
of
the
decision
boundary
the
geomet
ric
level
set
varshney
willsky
2010
generalizes
this
idea
to
standard
empirical
risk
minimization
schemes
with
margin-based
loss
along
this
line
the
euler’s
elastica
model
lin
et
al
2012
2015
proposes
regularization
class
probability
estimation
via
differential
geometric
regularization
technique
that
penalizes
both
the
gradient
oscillations
and
the
curvature
of
the
decision
boundary
however
all
three
methods
focus
on
the
geometry
of
the
decision
boundary
supported
in
the
domain
of
the
feature
space
and
the
small
local
oscillation
of
the
class
probability
is
not
explicitly
addressed
in
this
work
we
argue
that
the
small
local
oscillation
of
the
class
probability
actually
lies
in
the
product
space
of
the
feature
domain
and
the
probabilistic
output
space
and
can
be
characterized
by
the
geometry
of
submanifold
in
this
product
space
corresponding
to
the
class
probability
let
f
al~!be
class
probability
estimator
where
x
is
the
feature
space
and
a%~
is
the
probabilistic
simplex
for
classes
from
geometric
perspective
if
we
regard
{(x
f(z))|x
x}
the
functional
graph
in
the
geometric
sense
of
f
as
submanifold
in
al~
then
small
local
oscillations
can
be
measured
by
the
local
flatness
of
this
submanifold
in
our
approach
the
learning
process
can
be
viewed
as
submanifold
fitting
problem
that
is
solved
by
geometric
flow
method
in
particular
our
approach
finds
submani
fold
by
iteratively
fitting
the
training
samples
in
curvature
or
volume
decreasing
manner
without
any
priori
assump
tions
on
the
geometry
of
the
submanifold
in
al~1
we
use
gradient
flow
methods
to
find
an
optimal
direction
i.e
at
each
step
we
find
the
vector
field
pointing
in
the
optimal
direction
to
move
f
as
we
will
see
in
the
next
section
this
regularization
approach
naturally
handles
binary
and
mul
ticlass
classification
in
unified
way
while
previous
deci
sion
boundary
based
techniques
and
most
functional
reg
ularization
approaches
are
originally
designed
for
binary
classification
and
rely
on
one
versus
one
one
versus
all
or
more
efficiently
binary
coding
strategy
varshney
willsky
2010
to
generalize
to
multiclass
case
in
experiments
radial
basis
function
rbf
based
im
plementation
of
our
formulation
compares
favorably
to
widely
used
binary
and
multiclass
classification
methods
on
datasets
from
the
uci
repository
and
real-world
datasets
including
the
flickr
material
database
fmd
and
the
mnist
database
of
handwritten
digits
in
summary
our
contributions
are
geometric
perspective
on
overfitting
and
regular
ization
approach
that
exploits
the
geometry
of
robust
class
probability
estimator
for
classification
unified
gradient
flow
based
algorithm
for
both
bi
nary
and
multiclass
classification
that
can
be
applied
to
standard
loss
functions
and
rbf-based
implementation
that
achieves
promising
experimental
results
figure
1
example
of
three-class
learning
i.e
3
where
the
input
space
is
2d
training
samples
of
the
three
classes
are
marked
with
red
green
and
blue
dots
respectively
the
class
label
for
each
training
sample
corresponds
to
vertex
of
the
simplex
ae=1
as
result
each
mapped
training
point
(
z
lies
on
one
face
corresponding
to
its
label
y
of
the
space
x
a%
2
method
overview
in
our
work
we
propose
regularization
scheme
that
ex
ploits
the
geometry
of
robust
class
probability
estimator
and
suggest
gradient
flow
based
approach
to
solve
for
it
in
the
follow
we
will
describe
our
approach
related
math
ematical
notation
is
summarized
in
table
2
following
the
probabilistic
setting
of
classification
given
sample
feature
space
ry
label
space
{1
l}
and
finite
training
set
of
labeled
samples
tm
{(zi,y:)}™
where
each
training
sample
is
gen
erated
i.i.d
from
distribution
over
x
)
our
goal
is
to
find
hr
y
such
that
for
any
new
sample
x
hy
predicts
its
label
a
x
the
optimal
generalization
risk
bayes
risk
is
achieved
by
the
classifier
h*(z
argmax{n‘(zx),l
y}
wheren
n',...,n%
with
0,1
being
the
¢
class
probability
i.e
v'(@
ply
fla
our
regularization
approach
exploits
the
geometry
of
the
class
probability
estimator
and
can
be
regarded
as
hy
brid
plug-in/erm
scheme
audibert
tsybakov
2007
regularized
loss
minimization
problem
is
setup
to
find
an
estimator
al~
where
a“~
is
the
stan
dard
l
1)-simplex
in
r®
and
f%,...,fl)is
an
estimator
of
with
f
0,1
the
estima
tor
is
then
plugged-in
to
get
the
classifier
hy(x
argmax{
f(z
y}
figure
shows
an
example
of
the
setup
of
our
approach
for
synthetic
three-class
classification
problem
the
sub
manifold
corresponding
to
estimator
is
the
graph
in
the
geometric
sense
of
f
gr(f
{(z
fl(z
fl(z
x}
ap~
we
denote
point
in
the
space
ab
z,2
2f
2n
21
.
2e
where
xz
and
al~
then
in
this
product
space
class
probability
estimation
via
differential
geometric
regularization
alcr
aalcr
alcr
©
figure
2
example
of
binary
learning
via
gradient
flow
as
shown
in
a
the
feature
space
is
2d
training
points
are
sampled
uniformly
within
the
region
15
15
[
15,15
and
labeled
by
the
function
sign(10
||z||2
the
red
circle
in
the
initialization
step
shown
in
b
positive
and
negative
training
points
map
to
the
two
faces
of
the
space
x
a
respectively
our
gradient
flow
method
starts
from
neutral
function
f
and
moves
towards
the
negative
direction
red
and
blue
arrows
of
the
penalty
gradient
vpy
figure
c
shows
the
submanifold
gr(f
one
step
after
b
the
submanifold
then
continues
to
evolve
towards
v
py
step
by
step
and
the
final
output
after
convergence
of
the
algorithm
is
shown
in
d
training
pair
z;,y
¢
naturally
maps
to
the
point
i
z
x,0,...,1,...,0
with
the
one-hot
vector
z
with
the
in
its
th
slot
at
the
vertex
of
af~1
corre
sponding
to
p(y
y;|x
1
we
point
out
two
properties
of
this
geometric
setup
firstly
it
inherently
handles
multiclass
classification
with
binary
classification
as
special
case
secondly
while
the
dimen
sion
of
the
ambient
space
i.e
r¥+l
depends
on
both
the
feature
dimension
and
number
of
classes
l
the
intrinsic
dimension
of
the
submanifold
gr(f
only
depends
on
n
2.1
variational
formulation
we
want
gr(f
to
approach
the
mapped
training
points
while
remaining
as
flat
as
possible
so
we
impose
penalty
on
consisting
of
an
empirical
loss
term
p
and
geo
metric
regularization
term
pg
for
p
we
can
choose
ei
ther
the
widely-used
cross-entropy
loss
function
for
multi
class
classification
or
the
simpler
euclidean
distance
func
tion
between
the
simplex
coordinates
of
the
graph
point
and
the
mapped
training
point
for
pg
we
would
ideally
con
sider
an
l2
measure
of
the
riemann
curvature
of
gr(f
as
the
vanishing
of
this
term
gives
optimal
i.e
locally
distor
tion
free
diffeomorphisms
from
gr(f
to
r
however
the
riemann
curvature
tensor
takes
the
form
of
combina
tion
of
derivatives
up
to
third
order
and
the
corresponding
gradient
vector
field
is
even
more
complicated
and
ineffi
cient
to
compute
in
practice
as
result
we
measure
the
graph’s
volume
pg(f
fgr(f
dvol
where
dvol
is
the
induced
volume
from
the
lebesgue
measure
on
the
ambi
ent
space
rv+e
more
precisely
we
find
the
function
that
minimizes
the
fol
lowing
penalty
p
=pr
+apg
m=maps(x
a1
1
on
the
set
of
smooth
functions
from
to
ax~
where
is
the
tradeoff
parameter
between
empirical
loss
and
reg
ularization
it
is
important
to
note
that
any
relative
scal
ing
of
the
domain
will
not
affect
the
estimate
of
the
class
probability
7
as
scaling
will
distort
gr(f
but
will
not
change
the
critical
function
estimating
7
2.2
gradient
flow
and
geometric
foundation
the
standard
technique
for
solving
variational
formulas
is
the
euler-lagrange
pde
however
due
to
our
geomet
ric
term
pg
finding
the
minimal
solutions
of
the
euler
lagrange
equations
for
is
difficult
instead
we
solve
for
argmin
using
gradient
flow
in
functional
space
m
simple
but
intuitive
simulated
example
of
binary
learn
ing
using
gradient
flow
for
our
approach
is
given
in
fig
ure
2
for
the
explanation
purposes
only
we
replace
with
finite
dimensional
riemannian
manifold
m
with
out
loss
of
generality
we
also
assume
that
is
smooth
then
it
has
differential
dpy
ty
rforeach
m
where
t’y
is
the
tangent
space
to
at
f
since
dpy
is
linear
functional
on
t’y
m
there
is
unique
tangent
vec
tor
denoted
vpy
such
that
dp¢(v
v
vpy
for
all
tym
vpy
points
in
the
direction
of
maximal
in
crease
of
at
f
thus
the
solution
of
the
negative
gra
dient
flow
df,/dt
vpy
is
flow
line
of
steepest
descent
starting
at
an
initial
f
for
dense
open
set
of
initial
points
flow
lines
approach
local
minimum
of
at
oo
we
always
choose
the
initial
function
to
be
the
neutral
choice
fo(z
£,...,+
which
reasonably
assigns
equal
conditional
probability
to
all
classes
similar
gradient
flow
procedures
are
widely
used
in
variational
problems
such
as
level
set
methods
osher
sethian
1988
sethian
1999
mumford-shah
func
tional
mumford
shah
1989
etc
in
the
classification
literature
varshney
willsky
2010
were
the
first
to
use
gradient
flow
methods
for
solving
level
set
based
energy
functions
then
followed
by
lin
et
al
2012
2015
to
solve
euler’s
elastica
models
in
our
case
we
are
exploiting
the
class
probability
estimation
via
differential
geometric
regularization
geometry
in
the
space
al~
rather
than
standard
vec
tor
spaces
since
our
gradient
flow
method
is
actually
applied
on
the
infinite
dimensional
manifold
m
we
have
to
understand
both
the
topology
and
the
riemannian
geometry
of
m
for
the
topology
we
put
the
fréchet
topology
on
m
maps(x
re
the
set
of
smooth
maps
from
to
r
and
take
the
induced
topology
on
m
intuitively
speaking
two
functions
in
are
close
if
the
functions
and
all
their
par
tial
derivatives
are
pointwise
close
since
is
an
open
fréchet
submanifold
with
boundary
inside
the
vector
space
m
so
as
with
an
open
set
in
euclidean
space
we
can
canonically
identify
ty
with
m
for
the
riemannian
metric
we
take
the
l
metric
on
each
tangent
space
t’y
m
p1,02
y
b1(2)d2(w)dvol
with
¢
m
and
dvol
being
the
volume
form
of
the
induced
riemannian
metric
on
the
graph
of
f
strictly
speaking
the
volume
form
is
pulled
back
to
x
by
f
usually
denoted
by
f*dvol
the
differential
dpy
is
linear
as
above
and
by
direct
cal
culation
there
is
unique
tangent
vector
vpy
ty
such
that
dpy
vpy
¢
forall
ty
m
thus
we
can
construct
the
gradient
flow
equation
however
unlike
the
case
of
finite
dimensions
the
existence
of
flow
lines
is
not
automatic
assuming
the
existence
of
flow
lines
generic
initial
point
flows
to
local
minimum
of
p
in
any
case
our
rbf-based
implementation
in
§3
mimicking
gra
dient
flow
is
well
defined
note
that
we
think
of
x
as
large
enough
so
that
the
train
ing
data
actually
is
sampled
well
inside
x
this
allows
us
to
treat
a
as
closed
manifold
in
our
gradient
calcu
lations
so
that
boundary
effects
can
be
ignored
simi
lar
natural
boundary
condition
is
also
adopted
by
previous
work
varshney
willsky
2010
lin
et
al
2012
2015
2.3.more
on
related
work
there
exist
some
other
works
that
are
related
to
some
as
pects
of
our
work
most
notably
sobolev
regularization
involves
functional
norms
of
certain
number
of
deriva
tives
of
the
prediction
function
for
instance
the
manifold
regularization
belkin
et
al
2006
mentioned
in
§1
uses
sobolev
regularization
term
ivsrpare
tem
where
is
smooth
function
on
manifold
m
discrete
version
of
2
corresponds
to
the
graph
laplacian
regular
ization
zhou
scholkopf
2005
lin
et
al
2015
dis
cussed
in
detail
the
difference
between
sobolev
norm
and
curvature-based
norm
for
the
purpose
of
exploiting
the
geometry
of
the
decision
boundary
for
our
purpose
while
imposing
say
high
sobolev
norm
will
also
lead
to
flattening
of
the
hypersurface
gr(f
these
norms
are
not
specifically
tailored
to
measur
ing
the
flatness
of
gr(f
in
other
words
high
sobolev
norm
bound
will
imply
the
volume
bound
we
desire
but
not
vice
versa
as
result
imposing
high
sobolev
norm
constraints
regardless
of
computational
difficulties
over
shrinks
the
hypothesis
space
from
learning
theory
point
of
view
in
contrast
our
regularization
term
given
in
11
involves
only
the
combination
of
first
derivatives
of
that
specifically
address
the
geometry
behind
the
small
local
oscillation
prior
observed
in
practice
our
training
procedure
for
finding
the
optimal
graph
of
function
is
in
general
sense
also
related
to
the
manifold
learning
problem
tenenbaum
et
al
2000
roweis
saul
2000
belkin
niyogi
2003
donoho
grimes
2003
zhang
zha
2005
lin
zha
2008
the
most
closely
related
work
is
donoho
grimes
2003
which
seeks
flat
submanifold
of
euclidean
space
that
contains
dataset
again
there
are
key
differences
since
the
goal
of
donoho
grimes
2003
is
dimensionality
reduction
their
mani
fold
has
high
codimension
while
our
functional
graph
has
codimension
1
which
may
be
as
low
as
1
more
impor
tantly
we
do
not
assume
that
the
graph
of
our
target
func
tion
is
flat
or
volume
minimizing
submanifold
and
we
instead
flow
towards
function
whose
graph
is
as
flat
or
volume
minimizing
as
possible
in
this
regard
our
work
is
related
to
large
body
of
literature
on
morse
theory
in
finite
and
infinite
dimensions
and
on
mean
curvature
flow
chen
etal
1999
mantegazza
2011
3
example
formulation
rbfs
we
now
illustrate
our
approach
using
an
rbf
representa
tion
of
our
estimator
f
rbfs
are
also
used
by
previous
ge
ometric
classification
methods
varshney
willsky
2010
lin
et
al
2012
2015
given
values
of
are
probabilistic
vectors
it
is
common
to
represent
as
softmax
output
of
rbfs
i.e
hi
_
where
b/
afg;,(a
zzl:i
el
forj
=1
p=
3
where
z
e~#l@==
is
the
rbf
function
centered
at
training
sample
x
with
kernel
width
parameter
c
estimating
becomes
an
optimization
problem
for
the
coefficient
matrix
af
the
following
equation
determines
a
a(@1
.
h(x
ga
where
gy
p;j(z
4
high
sobolev
norm
is
the
conventional
term
for
sobolev
norm
with
high
order
of
derivatives
class
probability
estimation
via
differential
geometric
regularization
to
plug
this
rbf
representation
into
our
gradient
flow
scheme
the
gradient
vector
field
vpy
is
evaluated
at
each
sample
point
;
and
is
updated
by
a—7g7
vpu(z1
.
vpu(zm
(
where
is
the
step-size
parameter
and
vpn(zi
g—’{
vpg
(
6
here
vp
x
denotes
the
gradient
vector
field
w.rt
evaluated
at
x
and
the
jacobian
matrix
%
can
be
obtained
in
closed
form
from
3
in
the
fnllowinzlsub
sections
we
give
exact
forms
of
the
empirical
penalty
p
and
the
geometric
penalty
pc
and
discuss
the
computation
of
vp
for
both
penalty
terms
3.1
the
empirical
penalty
pr
we
consider
two
widely-used
loss
functions
for
the
empir
ical
penalty
term
py
quadratic
loss
since
pr
measures
the
deviation
of
gr(f
from
the
mapped
training
points
it
is
natural
to
choose
the
quadratic
function
of
the
euclidean
distance
in
the
simplex
az~1
pr.(f
=y
@
zl
@
i=1
where
z
is
the
one-hot
vector
corresponding
to
the
ground
truth
label
of
;
the
gradient
vector
w.r.t
evaluated
at
x
is
vp
1(®i
2(f(2
2i
the
gradient
vector
w.r.t
evaluated
at
is
vp
h(xi
=2
g—ﬂ
f(@i
=
®
evaluation
of
%
ti
is
the
same
as
in
6
cross-entropy
loss
the
cross-entropy
loss
function
is
also
widely-used
for
probabilistic
output
in
classification
pr.(f)=->.>
zllog
f(zi
i=1
=1
whose
gradient
vector
field
w.r.t
evaluated
at
x
is
vpr
n(x
f(z
2
10
3.2
the
geometric
penalty
pg
as
discussed
in
§2
we
wish
to
penalize
graphs
for
exces
sive
curvature
and
we
use
the
following
function
which
measures
the
volume
of
the
gr(f
pa(f
dvol
vdet(g)dat
.
dav
er(f
er(f
an
where
g
with
g
d
f{
f
is
the
riemma
nian
metric
on
gr
f
induced
from
the
standard
dot
product
on
rn+l
we
use
the
summation
convention
on
repeated
indices
note
that
this
regularization
term
is
clearly
very
different
from
the
standard
sobolev
norm
of
any
order
it
is
standard
that
vpg
tr
it
rn+
on
the
space
of
all
embeddings
of
x
in
rn+
if
we
restrict
to
the
sub
manifold
of
graphs
of
m/
itis
easy
to
calculate
that
the
gradient
of
geometric
penalty
11
is
vpaj=vay=—trik
12
where
tr
iiz
denotes
the
last
components
of
tr
ii
then
the
geometric
gradient
w.r.t
is
of
vpan=von=
6_h
tr
112
13
evaluation
of
%
and
tr
it
at
2
leads
to
vpan(x
the
formulation
given
above
is
general
in
that
it
encom
passes
both
the
binary
and
the
multiclass
cases
for
both
cases
evaluation
of
%
at
the
training
points
is
the
same
as
that
in
6
and
evaluation
of
tr
ii
at
any
point
can
be
performed
explicitly
by
the
following
theorem
theorem
1
for
rn
al=l
tr
1%
for
gr(f
is
given
by
tt
g
f}
g
y
a4
where
f{
ff%
denote
partial
derivatives
of
f
the
proof
is
in
appendix
a
note
that
for
our
rbf
rep
resentation
3
the
partial
derivatives
f{
f{
can
be
easily
obtained
in
closed
form
simplex
constraint
the
class
probability
estimators
al~
always
takes
values
in
al~1
re
while
this
constraint
is
automatically
satisfied
for
the
flow
of
the
empirical
gradient
vector
formula
8
and
10
it
may
fail
for
the
flow
of
geometric
gradient
vector
formula
12
there
are
two
ways
to
enforce
this
constraint
for
the
geo
metric
gradient
vector
field
first
since
our
initial
function
class
probability
estimation
via
differential
geometric
regularization
takes
values
at
the
center
of
ax~1
we
can
orthogonally
project
the
geometric
gradient
vector
vi
to
v(/j
in
the
tangent
space
{(y",...,y
rx
8y
0}
of
the
simplex
and
then
scale
7v
7
is
the
stepsize
to
ensure
that
the
range
of
the
new
lies
in
al~1
we
then
iterate
more
simply
we
can
select
of
the
com
ponents
of
f(x
call
the
new
function
f
rl~1
and
compute
the
l
1)-dimensional
gradient
vector
ve
/
following
12
and
14
the
omitted
component
of
the
de
sired
l-gradient
vector
is
determined
by
z[l;ll
v
g0
by
the
definition
of
z
our
implementation
reported
fol
lows
this
second
approach
where
we
choose
the
l
1
components
of
by
omitting
the
component
corresponding
to
the
class
with
least
number
of
training
samples
3.3
algorithm
summary
algorithm
gives
summary
of
the
classifier
learning
pro
cedure
input
to
the
algorithm
is
the
training
set
7
rbf
kernel
width
¢
trade-off
parameter
\
and
step-size
param
eter
7
for
initialization
our
algorithm
first
initializes
the
function
values
of
and
for
every
training
point
and
then
constructs
matrix
and
solves
for
by
4
in
the
subsequent
steps
at
each
iteration
our
algorithm
first
eval
uates
the
gradient
vector
field
vpy
at
every
training
point
then
updates
coefficient
matrix
by
5
for
the
overall
penalty
function
pr
apq
we
compute
the
total
gradient
vector
field
vp
evaluated
at
x
vpu(x
vpr
i
avpq
i
%
2(f(mi
z
ntr
hil
quadratic
flxi)—zi
%
triil
.
cross-entropy
@
our
algorithm
iterates
until
it
converges
or
reaches
the
maximum
iteration
number
the
same
algorithm
applies
to
both
the
quadratic
loss
and
the
cross-entropy
loss
to
evaluate
the
total
gradient
vec
tors
vpp(z
in
each
iteration
for
the
quadratic
loss
we
use
8
and
13
to
compute
the
total
gradient
vector
22
for
the
cross-entropy
loss
we
use
10
and
13
instead
the
remaining
steps
of
the
procedure
are
exactly
the
same
for
both
loss
functions
the
final
predictor
learned
by
our
algorithm
is
given
by
f(a
argmax{f‘(z
{1,2
l}}
16
4
experiments
to
evaluate
the
effectiveness
of
the
proposed
regulariza
tion
approach
we
compare
our
rbf-based
implementation
with
two
groups
of
related
classification
methods
the
first
algorithm
geometric
regularized
classification
input
training
data
7
{(x;,y;)}7
rbf
kernel
width
¢
trade-off
parameter
a
step-size
initialize
h(xz
1,...,1
f(z
%
ce
%
vi
{1,---,m}
construct
matrix
and
solve
by
4
fort
=1tot
do
evaluate
the
total
gradient
vector
vp
x
at
ev
ery
training
point
according
to
22
update
the
by
5
end
for
output
class
probability
estimator
given
by
3
group
of
methods
are
standard
rbf-based
methods
that
use
different
regularizers
than
ours
the
second
group
of
meth
ods
are
previous
geometric
regularization
methods
in
particular
the
first
group
includes
the
radial
basis
func
tion
network
rbn
svm
with
rbf
kernel
svm
and
the
import
vector
machine
ivm
zhu
hastie
2005
a
greedy
search
variant
of
the
standard
rbf
kernel
logis
tic
regression
classifier
note
that
both
svm
and
ivm
use
rkhs
regularizers
and
the
ivm
also
uses
the
similar
cross-entropy
loss
as
ours-ce
the
second
group
includes
the
level
learning
set
classi
fier
cai
sowmya
2007
lls
the
geometric
level
set
classifier
varshney
willsky
2010
gls
and
the
eu
15
ler’s
elastica
classifier
lin
et
al
2012
2015
ee
note
that
both
gls
and
ee
use
rbf
representations
and
ee
also
uses
the
same
quadratic
distance
loss
as
ours-q
we
test
both
the
quadratic
loss
version
ours-q
and
the
cross-entropy
loss
version
ours-ce
of
our
implementa
tion
4.1
uci
datasets
we
tested
our
classification
method
on
four
binary
classi
fication
datasets
and
four
multiclass
classification
datasets
given
that
varshney
willsky
2010
has
covered
several
methods
on
our
comparing
list
and
their
implementation
is
publicly
available
we
choose
to
use
the
same
datasets
as
varshney
willsky
2010
and
carefully
follow
the
ex
act
experimental
setup
tenfold
cross-validation
error
is
reported
for
each
of
the
ten
folds
the
kernel-width
con
stant
and
tradeoff
parameter
are
found
using
fivefold
cross-validation
on
the
training
folds
all
dimensions
of
input
sample
points
are
normalized
to
fixed
range
0
1
throughout
the
experiments
we
select
from
the
set
of
val
ues
{1/2°,1/24,1/2%,1/2%,1/2,1,2,4,8}
and
from
the
set
of
values
{1/1.5%,1/1.5%,1/1.5%,1/1.5,1,1.5}
that
minimizes
the
fivefold
cross-validation
error
the
step-size
0.1
and
iteration
number
7
are
fixed
over
all
class
probability
estimation
via
differential
geometric
regularization
table
1
tenfold
cross-validation
error
rate
percent
on
four
binary
and
four
multi
classification
datasets
from
the
uci
machine
learning
repository
l
n
denote
the
number
of
classes
and
input
feature
dimensions
respectively
we
compare
both
the
quadratic
loss
version
ours-q
and
the
cross-entropy
loss
version
ours-ce
of
our
method
with
rbf-based
classification
methods
and
or
geometric
regularization
methods
svm
with
rbf
kernel
svm
radial
basis
function
network
rbn
level
learning
set
classifier
cai
sowmya
2007
lls
geometric
level
set
classifier
varshney
willsky,2010
gls
import
vector
machine
zhu
hastie
2005
ivm
euler’s
elastica
classifier
lin
et
al.,2012
2015
ee
the
mean
error
rate
averaged
over
all
eight
datasets
is
shown
in
the
bottom
row
top
performance
for
each
dataset
is
shown
in
bold
dataset(l,n
rbn
svm
ours-ce
pima(2.9
24.60
24.12
wdbc(2,30
5.79
2.81
liver(2,6
35.65
28.66
tonos.(2,34
738
399
wine(3,13
t.70
l.11
tris(3.4
167
267
glass(6.9
34.50
31.77
segm.(7.19
1307
3.81
all-avg
15.92
12.37
datasets
we
used
the
same
settings
for
both
loss
functions
table
reports
the
results
of
this
experiment
the
top
per
former
for
each
dataset
is
marked
in
bold
and
the
aver
aged
performance
of
each
method
over
all
testing
datasets
is
summarized
in
the
bottom
row
the
numbers
for
rbn
lls
and
gls
are
copied
from
table
of
varshney
will
sky,2010
results
for
svm
and
ivm
are
obtained
by
run
ning
publicly
available
implementations
for
svm
chang
lin
2011
and
ivm
roscher
et
al
2012
results
for
ee
are
obtained
by
running
an
implementation
provided
by
the
authors
of
lin
et
al
2012
when
running
these
im
plementations
we
followed
the
same
experimental
setup
as
described
above
and
exhaustively
searched
for
the
optimal
range
for
the
kernel
bandwidth
and
the
trade-off
parameter
via
cross-validation
as
shown
in
the
last
row
of
table
1
two
versions
of
our
approach
are
overall
the
top
two
performers
among
all
re
ported
methods
in
particular
ours-q
attains
top
perfor
mance
on
four
out
of
the
eight
benchmarks
ours-ce
at
tains
top
performance
on
three
out
of
the
eight
benchmarks
the
performance
of
the
two
versions
of
our
method
are
very
close
which
shows
the
robustness
of
our
geometric
regularization
approach
cross
different
loss
functions
for
classification
note
that
three
pairs
of
comparisons
ivm
vs
ours-ce
gls
vs
ours-q/ours-ce
and
ee
vs
ours-q
are
of
particular
interest
we
are
going
to
discuss
them
in
detail
respectively
the
ivm
method
of
kernel
logistic
regression
uses
the
same
rbf-based
implementation
and
very
similar
cross
entropy
loss
as
our
cross-entropy
version
ours-ce
and
both
methods
handle
the
multiclass
case
inherently
the
main
difference
lies
in
regularization
i.e
the
standard
rkhs
norm
regularizer
vs
our
geometric
regularizer
ours-ce
outperforms
ivm
on
six
of
the
eight
benchmars
in
table
1
and
achieves
equal
performance
on
one
of
the
remaining
two
and
is
only
slightly
behind
on
pima
the
overall
superior
performance
of
ours-ce
demonstrates
the
advantage
of
the
proposed
geometric
regularization
over
the
standard
rkhs
norm
regularization
the
gls
method
uses
the
same
rbf-based
implementa
tion
as
ours
and
also
exploits
volume
geometry
for
regu
larization
as
described
in
§1
however
there
are
key
dif
ferences
between
the
two
regularization
techniques
gls
measures
the
volume
of
the
decision
boundary
supported
in
x
while
our
approach
measures
the
volume
of
sub
manifold
supported
in
al~
that
corresponds
to
the
class
probability
estimator
our
regularization
technique
handles
the
binary
and
multiclass
cases
in
unified
frame
work
while
the
decision
boundary
based
techniques
such
as
gls
and
ee
were
inherently
designed
for
the
binary
case
and
rely
on
binary
coding
strategy
to
train
logy
decision
boundaries
to
generalize
to
the
multiclass
case
in
our
experiments
both
ours-q
and
ours-ce
outperform
gls
on
all
the
benchmarks
we
have
tested
this
demon
strates
the
effectiveness
of
exploiting
the
geometry
of
the
class
probability
in
addressing
the
small
local
oscillation
for
classification
the
ee
method
of
euler’s
elastica
model
uses
the
same
rbf-based
implementation
and
the
same
quadratic
loss
as
our
quadratic
loss
version
ours-q
the
main
differ
ence
again
lies
in
regularization
i.e
combination
of
1
sobolev
norm
and
curvature
penalty
on
the
decision
bound
ary
vs
our
volume
penalty
on
the
submanifold
correspond
ing
to
the
class
probability
estimator
since
ee
adopts
combination
of
sophisticated
geometric
measures
on
the
decision
boundary
which
fit
specifically
the
binary
case
it
achieves
top
performance
on
binary
datasets
however
as
explained
in
§1
the
geometry
of
the
class
probability
for
class
probability
estimation
via
differential
geometric
regularization
table
2
notations
hg(z
argmax
f*(x
plug-in
classifier
of
a"~
ey
at~t
the
standard(l
1)-simplex
in
r%
n(x
n'(x),...,n"(x
class
probability
5
z
p(y
{|x
fecc™}
{f
>r
fec™}
ty
the
tangent
space
to
at
some
m
ty
m
the
graph
of
or
m
gr(f
{(z
f(x
x}
207
9ij
(
=9
the
riemannian
metric
on
gr
f)induced
from
the
standard
dot
product
on
rn
with
gij)ij=1,...n
:
the
volume
element
on
gr
f
{ei}:i
smoothly
varying
orthonormal
basis
of
the
tangent
spaces
t(s
2)r(f)of
the
graph
of
il
tl
(
dees
the
trace
of
the
second
fundamental
form
of
gr(f
tr
it
rn
with
the
orthogonal
projection
to
the
subspace
perpendicular
to
the
tangent
space
of
gr(f
and
d,w
the
directional
derivative
of
in
direction
tr
it
the
projection
of
tr
ii
onto
the
last
coordinates
of
7%
vp
the
gradient
vector
field
of
function
on
possibly
infinite
dimensional
manifold
general
classification
which
is
captured
by
our
approach
cannot
be
captured
by
decision
boundary
based
techniques
that
is
the
reason
why
ours-q
general
scheme
for
both
the
binary
and
multiclass
case
outperforms
ee
on
all
four
multiclass
datasets
while
it
still
achieves
top
performance
on
binary
datasets
this
again
demonstrates
our
geomet
ric
perspective
and
regularization
approach
that
exploits
the
geometry
of
the
class
probability
4.2.real-world
datasets
to
test
the
scalability
of
our
method
to
high
dimen
sional
and
large-scale
problems
we
also
conduct
exper
iments
on
two
real-world
datasets
i.e
the
flickr
ma
terial
database
fmd
for
image
classification
and
the
mnist
mnist
database
of
handwritten
digits
fmd
4096
dimensional
the
fmd
dataset
contains
10
categories
of
images
with
100
images
per
category
we
ex
tract
image
features
using
the
sift
descriptor
augmented
by
its
feature
coordinates
implemented
by
the
vlfeat
li
brary
vlfeat
with
this
descriptor
bag-of-visual-words
uses
4096
vector-quantized
visual
words
histogram
square
rooting
followed
by
l2
normalization
we
compare
our
method
with
an
svm
classifier
with
rbf
kernels
using
exactly
the
same
4096
dimensional
feature
our
method
achieves
correct
classification
rate
of
48.8%
while
the
svm
baseline
achieves
46.4%
note
that
while
recent
works
qi
et
al
2015
cimpoi
et
al
2015
report
better
performance
on
this
dataset
the
effort
focuses
on
better
feature
design
not
on
the
classifier
itself
the
features
used
in
those
works
such
as
local
texture
descriptors
and
cnn
features
are
more
sophisticated
mnist
60,000
samples
the
mnist
dataset
contains
10
classes
0
9
of
handwritten
digits
with
60
000
sam
ples
for
training
and
10
000
samples
for
testing
each
sam
ple
is
28
28
grey
scale
image
we
use
1000
rbfs
to
represent
our
function
f
with
rbf
centers
obtained
by
ap
plying
k-means
clustering
on
the
training
set
note
that
our
learning
and
regularization
approach
still
handles
all
the
60
000
training
samples
as
described
by
algorithm
1
our
method
achieves
an
error
rate
of
2.74%
while
there
are
many
results
reported
on
this
dataset
we
feel
that
the
most
comparable
method
with
our
representation
is
the
ra
dial
basis
function
network
with
1000
rbf
units
lecun
et
al
1998
which
achieves
an
error
rate
of
3.6%
this
experiment
shows
the
potential
that
our
geometric
regular
ization
approach
scales
to
larger
datasets
5
discussion
our
geometric
regularization
approach
can
also
be
viewed
as
combination
of
common
physical
models
as
illus
trated
in
figure
and
2
each
training
pair
x,y
corre
sponds
to
point
at
one
of
the
vertices
of
the
simplex
asso
ciated
with
as
aresult
all
training
data
lie
on
the
bound
ary
of
the
space
x
af~1
while
the
functional
graph
of
class
probability
estimator
is
hypersurface
subman
ifold
in
af~1
an
initial
estimator
without
training
information
corresponds
to
the
flat
hyperplane
in
the
neu
tral
position
in
response
to
the
presence
of
the
training
data
this
neutral
hypersurface
deforms
towards
the
train
ing
data
as
if
attracted
by
gravitational
force
due
to
point
masses
centered
at
the
training
points
simultaneously
the
regularization
term
forces
the
hypersurface
to
remain
as
flat
or
as
volume
minimizing
as
possible
as
if
in
the
presence
of
surface
tension
thus
this
term
follows
the
physics
of
class
probability
estimation
via
differential
geometric
regularization
soap
films
and
minimal
surfaces
dierkes
et
al
1992
ge
ometric
flows
like
the
one
proposed
here
are
often
modeled
on
physical
processes
in
our
case
the
flow
can
be
viewed
as
mixed
gravity
and
surface
tension
physical
experiment
6
conclusion
we
have
introduced
new
geometric
perspective
on
regu
larization
for
classification
that
exploits
the
geometry
of
robust
class
probability
estimator
under
this
perspective
we
propose
general
regularization
approach
that
applies
to
both
binary
and
multiclass
cases
in
unified
way
in
ex
periments
with
an
example
formulation
based
on
rbfs
our
implementation
achieves
favorable
results
comparing
with
widely
used
rbf-based
classification
methods
and
previ
ous
geometric
regularization
methods
while
experimen
tal
results
demonstrate
the
effectiveness
of
our
geometric
regularization
technique
it
is
also
important
to
study
con
vergence
properties
of
this
approach
from
learning
the
ory
perspective
as
an
initial
attempt
we
have
established
bayes
consistency
for
an
easy
case
of
empirical
penalty
function
and
details
are
provided
in
appendix
b
we
will
continue
this
study
in
the
future
references
audibert
jean-yves
and
tsybakov
alexandre
fast
learn
ing
rates
for
plug-in
classifiers
annals
of
statistics
35
2):608-633,2007
bartlett
peter
l
jordan
michael
i
and
mcauliffe
jon
d
convexity
classification
and
risk
bounds
journal
of
the
american
statistical
association
101(473):138
156
2006
belkin
mikhail
and
niyogi
partha
laplacian
eigen
maps
for
dimensionality
reduction
and
data
representa
tion
neural
computation
15(6):1373-1396,2003
belkin
mikhail
niyogi
partha
and
sindhwani
vikas
manifold
regularization
geometric
framework
for
learning
from
labeled
and
unlabeled
examples
journal
of
machine
learning
research
7:2399-2434,2006
cai
xiongcai
and
sowmya
arcot
level
learning
set
novel
classifier
based
on
active
contour
models
in
proc
european
conf
on
machine
learning
ecml
pp
79
90.2007
chang
chih-chung
and
lin
chih-jen
libsvm
library
for
support
vector
machines
acm
transactions
on
intelligent
systems
and
technol
ogy
2:27:1-27:27
2011
software
available
at
http://www.csie.ntu.edu.tw/~cjlin/libsvm
chen
yun-gang
giga
yoshikazu
and
goto
shun’ichi
uniqueness
and
existence
of
viscosity
solutions
of
gen
eralized
mean
curvature
flow
equations
in
fundamental
contributions
to
the
continuum
theory
of
evolving
phase
interfaces
in
solids
pp
375—412
springer
berlin
1999
devroye
luc
gyorfi
lasz10
and
lugosi
gdbor
prob
abilistic
theory
of
pattern
recognition
springer
1996
dierkes
ulrich
hildebrandt
stefan
kiister
albrecht
and
wohlrab
ortwin
minimal
surfaces
springer
1992
donoho
david
and
grimes
carrie
hessian
eigen
maps
locally
linear
embedding
techniques
for
high
dimensional
data
proceedings
of
the
national
academy
of
sciences
100(10):5591-5596,2003
fmd
http://people.csail.mit.edu/celiu/cvpr2010/fmd/
accessed
2015-06-01
goodfellow
ian
j
shlens
jonathon
and
szegedy
chris
tian
explaining
and
harnessing
adversarial
examples
arxiv
preprint
arxiv
1412.6572,2014
guckenheimer
john
and
worfolk
patrick
dynamical
sys
tems
some
computational
problems
in
bifurcations
and
periodic
orbits
of
vector
fields
montreal
pq
1992
volume
408
of
nato
adv
sci
inst
ser
math
phys
sci
pp
241-277.kluwer
acad
publ
dordrecht
1993
lecun
yann
bottou
léon
bengio
yoshua
and
haffner
patrick
gradient-based
learning
applied
to
document
recognition
proceedings
of
the
ieee
86(11):2278
2324,1998
lin
tong
and
zha
hongbin
riemannian
manifold
learn
ing
ieee
trans
on
pattern
analysis
and
machine
in
telligence
pami
30(5):796-809,2008
lin
tong
xue
hanlin
wang
ling
and
zha
hongbin
total
variation
and
euler’s
elastica
for
supervised
learn
ing
proc
international
conf
on
machine
learning
icml
2012
lin
tong
xue
hanlin
wang
ling
huang
bo
and
zha
hongbin
supervised
learning
via
euler’s
elastica
mod
els
journal
of
machine
learning
research
16:3637
3686,2015
mantegazza
carlo
lecture
notes
on
mean
curva
ture
flow
volume
290
of
progress
in
mathematics
birkhiuser/springer
basel
ag
basel
2011
mnist
http://http://yann
lecun.com/exdb/mnist/
ac
cessed
2015-06-01
mumford
david
and
shah
jayant
optimal
approxima
tions
by
piecewise
smooth
functions
and
associated
vari
ational
problems
communications
on
pure
and
applied
mathematics
42(5):577-685,1989
class
probability
estimation
via
differential
geometric
regularization
osher
stanley
and
sethian
james
fronts
propagating
with
curvature-dependent
speed
algorithms
based
on
hamilton-jacobi
formulations
journal
of
computational
physics
79(1):12-49
1988
roscher
ribana
forstner
wolfgang
and
waske
bjorn
vm
incremental
import
vector
machines
image
and
vision
computing
30(4):263-278,2012
roweis
sam
and
saul
lawrence
nonlinear
dimensional
ity
reduction
by
locally
linear
embedding
science
290
5500):2323-2326,2000
scholkopf
bernhard
and
smola
alexander
learning
with
kernels
support
vector
machines
regularization
opti
mization
and
beyond
mit
press
2002
sethian
james
albert
level
set
methods
and
fast
marching
methods
evolving
interfaces
in
computational
geometry
fluid
mechanics
computer
vision
and
materials
science
volume
3
cambridge
university
press
1999
steinwart
ingo
consistency
of
support
vector
machines
and
other
regularized
kernel
classifiers
ieee
trans
in
formation
theory
51(1):128-142,2005
stone
charles
consistent
nonparametric
regression
an
nals
of
statistics
pp
595-620
1977
tenenbaum
joshua
de
silva
vin
and
langford
john
global
geometric
framework
for
nonlinear
dimensional
ity
reduction
science
290(5500):2319-2323,2000
vapnik
vladimir
naumovich
statistical
learning
theory
volume
1
wiley
new
york
1998
varshney
kush
and
willsky
alan
classification
using
geometric
level
sets
journal
of
machine
learning
re
search
11:491-516,2010
vlfeat
http://www.vlfeat.org/applications/apps.html
ac
cessed
2015-06-01
zhang
zhenyue
and
zha
hongyuan
principal
manifolds
and
nonlinear
dimensionality
reduction
via
tangent
space
alignment
siam
journal
on
scientific
computing
26
1):313-338,2005
zhou
dengyong
and
scholkopf
bernhard
regularization
on
discrete
spaces
in
pattern
recognition
pp
361-368
springer
2005
zhu
ji
and
hastie
trevor
kernel
logistic
regression
and
the
import
vector
machine
journal
of
computational
and
graphical
statistics
2005
class
probability
estimation
via
differential
geometric
regularization
a.proof
of
theorem
thus
proof
for
rn
al~
re
tr
11
a7
ry=ri@
00
1
fe
=1
n}
97
y
0,0
fiyo
£i5
as
is
basis
of
the
tangent
space
tpgr(f
to
gr(f
here
0
a(\)iﬁflijv
ka
{{ie:ha@vif‘.let
{e;}
be
an
orthonormal
frame
of
t
gr(f
pt(g
vm0
.
/
flij
jlf}
ei
blr
ro
0
1
for
some
invertible
matrix
bf
ik
=g
g™
0
frs
i
mylte
ro
0
1
=g
(
(
0<
=tssr0
19
define
the
metric
matrix
for
the
basis
{r;}
by
gr
with
gij
=i
75
0y
flf
then
bij
=e€;-€ej
b:"b;rk
st
bfb;gm
=1=(bb")g=
bb
=4
=t
9’1)"sfﬁ,ffff)<
thus
bb
is
computeble
in
terms
of
derivatives
of
after
relabeling
of
indices
therefore
the
last
compo
let
d,w
be
the
rn*l
directional
derivative
of
in
the
nent
of
tr
ii
are
given
by
direction
u
then
~1yij
£1
~1yrs
fa
a1
pdoci=pdy
bn
bp0
67
=
sgest
b
p*((dy,;b¥)ri
b{bed
1y
5=
gy
ij
b!bp'd,,r
gil)jkp”dm
tk
since
pyrp
0
b
an
easy
example
with
bayes
consistency
we
have
we
now
give
an
example
with
loss
function
that
enables
o=
0l
fiet
)
et
t
ild
initiali
errerieres
at
sy
y
jee
at
5
)
easy
bayes
consistency
proof
under
some
mild
initializa
lnal
ndl
tion
assumption
related
notation
is
summerized
in
§c
o+
fioke
i=1
for
ease
of
reading
we
change
the
notation
for
empirical
penalty
pr
in
the
appendix
to
pp,i.e.,p
pp
apq
so
in
particular
0}rn+lt;€
0if
n
thus
pp
measures
the
deviation
of
gr(f
from
the
mapped
train
ing
points
natural
geometric
distance
penalty
term
is
an
dyre=(0
g
fﬁ
klj
l
distance
in
r®
fr(?m
f(a
to
the
averaged
component
of
the
k-nearest
training
points
so
far
we
have
po(f
ro7.k(f
d
<f<x>
22
da
tril=
g~
"
p(0,...,0
fl
.
?
where
is
the
euclidean
distance
in
r
%
is
since
is
given
in
terms
of
derivatives
of
f
we
need
to
the
vector
of
the
last
components
of
&%
17
pt
or
of
comy
write
}?h
p
in
terms
of
derivatives
of
f
for
any
mll
zi\
z%
zzl
with
&
the
i'®
nearest
neighbor
uerr
we
have
of
in
7y
and
dz
is
the
lebesgue
measure
the
gradient
ptu
ptu-e)e
u
bfrj)bfrk
vector
field
is
bibf(u-rj)r
1y
(
v(bo
0)s(e
f(e
f(@
20
class
probability
estimation
via
differential
geometric
regularization
however
v(rp,7
x)s
is
discontinuous
on
the
set
of
points
such
that
has
equidistant
training
points
among
its
nearest
neighbors
is
the
union
of
n
1
dimensional
hyperplanes
in
x
so
has
measure
zero
such
points
will
necessarily
exist
unless
the
last
com
ponents
of
the
mapped
training
points
are
all
or
all
0
to
rectify
this
we
can
smooth
out
v(rp
7
k
to
vector
field
2¢(x
f@)-z
=1
vvavv
here
x
is
smooth
damping
function
close
to
the
sin
gular
function
dp
which
has
dp(z
for
and
0p(z
for
d
outside
any
open
neighborhood
of
d,vrp
t
vb,s,4
for
close
enough
to
dp
recall
the
geometric
penalty
from
the
submission
i.e
pa(f
/
(
dvol
with
the
geometric
gradient
vector
field
being
v5
tr
itx
then
the
gradient
vector
field
vi
m.f
of
this
example
penalty
is
vp;=vpse+avay
2<d]e$>
f(m
%
\tr
1f22
viot
am.f.6
b.1
consistency
analysis
for
training
set
7
we
let
fr
fr
%
be
the
class
probability
estimator
given
by
our
approach
we
denote
the
generalization
risk
of
the
corresponding
plug-in
classifier
hy
by
rp(fr
ep[]lthm
@)#y
the
bayes
risk
is
defined
by
rp
inf
rp(h
h:x—=y
ep[lp,(2)£y
our
algorithm
is
bayes
consistent
if
liindo
rp(f7
rp
holds
in
probability
for
all
distribu
t?nns
on
)
usually
gradient
flow
methods
are
ap
plied
to
convex
functional
so
that
flow
line
approaches
the
unique
global
minimum
if
the
domain
of
the
functional
is
an
infinite
dimensional
manifold
of
e.g
smooth
func
tions
we
always
assume
that
flow
lines
exist
and
that
the
actual
minimum
exists
in
this
manifold
because
our
functionals
are
not
convex
and
because
we
are
strictly
speaking
not
working
with
gradient
vector
fields
we
can
only
hope
to
prove
bayes
consistency
for
the
set
of
initial
estimators
in
the
stable
manifold
of
stable
fixed
point
or
sink
of
the
vector
field
guckenheimer
wor
folk
1993
recall
that
stable
fixed
point
has
maxi
mal
open
neighborhood
the
stable
manifold
sy
on
which
flow
lines
tend
towards
f
for
the
manifold
m
the
sta
ble
manifold
for
stable
critical
point
of
the
vector
field
viot,\,m
4
is
infinite
dimensional
the
proof
of
bayes
consistency
for
multiclass
including
binary
classification
follows
these
steps
step
1
=0
cp
rppr=0
step2
lim
rpp(f
=0=
lim
re(f,)=
rp
step
3
forall
maps(x
a1
|rp
7
f
m-—00
rp,p(f
==
in
probability
proofs
of
these
steps
are
provided
in
following
sub
sections
for
the
notation
see
§c
rp
p
is
the
minimum
of
the
regularized
risk
rp
pa(f
for
f
rppx(f
rpr(f
apa(f
with
rpp(f
yd(f(@),n(x))de
the
d-risk
also
rpr1,.(f
rpgz,.(f
apg(f
with
rp1,(f
vd
f(m
zle
ei
dz
the
empirical
d-risk
theorem
bayes
consistency
let
be
the
size
of
the
training
data
set
let
fy
sy
the
sta
ble
manifold
for
the
global
minimum
fp
\
of
rp
7
x
and
let
f
)
be
sequence
of
functions
on
the
flow
line
of
vipt
x,m,f
starting
with
fl,a,m
with
the
flow
time
m,n—00
a—=0,0—6p
in
probability
for
all
distributions
on
y
if
k/m
asm
00
t
o0
as
oo
then
rp(f,mv,,w
proof
in
the
notation
of
§c
if
f
-
is
global
mini
mum
for
rp,7
then
outside
of
d
fp
7
is
the
limit
of
critical
points
for
the
negative
flow
of
vig
xm,f
as
0p
to
see
this
fix
an
¢
neighborhood
d
of
d
for
sequence
¢
p
viot,a,m.f.6
15
independent
of
j(e&
on
d
so
we
find
function
f
criti
cal
point
of
lev)‘-mvad’xm
equal
to
fd,tm.a
on
x\
d
since
any
lies
outside
some
d
the
sequence
f
converges
at
if
we
let
¢
0
thus
we
can
ignore
the
choice
of
in
our
proof
and
drop
from
the
notation
for
our
algorithm
for
fixed
a,m
we
have
as
above
lim
so
jim
fram
o700
lim
rp
7
a(fnam
b0
70
f
d70
0)s
n—oo
for
f
sf
by
step
2
it
suffices
to
show
rp.p(fpr,.0
%
0
in
probability
we
have
v§
class
probability
estimation
via
differential
geometric
regularization
0
3m
such
that
rp.p(fpr1,.2
rpr(fpr.2
+apc(fp7in
rpr.(fpr1.5
+apc(fpr
+
step
3
bor.a(for.)+3
rpr1.2(fppr)+
minimality
of
fp
7
1
ro1,.(fppa
+apc(fppy
rpp(fppr
+apc(fpry)+
2?6
step
3
rppra(fppyr)+
2?6
=rppr+
4
stepl
for
close
to
zero
since
rp
p(fp
7
1
0
we
are
done
b.2.step
lemma
3
step
1
lim
r}
py
0
a=00
proof
after
the
smoothing
procedure
in
§3.1
for
the
dis
tance
penalty
term
the
function
rp
py
is
con
tinuous
in
the
fréchet
topology
on
m
we
check
that
the
functions
rp
are
equicontinuous
in
a
for
fixed
f
and
0
there
exists
0(f,€
such
that
\
n'|
|rp
pa(fo
rp,pa(fo)|
this
is
immediate
bp.pa(fo
bp.px
o)l
a=
x)pa(fo)l
<e
if
€/|pa(fo)|
itis
standard
that
the
infimum
inf
ry
of
an
equicontinuous
family
of
functions
is
continuous
in
a
s0
li
=r
=0
»s0
lim
rp
pr=rb
pro=rpr(n)=0
b.3.step
we
assume
that
the
class
probability
function
n(xz
rn
r
is
smooth
and
that
the
marginal
distribution
p(z
is
continuous
we
also
let
denote
the
correspond
ing
measure
on
x
notation
hy(x
argmax{
f(z),{
y}
of
course
hp(x
£y
hy(@)7y
=\
o
hy(x
y
lemma
4
step
for
subsequence
lim
rpp(f,)=0=
lim
rp(f,)=rp
100
n—o0
for
some
subsequence
f
}7°
of
f,}
proof
the
left
hand
side
of
the
lemma
is
f
(
n(x))dz
0
which
is
equivalent
to
/x
£
@
n(@))p(@)dz
0
23
since
x
is
compact
and
is
continuous
therefore
it
suf
fices
to
show
/x
f
@
n(@)a@)dz
24
ep[l
@)2y
ep[ln,@
we
recall
that
l
convergence
implies
pointwise
conver
gence
a.e
so
23
implies
that
subsequence
of
f
also
denoted
f
has
f
m(x
pointwise
a.e
on
x
by
our
assumption
on
z
these
statements
hold
for
either
1o
or
lebesgue
measure
by
egorov’s
theorem
for
any
0
there
exists
set
with
p(b
such
that
£
n(x
uniformly
on
b
fix
and
set
zs
{xex:#{argmaxn’(z)}
=1
ey
ey
(
submax
2)|
0}
where
su}?en)l}ax
denotes
the
second
largest
element
in
{n'(x),...,n%(x)}
for
the
moment
assume
that
zy
{z
#{argmax
n’(z)}
1}
has
u(zo
0
ley
1t
follows
easily
that
y(zs
as
0.0n
x'\
zs
b
we
have
ly
@
2y
lhy(a)zy
forn
ns
thus
ep[la\(zsun
lny
@)2y
ep[la\(z50b
ly
@)24
let
ay
be
sets
with
ayy1
ay
and
with
n2
ay
0
if
p(ax
0
then
there
exists
subsequence
also
called
ay
with
p(ay
for
some
k
we
claim
p(nag
k
contradiction
for
the
claim
let
na
if
z
p(ay
for
all
k
we
are
done
if
not
since
the
ay
are
nested
we
can
replace
ay
by
aset
also
called
ay
of
measure
and
such
that
the
new
ay
are
still
nested
for
the
relabeled
nagk
ay
for
all
k
and
we
may
assume
z
k
thus
there
exists
z
a
with
z
and
u(z
0
since
pu(ai
k
we
must
have
a
z
for
all
i
thus
na
is
strictly
larger
than
z
contradiction
in
summary
the
claim
must
hold
so
we
get
contradiction
to
assuming
1(ag
0
class
probability
estimation
via
differential
geometric
regularization
here
is
the
characteristic
function
of
set
a
asd
0
ep[la\(zsub
1n
@)2y
ep[la\
1n
2)2y
and
similarly
for
f
replaced
by
n(z
during
this
process
presumably
goes
to
oo
but
that
precisely
means
lim
ep[l
lny
@yl
ep[lyyp
lnp(@)2y
n—o0
since
ep[ﬂx\sﬂnfn(m#y
ep[]lhfn(m)#y
<€
and
similarly
for
n(z
we
get
im
ep[ly
@
ep[ilhn(w)#y
n—oo
lim
ep[]ix\be]lhfn(i)#y
nllii;o
ep[l"f"(m#y
oo
}lh;o
ep[las
lny@
ep[ilnnu)#y
3e
strictly
speaking
lim
ep[]l,,f
@)y
18
first
lim
sup
and
then
lim
inf
to
show
that
the
limit
exists
since
is
arbitrary
the
proof
is
complete
if
11(zo
0
if
1(zo
0
we
rerun
the
proof
with
replaced
by
zj
as
above
f,|z
converges
uniformly
to
n(x
off
set
of
measure
€
the
argument
above
without
the
set
zs
gives
/th,n(m);éyli(m)dm—>/
lh
@
£yt
t)d
we
then
proceed
with
the
proof
above
on
x'\
zj
corollary
5
step
in
general
for
our
algorithm
jim
rp
p(fam
0=
lim
re(f,am
rp
i—oo
proof
choose
f
,
as
in
theorem
2
since
vlolvkmvfmx‘m
has
pointwise
length
going
to
zero
as
00
£
m
@)}
is
cauchy
sequence
for
all
2
this
im
plies
that
f
.
and
not
just
subsequence
converges
pointwise
to
7
b.4.step
lemma
6
step
3
if
oo
and
k/m
as
oo
then
for
maps(x,af~1
rp
7,.(f
rp.p(f
==
in
probability
for
all
distributions
that
generate
7y
jim
ep[ly\pln
@)2
ep[hx\lehn(w)#y
proof
since
rp
p(f
is
constant
for
fixed
and
p
convergence
in
probability
will
follow
from
weak
conver
gence
ie
m—oo
e7,.[|rp,7.,(f
rp,p(f)l
—
0
we
have
|rp.7
f
rp.p(f
/x
{(f
f(z
%z
22
dz(f(w)ﬂ(w))}
dz
seta
f(z
1
zi
b=
f(x
n(z
then
in
de
d
f(z
%z
21
d&*(f
(
n(2
=1
lnall3
llbll3
doat
=y
ef
b}
=1
=1
=1
la
b7
<2
jae
bel
max{|ae|
be[}
=1
zz
lag
by
since
f/(z
zf
#,n(x
0,1
therefore
it
suffices
to
show
that
/
>_er
=1
so
the
result
follows
if
in
@
3220
@)~
@
dz}
1m~
00
0
x
/}
oforalll
25
lim
e7
m—»o00
by
jensen’s
inequality
e[f
e(f
25
follows
if
nl(w
lim
er
z
=0
forall
£
m-—00
26
let
hi,m(ﬂ
zf
#
then
nf__m
is
actually
an
estimate
of
the
class
probability
7
z
by
the
k-nearest
neighbor
rule
following
the
proof
of
stone’s
theorem
stone
1977
m—00
1m
»00
devroye
et
al
1996
if
kk
—
oo
and
k/m
—
0
26
holds
for
all
distributions
p
class
probability
estimation
via
differential
geometric
regularization
c
notation
hy
argmax{
f'(@
y}
lng
(
2y
rp(f
ep[ilhﬂw)#y
n
d-risk
for
our
pp
rp
p(f
empirical
d-risk)rp
7
f
rp
7
1(f
volume
penalty
term
p
f
rp
pa(f
rp
p(f
apc(f
rp.1,,a(f
rp.7,,(f
apc(f
fp.pa
r*d,p,a
rd.p,a(fd,p.a
fo7a=fd
70k
note
that
we
assume
fp
and
fp
exist
plug-in
classifier
of
estimator
af~1
hy(x
#y
0
hg(xz)=1y
generalization
risk
for
the
estimator
class
probability
function
n‘(x
p(y
|x
bayes
risk
&(f
@
n(w))da
v
135
oo
where
z
is
the
vector
of
the
last
components
of
@
2
with
&
the
i
nearest
neighbor
of
in
7
dvol
gr(f
regularized
d-risk
for
estimator
regularized
empirical
d-risk
for
estimator
function
attaining
the
global
minimum
for
rp
minimum
value
for
rp
function
attaining
the
global
minimum
for
rp
7
a(f
